{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":"<p><code>Entity</code> is an open-source coordinate-agnostic particle-in-cell (PIC) code written in C++17 specifically targeted to study plasma physics in relativistic astrophysical systems. The main algorithms of the code are written in covariant form, allowing to easily implement arbitrary grid geometries. The code is highly modular, and is written in the architecture-agnostic way using the Kokkos performance portability library, allowing the code to efficiently use device parallelization on CPU and GPU architectures of different types. The multi-node parallelization is implemented using the MPI library, and the data output is done via the ADIOS2 library which supports multiple output formats, including HDF5 and BP5.</p> <p><code>Entity</code> is part of the <code>Entity toolkit</code> framework, which also includes a Python library for fast and efficient data analysis and visualization of the simulation data: <code>nt2py</code>.</p> <p>This documentation includes everything you need to know to get started with using and/or contributing to the <code>Entity toolkit</code>. If you find bugs or issues, please feel free to add a GitHub issue or submit a pull request. Users with significant contributions to the code will be added to the list of developers, and assigned an emoji of their choice (important).</p>"},{"location":"#contributors-alphabetical","title":"Contributors (alphabetical)","text":"<ul> <li> Ludwig B\u00f6ss {@LudwigBoess}</li> <li> Yangyang Cai {@StaticObserver}</li> <li> Alexander Chernoglazov {@SChernoglazov}</li> <li> Benjamin Crinquand {@bcrinquand}</li> <li> Alisa Galishnikova {@alisagk}</li> <li> Evgeny Gorbunov {@Alcauchy}</li> <li> Hayk Hakobyan {@haykh}</li> <li> Jens Mahlmann {@jmahlmann}</li> <li> Sasha Philippov {@sashaph}</li> <li> Siddhant Solanki {@sidruns30}</li> <li> Arno Vanthieghem {@vanthieg}</li> <li> Muni Zhou {@munizhou}</li> </ul>"},{"location":"#supporting-grants","title":"Supporting grants","text":"<p>The development of the code was supported by the following grants and awards:</p> <ul> <li>U.S. Department of Energy under contract number DE-AC02-09CH11466.</li> <li>NSF Cyberinfrastructure for Sustained Scientific Innovation (CSSI) program.</li> <li>NVIDIA Corporation Academic Hardware Grant Program.</li> </ul> <p>The developers are pleased to acknowledge that the work was performed using the Princeton Research Computing resources at Princeton University which is a consortium of groups led by the Princeton Institute for Computational Science and Engineering (PICSciE) and Office of Information Technology's Research Computing.</p>"},{"location":"content/1-getting-started/1-compile-run/","title":"Compiling/running","text":"<p>First, make sure you have all the necessary dependencies installed (<code>Kokkos</code> and <code>ADIOS2</code> can be built in-tree with the code, so no additional configuration necessary).</p>"},{"location":"content/1-getting-started/1-compile-run/#configuring-compiling","title":"Configuring &amp; compiling","text":"<ol> <li> <p>Clone the repository with the following command:   <pre><code>git clone --recursive https://github.com/entity-toolkit/entity.git\n</code></pre></p> <p>Note</p> <p>For developers with write access, it is highly recommended to use <code>ssh</code> for cloning the repository:  <pre><code>git clone --recursive git@github.com:entity-toolkit/entity.git\n</code></pre> If you have not set up your github <code>ssh</code> yet, please follow the instructions here. Alternatively, you can clone the repository with <code>https</code> as shown above.</p> </li> <li> <p>Configure the code from the root directory using <code>cmake</code>, e.g.:   <pre><code># from the root of the repository\ncmake -B build -D pgen=&lt;PROBLEM_GENERATOR&gt; -D Kokkos_ENABLE_CUDA=ON &lt;...&gt;\n</code></pre></p> <p>Problem generators can either be one of the default ones, located in the <code>pgens/</code> directory (e.g., <code>-D pgen=reconnection</code>), or the ones from the <code>entity-pgens</code> submodule, in which case you need to prepend a <code>pgens/</code> suffix (e.g., <code>-D pgen=pgens/kelvin-helmholtz</code>; make sure you have the submodule downloaded with <code>git submodule update --init</code>). Alternatively, you may pass a path to any directory containing your problem generator <code>pgen.hpp</code> (either relative or absolute path).</p> <p>All the build options are specified using the <code>-D</code> flag followed by the argument and its value (as shown above). Boolean options are specified as <code>ON</code> or <code>OFF</code>. The following are all the options that can be specified:</p> Option Description Values Default <code>pgen</code> problem generator e.g., see <code>pgens/</code> directory <code>precision</code> floating point precision <code>single</code>, <code>double</code> <code>single</code> <code>output</code> enable output <code>ON</code>, <code>OFF</code> <code>ON</code> <code>mpi</code> enable multi-node support <code>ON</code>, <code>OFF</code> <code>OFF</code> <code>gpu_aware_mpi</code> 1.2.0 enable GPU-aware MPI communications <code>ON</code>, <code>OFF</code> <code>ON</code> <code>DEBUG</code> enable debug mode <code>ON</code>, <code>OFF</code> <code>OFF</code> <code>TESTS</code> compile the unit tests <code>ON</code>, <code>OFF</code> <code>OFF</code> <p>Optionally, when compiling the Kokkos/ADIOS2 in-tree, there are some CMake and other library-specific options (for Kokkos and ADIOS2) that can be specified along with the above ones. While the code picks most of these options for the end-user, some of them can/should be specified manually. In particular:</p> Option Description Values Default <code>Kokkos_ENABLE_CUDA</code> enable CUDA <code>ON</code>, <code>OFF</code> <code>OFF</code> <code>Kokkos_ENABLE_HIP</code> enable HIP <code>ON</code>, <code>OFF</code> <code>OFF</code> <code>Kokkos_ENABLE_SYCL</code> enable SYCL <code>ON</code>, <code>OFF</code> <code>OFF</code> <code>Kokkos_ENABLE_OPENMP</code> enable OpenMP <code>ON</code>, <code>OFF</code> <code>OFF</code> <code>Kokkos_ARCH_***</code> use particular CPU/GPU architecture see Kokkos documentation <code>Kokkos</code> attempts to determine automatically <p>When using an external Kokkos/ADIOS2, these flags are not needed.</p> <p>Note</p> <p>When simply compiling with <code>-D Kokkos_ENABLE_CUDA=ON</code> or <code>_HIP=ON</code> without additional flags, <code>CMake</code> will try to deduce the GPU architecture based on the machine you are compiling on. Oftentimes this might not be the same as the architecture of the machine you are planning to run on (and sometimes the former might lack GPU altogether). To be more explicit, you can specify the GPU architecture manually using the <code>-D Kokkos_ARCH_***=ON</code> flags. For example, to explicitly compile for <code>A100</code> GPUs, you can use <code>-D Kokkos_ARCH_AMPERE80=ON</code>. For <code>V100</code> -- use <code>-D Kokkos_ARCH_VOLTA70=ON</code>.</p> </li> <li> <p>After the <code>cmake</code> is done configuring the code, a directory named <code>build</code> will be created in the root directory. You can now compile the code by running:   <pre><code>cmake --build build -j $(nproc)\n</code></pre>   where <code>&lt;NCORES&gt;</code> is the number of cores you want to use for the compilation (if you skip the <code>&lt;NCORES&gt;</code> and just put <code>-j</code>, <code>cmake</code> will attempt to take as many threads as possible). Note, that the <code>-j</code> flag is optional, and if not specified, the code will compile using a single core.</p> </li> <li> <p>After the compilation is done, you will find the executable called <code>entity.xc</code> in the <code>./build/src/</code> directory. That's it! You can now finally run the code.</p> </li> <li> <p>You may also \"install\" the executable in a specific direction (by default, it would be <code>./bin</code>, which can be overriden using the <code>-D CMAKE_INSTALL_PREFIX</code> flag) by running <code>cmake --install build</code> after the compilation is done.</p> </li> </ol>"},{"location":"content/1-getting-started/1-compile-run/#running","title":"Running","text":"<p>You can run the code with the following command:</p> <p><pre><code>/path/to/entity.xc -input /path/to/input_file.toml\n</code></pre> <code>entity.xc</code> runs headlessly, producing several diagnostic outputs. <code>.info</code> file contains the general information about the simulation including all the parameters used, the compiler version, the architecture, etc. <code>.log</code> file contains timestamps of each simulation substep and is mainly used for debugging purposes. In case the simulation fails or throws warnings, an <code>.err</code> file will be generated, containing the error message. The simulation also dumps a live stdout report after each successfull simulation step, which contains information about the time spent on each simulation substep, the number of active particles, and the estimated time for completion. It may look something like this: <pre><code>................................................................................\nStep: 1260     [of 1448]\nTime: 1.7401   [\u0394t = 0.0014]\n\n[SUBSTEP]                  [DURATION]  [% TOT]\n  Communications............314.00 \u00b5s     9.55\n  CurrentDeposit............400.00 \u00b5s    12.17\n  CurrentFiltering..........803.00 \u00b5s    24.43\n  Custom....................929.00 \u00b5s    28.26\n  FieldBoundaries.............0.00 ns     0.00\n  FieldSolver...............502.00 \u00b5s    15.27\n  ParticleBoundaries..........0.00 ns     0.00\n  ParticlePusher............339.00 \u00b5s    10.31\nTotal                         3.29 ms\n\nParticle count:                [TOT (%)]\n  species 1 (e-)........2.59e+04 ( 2.6%)\n  species 2 (e+)........2.59e+04 ( 2.6%)\n\nAverage timestep: 9.57 ms\nRemaining time: 1.80 s\nElapsed time: 11.12 s\n[\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0          ]  87.02%\n................................................................................\n</code></pre></p>"},{"location":"content/1-getting-started/1-compile-run/#testing","title":"Testing","text":"<p>1.0.0</p> <p>To compile the unit tests, you need to specify the <code>-D TESTS=ON</code> flag when configuring the code with <code>cmake</code>. After the code is compiled, you can run the tests with the following command: <pre><code>ctest --test-dir build/\n</code></pre></p> <p>You may also specify the <code>--output-on-failure</code> flag to see the output of the tests that failed.</p> <p>To run only specific tests, you can use the <code>-R</code> flag followed by the regular expression that matches the test name. For example, to run all the tests that contain the word <code>particle</code>, you can use: <pre><code>ctest --test-dir build/ -R particle\n</code></pre></p>"},{"location":"content/1-getting-started/1-compile-run/#specific-architectures","title":"Specific architectures","text":""},{"location":"content/1-getting-started/1-compile-run/#hiprocm-amd-gpus","title":"HIP/ROCm @ AMD GPUs","text":"<p> 1.1.0 </p> <p>Compiling on AMD GPUs is typically not an issue: </p> <ol> <li>Make sure you have the ROCm library loaded: e.g., run <code>rocminfo</code>;</li> <li> <p>Sometimes the environment variables are not properly set up, so make sure you have the following variables properly defined: </p> </li> <li> <p><code>CMAKE_PREFIX_PATH=/opt/rocm</code> (or wherever ROCm is installed),</p> </li> <li><code>CC=hipcc</code> &amp; <code>CXX=hipcc</code>,</li> <li> <p>in rare occasions, you might have to also explicitly pass <code>-D CMAKE_CXX_COMPILER=hipcc -D CMAKE_C_COMPILER=hipcc</code> to cmake during the configuration stage;</p> </li> <li> <p>Compile the code with proper Kokkos flags; i.e., for MI250x GPUs you would use: <code>-D Kokkos_ENABLE_HIP=ON</code> and <code>-D Kokkos_ARCH_AMD_GFX90A=ON</code>.</p> </li> </ol> <p>Now running is a bit trickier and the exact instruction might vary from machine to machine (part of it is because ROCm is much less streamlined than CUDA, but also system administrators on clusters are often more negligent towards AMD GPUs). </p> <ul> <li> <p>If you are running this on a cluster -- the first thing to do is to inspect the documentation of the cluster. There you might find the proper <code>slurm</code> command for requesting GPU nodes and binding each GPU to respective CPUs. </p> </li> <li> <p>On personal machines figuring this out is a bit easier. First, inspect the output of <code>rocminfo</code> and <code>rocm-smi</code>. From there, you should be able to find the ID of the GPU you want to use. If you see more than one device -- that means you either have an additional AMD CPU, or an integrated GPU installed as well; ignore them. You will need to override two environment variables:</p> </li> <li> <p><code>HSA_OVERRIDE_GFX_VERSION</code> set to GFX version that you used to compile the code (if you used <code>GFX1100</code> Kokkos flag, that would be <code>11.0.0</code>);</p> </li> <li><code>HIP_VISIBLE_DEVICES</code>, and <code>ROCR_VISIBLE_DEVICES</code> both need to be set to your device ID (usually, it's just a number from 0 to the number of devices that support HIP).</li> </ul> <p>For example, the output of <code>rocminfo | grep -A 5 \"Agent \"</code> may look like this: <pre><code>Agent 1                  \n*******                  \n  Name:                    AMD Ryzen 9 7940HS w/ Radeon 780M Graphics\n  Uuid:                    CPU-XX                             \n  Marketing Name:          AMD Ryzen 9 7940HS w/ Radeon 780M Graphics\n  Vendor Name:             CPU                                \n--\nAgent 2                  \n*******                  \n  Name:                    gfx1100                            \n  Uuid:                    GPU-XX                             \n  Marketing Name:          AMD Radeon\u2122 RX 7700S             \n  Vendor Name:             AMD                                \n--\nAgent 3                  \n*******                  \n  Name:                    gfx1100                            \n  Uuid:                    GPU-XX                             \n  Marketing Name:          AMD Radeon Graphics                \n  Vendor Name:             AMD\n</code></pre> In this case, the required GPU is the <code>Agent 2</code>, which supports GFX1100. <code>rocm-smi</code> will look something like this: <pre><code>============================================ ROCm System Management Interface ============================================\n====================================================== Concise Info ======================================================\nDevice  Node  IDs              Temp    Power    Partitions          SCLK  MCLK     Fan    Perf  PwrCap       VRAM%  GPU%  \n              (DID,     GUID)  (Edge)  (Avg)    (Mem, Compute, ID)                                                        \n==========================================================================================================================\n0       1     0x7480,   19047  35.0\u00b0C  0.0W     N/A, N/A, 0         0Mhz  96Mhz    29.8%  auto  100.0W       0%     0%    \n1       2     0x15bf,   17218  48.0\u00b0C  19.111W  N/A, N/A, 0         None  1000Mhz  0%     auto  Unsupported  82%    5%    \n==========================================================================================================================\n================================================== End of ROCm SMI Log ===================================================\n</code></pre> so the GPU we need has <code>Device</code> ID of <code>0</code> (since it's the dedicated GPU, it might automatically turn off when idle to save power on laptops; hence <code>Power = 0.0W</code>). Now we can run the code with:  <pre><code>HSA_OVERRIDE_GFX_VERSION=11.0.0 HIP_VISIBLE_DEVICES=0 ROCR_VISIBLE_DEVICES=0 ./executable ...\n</code></pre></p>"},{"location":"content/1-getting-started/2-dependencies/","title":"Dependencies","text":"<p>To compile the code you need to have the following dependencies installed:</p> <ul> <li><code>CMake</code> (version &gt;= 3.16; verify by running <code>cmake --version</code>).</li> <li><code>GCC</code> (version &gt;= 8.3.1; verify by running <code>g++ --version</code>), <code>llvm</code> (tested on version &gt;= 11; verify by running <code>clang++ --version</code>) or Intel C++ compiler (version &gt;= 19.1 or higher; verify by running <code>icx --version</code>).</li> <li>to compile for NVIDIA GPUs, you need to have the <code>CUDA toolkit</code> installed (version &gt;= 11.0; verify by running <code>nvcc --version</code>).</li> <li>to compile for AMD GPUs, you will need the ROCm libraries and the HIP compilers/runtime (verify by running <code>hipcc --version</code>).</li> <li><code>MPI</code> (e.g., <code>OpenMPI</code>, <code>MPICH</code>, etc.; verify by running <code>mpicxx --version</code>) for multi-node simulations.</li> <li><code>HDF5</code> for data output (verify by running <code>h5c++ --version</code>).</li> </ul> <p>Cuda compatibility</p> <p>Note, that different versions of <code>CUDA</code> are compatible with different host compiler versions (i.e., <code>gcc</code>, <code>llvm</code>, etc.). Please, refer to the following curated list for the compatibility matrix.</p> <p>All the other third-party dependencies, such as <code>Kokkos</code> and <code>ADIOS2</code>, are included in the repository as submodules and can be automatically compiled when you run <code>cmake</code> (although, we recommend to install <code>ADIOS2</code> externally as it can take a while to compile). </p> <p>In addition to this section, we also provide more detailed instructions on how to set up the dependencies as well as the submit scripts for the most popular and widely used clusters in the following section.</p> <p>Note</p> <p>To play with the code with all the dependencies already installed in the containerized environment, please refer to the section on Docker.</p>"},{"location":"content/1-getting-started/2-dependencies/#preinstalling-third-party-libraries","title":"Preinstalling third-party libraries","text":"<p>To speed up the compilation process, it is often beneficial to precompile &amp; install the third-party libraries and use those during the build process, either by setting the appropriate environment variables, using it within a conda/spack environment, or by using environment modules. Alternatively, of course, you can use the libraries provided by your system package manager (<code>pacman</code>, <code>apt</code>, <code>brew</code>, <code>nix</code>, ...), or the cluster's module system.</p> <p>Warning</p> <p>If the system you're working on has <code>MPI</code> or <code>HDF5</code> already installed (either through environment modules or any package manager), it's highly recommended to use these libraries, instead of building your own. Instructions for these two here are provided as a last resort.</p>"},{"location":"content/1-getting-started/2-dependencies/#spack-recommended","title":"Spack (recommended)","text":"<p>Spack is essentially a package manager for HPC systems which allows to install all the dependencies locally, optionally cross-compiling them with the already available libraries. If spack is not already available on your system (or on a cluster), you can simply download it (preferably to your home directory) with:</p> <pre><code>git clone -c feature.manyFiles=true --depth=2 https://github.com/spack/spack.git\n</code></pre> <p>and add the following to your <code>.bashrc</code> or <code>.zshrc</code> (or analogous) startup file:</p> <pre><code>. spack/share/spack/setup-env.sh\n</code></pre> <p>to activate <code>spack</code> on shell login.</p>"},{"location":"content/1-getting-started/2-dependencies/#identifying-pre-installed-compilerspackages","title":"Identifying pre-installed compilers/packages","text":"<p>Since spack compiles everything from the source code, it is recommended to use as many of the already pre-installed packages as possible. In particular, you can use the already installed compilers, or big libraries, such as the MPI and HDF5. To make spack aware of their existence, you can simply run:</p> <pre><code># to add compilers\nspack compiler add\n# and to add all libraries\nspack external find\n</code></pre> <p>Note</p> <p>If your machine is using environment modules, you may need to first load the compilers/libraries you need, before running the command above, e.g.: <pre><code>module load gcc/13\nmodule load openmpi/5\n</code></pre></p> <p>If for some reason spack does not find the local package you need, you may want to add it manually, by modifying the <code>$HOME/.spack/packages.yaml</code> file to add the following (example for locally installed <code>cuda</code> and <code>openmpi</code>): <pre><code>packages:\n  # ...\n  cuda:\n    buildable: false\n    externals:\n    - prefix: /opt/cuda/\n      spec: cuda@12.8\n  openmpi:\n    buildable: false\n    externals:\n    - prefix: /usr/\n      spec: openmpi@5.0.6\n</code></pre></p> <p>Then you can run, e.g., <code>spack spec cuda</code> to check whether it finds the package: <code>[e]</code> at the front will indicate that it found the external package, if so -- you can \"install\" it in spack by using <code>spack install cuda</code> or <code>spack install openmpi</code>.</p> <p>To check which packages <code>spack</code> has found, simply run <code>spack find</code> or to check the compilers, run <code>spack compilers</code>.</p> <p>Note</p> <p>It is strongly recommended to use the pre-installed <code>MPI</code>, <code>CUDA</code> and other big libraries, instead of installing them via <code>spack</code> since these can be specifically configured on the machine you're running on.</p>"},{"location":"content/1-getting-started/2-dependencies/#setting-up-spack-environment","title":"Setting up spack environment","text":"<p>After that, it is recommended to create a spack environment and install all the other libraries within it. To do so, first create &amp; activate the environment by running:</p> <pre><code>spack env create entity-env\nspack env activate entity-env\n</code></pre> <p>Whenever you activate this environment, spack will automatically add the libraries installed within it to the <code>PATH</code> so that <code>cmake</code> can identify them when compiling the <code>Entity</code>. Within the environment you may now install all the necessary libraries. Below we present the possible commands you may need to run for each of them. Make sure to first check which dependencies will <code>spack</code> use to compile the library before actually installing. For that, you can run, e.g.,</p> <pre><code>spack spec kokkos &lt;OPTIONS&gt;\n</code></pre> <p>which will show all the dependencies it will use. In front of each dependency, you'll see one of the following:</p> <ul> <li> <p><code>[e]</code>: external (locally installed) package, </p> </li> <li> <p><code>[+]</code>: a package already installed within spack, </p> </li> <li> <p><code>[-]</code>: a package that will be downloaded and built during the installation.</p> </li> </ul> <p>Once you're satisfied, you may run <code>spack install --add kokkos &lt;OPTIONS&gt;</code> to actually perform the installation (within the environment).</p> <code>HDF5</code><code>ADIOS2</code><code>Kokkos</code> <p>It is highly recommended to use the <code>HDF5</code> already installed on the cluster and find it via <code>spack</code> as described above. Nonetheless, you may also install it via <code>spack</code> using the following command: <pre><code>spack install --add hdf5 +cxx\n</code></pre> You may also add <code>-mpi</code> flag to disable the <code>MPI</code> support.</p> <p>Because we rely on <code>HDF5</code> together with the <code>ADIOS2</code>, it is recommended to have <code>hdf5</code> installed externally (and make sure <code>spack</code> sees that installation by running <code>spack spec hdf5</code>). You can then install <code>ADIOS2</code> (in a <code>spack</code> environment) using the following command: <pre><code>spack install --add adios2 +hdf5 +pic\n</code></pre> You may also add <code>-mpi</code> option to disable the <code>MPI</code> support (<code>HDF5</code> will also have to be serial for that to work).</p> <p>For <code>Kokkos</code>, you will always use the following settings <code>+pic +aggressive_vectorization</code> on top of the architecture specific settings. For example, to compile with <code>CUDA</code> support on Ampere80 architecture (A100 card), you can do <pre><code>spack install --add kokkos +pic +aggressive_vectorization +cuda +wrapper cuda_arch=80\n</code></pre></p> <p>And, again, before running this command, make sure to run it with <code>spack spec ...</code> instead of <code>spack install --add</code> with the same options just to make sure spack will not install an external CUDA.</p> <p>Note</p> <p>Sometimes <code>spack</code> might not recognize the CPU architecture properly, especially when compiling on a node different from the one where the code will be running (e.g., login node vs compute node). In that case, when compiling the <code>kokkos</code>, you may need to provide also the following option: <code>target=zen2</code> (or other target cpu architecture). This might fail on the first try, since by default <code>spack</code> does not allow for manual architecture specification, in which case first reconfigure spack using <code>spack config add concretizer:targets:host_compatible:false</code>, and then try again.</p> <p><code>spack info</code></p> <p>To see all the available configuration options for a given package, simply run <code>spack info &lt;PACKAGE&gt;</code>.</p> <p>Using an explicit compiler</p> <p>You can instruct <code>spack</code> to use a specific compiler which it has identified (find out by running <code>spack compilers</code>) by adding the following flag (example for <code>clang</code>): <pre><code>spack install &lt;PACKAGE&gt; &lt;OPTIONS&gt; %clang\n</code></pre></p> <p>Garbage collection</p> <p>Simply uninstalling the package may left behind some build caches which often take up a lot of space. To get rid of these, you may run <code>spack gc</code> which will try its best to delete all these caches as well as all the unused packages.</p>"},{"location":"content/1-getting-started/2-dependencies/#anaconda","title":"Anaconda","text":"<p>If you want to have <code>ADIOS2</code> with the serial <code>HDF5</code> support (i.e., without <code>MPI</code>) installed in the conda environment, we provide a shell script <code>conda-entity-nompi.sh</code> which installs the proper compiler, the <code>hdf5</code> library, and the <code>ADIOS2</code>. Run the scripts via:</p> <pre><code>source conda-entity-nompi.sh\n</code></pre> <p>This also <code>pip</code>-installs the <code>nt2.py</code> package for post-processing. With this configuration, the <code>Kokkos</code> library will be built in-tree.</p>"},{"location":"content/1-getting-started/2-dependencies/#building-dependencies-from-source","title":"Building dependencies from source","text":"<p>The form below allows you to generate the appropriate build scripts and optionally the environment modules for the libraries you want to compile and install. </p> <code>MPI</code><code>hdf5</code><code>ADIOS2</code><code>Kokkos</code> <p>Prerequisites:</p> <ul> <li>Make sure to have a host (CPU) compiler such as GCC or LLVM (if necessary, load using <code>module load</code>). </li> <li>If using CUDA, make sure that <code>$CUDA_HOME</code> points to CUDA install path (if necessary, load cudatoolkit using <code>module load</code>). </li> </ul> <p>Note: If using environment modules, add the mentioned <code>module load ...</code> commands to the new modulefile created at step #4.</p> <p>Possible configurations:</p> <p><p> CUDA support </p></p> <p>Procedure:</p> <ol> <li> <p>Download the OpenMPI source code: <pre><code>git clone https://github.com/open-mpi/ompi.git \ncd ompi\n</code></pre></p> </li> <li> <p>Run the script below to configure  </p> </li> <li> <p>Compile &amp; install with <pre><code>make -j\nmake install\n</code></pre></p> </li> <li> <p>Optionally, if using environment modules, create a modulefile with the following content:   Change the <code>&lt;MPI_INSTALL_DIR&gt;</code> and add <code>module load</code>s for the appropriate compilers as needed.</p> </li> </ol> <p>Prerequisites:</p> <ul> <li>Make sure to have a host (CPU) compiler such as GCC or LLVM (if necessary, load using <code>module load</code>). </li> <li>If using MPI, make sure that <code>$MPI_HOME</code> points to its install directory (if necessary, load with <code>module load</code>). </li> </ul> <p>Note: If using environment modules, add the mentioned <code>module load ...</code> commands to the new modulefile created at step #4.</p> <p>Possible configurations:</p> <p><p> MPI support </p></p> <p>Procedure:</p> <ol> <li> <p>Download the latest HDF5 source code (below is an example for <code>1.14.6</code>) into a temporary directory: <pre><code>mkdir hdf5src\ncd hdf5src\nwget https://github.com/HDFGroup/hdf5/releases/download/hdf5_1.14.6/hdf5-1.14.6.tar.gz\ntar xvf hdf5-1.14.6.tar.gz\n</code></pre></p> </li> <li> <p>Download the latest dependencies into the same (<code>hdf5src</code>) temporary directory (do not extract):</p> <ul> <li>HDF5 plugins (e.g., 1.14.6): <pre><code>wget https://github.com/HDFGroup/hdf5_plugins/releases/download/hdf5-1.14.6/hdf5_plugins-1.14.tar.gz\n</code></pre></li> <li>ZLIB (e.g., 1.3.1):  <pre><code>wget https://github.com/madler/zlib/releases/download/v1.3.1/zlib-1.3.1.tar.gz\n</code></pre></li> <li>ZLIBNG (e.g., 2.2.4): <pre><code>wget https://github.com/zlib-ng/zlib-ng/archive/refs/tags/2.2.4.tar.gz\n</code></pre></li> <li>LIBAEC (e.g., 1.1.3): <pre><code>wget https://github.com/MathisRosenhauer/libaec/releases/download/v1.1.3/libaec-1.1.3.tar.gz\n</code></pre></li> </ul> </li> <li> <p>Copy three <code>.cmake</code> scripts from the uncompressed HDF5 directory to the temporary directory: <pre><code>cp hdf5-1.14.6/config/cmake/scripts/*.cmake .\n</code></pre></p> </li> <li> <p>In <code>HDF5options.cmake</code> uncomment the following line: <pre><code>set (ADD_BUILD_OPTIONS \"${ADD_BUILD_OPTIONS} -DBUILD_TESTING:BOOL=OFF\")\n</code></pre></p> </li> <li> <p>In <code>CTestScript.cmake</code> uncomment the following line (or if not present, simply add it below <code>cmake_minimum_required</code>): <pre><code>set (LOCAL_SKIP_TEST \"TRUE\")\n</code></pre></p> </li> <li> <p>From the same temporary directory run the following:  </p> </li> <li> <p>Optionally, if using environment modules, create a modulefile with the following content:  </p> </li> </ol> <p>Change the <code>&lt;HDF5_INSTALL_DIR&gt;</code> and add <code>module load</code>s for the appropriate compiler/MPI as needed.</p> <p>Prerequisites:</p> <ul> <li>Make sure to have a host (CPU) compiler such as GCC or LLVM (if necessary, load using <code>module load</code>). </li> <li>Also make sure to have an HDF5 installed; check that <code>$HDF5_ROOT</code> properly points to the install directory (if necessary, load with <code>module load</code>). </li> <li>If using MPI, make sure that <code>$MPI_HOME</code> points to its install directory (if necessary, load with <code>module load</code>). </li> </ul> <p>Note: If using environment modules, add the mentioned <code>module load ...</code> commands to the new modulefile created at step #4.</p> <p>Possible configurations:</p> <p><p> MPI support </p></p> <p>Procedure:</p> <ol> <li>Download the ADIOS2 source code:   <pre><code>git clone https://github.com/ornladios/ADIOS2.git\ncd ADIOS2\n</code></pre></li> <li>Run the script below to configure  </li> <li>Compile &amp; install with <pre><code>cmake --build build -j\ncmake --install build\n</code></pre></li> <li>Optionally, if using environment modules, create a modulefile with the following content:   Change the <code>&lt;ADIOS2_INSTALL_DIR&gt;</code> and add <code>module load</code>s for the appropriate compiler/HDF5/MPI as needed.</li> </ol> <p>Prerequisites:</p> <ul> <li>Make sure to have a host (CPU) compiler such as GCC or LLVM (if necessary, load using <code>module load</code>). </li> <li>If using CUDA, make sure that <code>$CUDA_HOME</code> points to CUDA install path (if necessary, load cudatoolkit using <code>module load</code>). </li> <li>If using ROCm/HIP, make sure to have <code>hipcc</code>, and set <code>CC</code> and <code>CXX</code> variables to <code>hipcc</code> (if necessary, load HIP SDK using <code>module load</code>).</li> </ul> <p>Note: If using environment modules, add the mentioned <code>module load ...</code> commands to the new modulefile created at step #4.</p> <p>Possible configurations:</p> <p><p> GPU support </p> <p> CPU architecture:  Native ARMv8.2 with SVE Support ARMv8.0 ARMv8.1 ARMv8 ThunderX ARMv8 ThunderX2 AMDAVX Zen Zen 2 Zen 3 Sapphire Rapids Skylake Broadwell Haswell Sandy Bridge Knights Landing Knights Corner POWER9 POWER8 </p> <p> GPU architecture:  Native Hopper 9.0: H100 Ada Lovelace 8.9: L4/L40 Ampere 8.6: A40/A10/A16/A2 Ampere 8.0: A100/A30 Turing 7.5: T4 Volta 7.2 Volta 7.0: V100 Pascal 6.1: P40/P4 Pascal 6.0: P100 Maxwell 5.3 Maxwell 5.2: M60/M40 Maxwell 5.0 Kepler 3.7: K80 Kepler 3.5: K40/K20 Kepler 3.2 Kepler 3.0: K10 GFX942: MI300A/MI300X GFX940: MI300A (pre-production) GFX90A: MI200 series GFX90A: MI100 GFX906: MI50/MI60 GFX1100: 7900xt GFX1030: V620/W6800 Xe-HPC: Max 1550 Xe-HP Iris Xe MAX (DG1) Gen12LP: UHD Graphics 770 Gen11: UHD Graphics Gen9: HD Graphics 510/Iris Pro 580 Just-In-Time compilation </p></p> <p>Procedure:</p> <ol> <li>Download the Kokkos source code:   <pre><code>git clone -b master https://github.com/kokkos/kokkos.git\ncd kokkos\n</code></pre></li> <li>Run the script below to configure  </li> <li>Compile &amp; install with <pre><code>cmake --build build -j\ncmake --install build\n</code></pre></li> <li>Optionally, if using environment modules, create a modulefile with the following content:   Change the <code>&lt;KOKKOS_INSTALL_DIR&gt;</code> and add <code>module load</code>s for the appropriate host compiler/CUDA/HIP/SYCL as needed.</li> </ol> <p>Note</p> <p>We also provide a command-line tool called <code>ntt-dploy</code> which can be used for the same purpose.</p> <p>Nix</p> <p> 1.2.0 </p> <p>On systems with the <code>nix</code> package manager, you can quickly make a development environment with all the dependencies installed using <code>nix-shell</code> (from the root directory of the code):</p> <p><pre><code>nix-shell dev/nix --arg hdf5 true --arg mpi true --argstr gpu HIP --argstr arch amd_gfx1100\n\n# you can inspect the default settings by\nhead dev/nix/shell.nix\n</code></pre> Note the escapes of quotation marks when specifying a string argument.</p>"},{"location":"content/1-getting-started/3-inputfile/","title":"Input parameters","text":"<p>Entity reads almost all the information (except for the problem generator) about the simulation at runtime from an input file provided in the <code>.toml</code> format. The most up-to-date full version of the input file with all the possible input parameters with their descriptions can be found in the root directory of the main repository in the <code>input.example.toml</code>.</p> <pre>required</pre> These paremeters are required to be specified for any simulation <pre>inferred</pre> These parameters are not directly specified by the user, but are inferred from other input parameters parameter type description default <pre>simulation</pre> <pre>name</pre> <pre>string</pre> Name of the simulationThe name is used for the output files. <pre>engine</pre> <pre>string</pre>\"SRPIC\", \"GRPIC\" Simulation engine to use <pre>runtime</pre> <pre>float </pre>&gt; 0 Max runtime in physical (code) units <pre>domain</pre> <pre>number</pre> <pre>int</pre> Number of domains 1 [no MPI]; MPI_SIZE [MPI] <pre>decomposition</pre> <pre>array&lt;int&gt; </pre>size 1 :-&gt;: 3 Decomposition of the domain (for MPI) in each of the directions* -1 means the code will determine the decomposition in the specific direction automatically.* Automatic detection is either done by inference from # of MPI tasks, or by balancing the grid size on each domain. $[-1, -1, -1]$ <pre>grid</pre> <pre>resolution</pre> <pre>array&lt;uint&gt; </pre>size 1 :-&gt;: 3 Spatial resolution of the gridDimensionality is inferred from the size of this array. <pre>extent</pre> <pre>array&lt;tuple&lt;float&gt;&gt; </pre>size 1 :-&gt;: 3 Physical extent of the grid* For spherical geometry, only specify <pre>[[rmin, rmax]]</pre>, other values are set automatically.* For cartesian geometry, cell aspect ratio has to be 1: <pre>dx=dy=dz</pre>. <pre>dim</pre> <pre>short</pre>1, 2, 3 Dimensionality of the grid <pre>metric</pre> <pre>metric</pre> <pre>string</pre>\"Minkowski\", \"Spherical\", \"QSpherical\", \"Kerr_Schild\", \"QKerr_Schild\", \"Kerr_Schild_0\" Metric on the grid <pre>qsph_r0</pre> <pre>float </pre>-\u221e -&gt; rmin <code>r0</code> paramter for the QSpherical metric <code>x1 = log(r-r0)</code>Negative values produce almost uniform grid in r. $0.0$ <pre>qsph_h</pre> <pre>float </pre>-1 :-&gt;: 1 <code>h</code> paramter for the QSpherical metric <code>th = x2 + 2*h x2 (pi-2*x2)*(pi-x2)/pi^2</code> $0.0$ <pre>ks_a</pre> <pre>float </pre>0 :-&gt; 1 Spin parameter for the Kerr Schild metric $0.0$ <pre>coord</pre> <pre>string</pre>\"cartesian\", \"spherical\", \"qspherical\" Coordinate system on the grid <pre>ks_rh</pre> <pre>float</pre> Size of the horizon for GR Kerr Schild <pre>params</pre> <pre>map&lt;string, float&gt;</pre> A map of all metric-specific parameters together (for easy access) <pre>boundaries</pre> <pre>fields</pre> <pre>array&lt;tuple&lt;string&gt;&gt; </pre>\"PERIODIC\", \"MATCH\", \"FIXED\", \"ATMOSPHERE\", \"CUSTOM\", \"HORIZON\", \"CONDUCTOR\" Boundary conditions for fields* When periodic in any of the directions, you should only set one value: [..., [\"PERIODIC\"], ...].* In spherical, bondaries in theta/phi are set automatically (only specify bc @ <pre>[rmin, rmax]</pre>): [[\"ATMOSPHERE\", \"MATCH\"]].* In GR, the horizon boundary is set automatically (only specify bc @ rmax): [[\"MATCH\"]]. <pre>particles</pre> <pre>array&lt;tuple&lt;string&gt;&gt; </pre>\"PERIODIC\", \"ABSORB\", \"ATMOSPHERE\", \"CUSTOM\", \"REFLECT\", \"HORIZON\" Boundary conditions for fields* When periodic in any of the directions, you should only set one value [..., [\"PERIODIC\"], ...].* In spherical, bondaries in theta/phi are set automatically (only specify bc @ <pre>[rmin, rmax]</pre>) [[\"ATMOSPHERE\", \"ABSORB\"]].* In GR, the horizon boundary is set automatically (only specify bc @ <pre>rmax</pre>): [[\"ABSORB\"]]. <pre>match</pre> <pre>ds</pre> <pre>float </pre> or <pre> array&lt;tuple&lt;float&gt;&gt;</pre> Size of the matching layer in each direction for fields in physical (code) unitsIn spherical, this is the size of the layer in <pre>r</pre> from the outer wall. 1% of the domain size (in shortest dimension) <pre>absorb</pre> <pre>ds</pre> <pre>float</pre> Size of the absorption layer for particles in physical (code) units* In spherical, this is the size of the layer in <pre>r</pre> from the outer wall.* In cartesian, this is the same for all dimensions where applicable. 1% of the domain size (in shortest dimension) <pre>atmosphere</pre> <pre>temperature</pre> <pre>float</pre> Temperature of the atmosphere in units of <code>m0 c^2</code>[required] if <pre>ATMOSPHERE</pre> is one of the boundaries. <pre>density</pre> <pre>float</pre> Peak number density of the atmosphere at base in units of <code>n0</code> <pre>height</pre> <pre>float</pre> Pressure scale-height in physical units <pre>species</pre> <pre>array&lt;int&gt; </pre>size 2 Species indices of particles that populate the atmosphere <pre>ds</pre> <pre>float</pre> Distance from the edge to which the gravity is imposed in physical units0.0 means no limit. $0.0$ <pre>g</pre> <pre>float</pre> Acceleration due to imposed gravity <pre>scales</pre> <pre>larmor0</pre> <pre>float </pre>&gt; 0.0 Fiducial larmor radius <pre>skindepth0</pre> <pre>float </pre>&gt; 0.0 Fiducial plasma skin depth <pre>dx0</pre> <pre>float</pre> fiducial minimum size of the cell <pre>V0</pre> <pre>float</pre> fiducial elementary volume <pre>n0</pre> <pre>float</pre> Fiducial number density <pre>q0</pre> <pre>float</pre> Fiducial elementary charge <pre>sigma0</pre> <pre>float</pre> Fiducial magnetization parameter <pre>B0</pre> <pre>float</pre> Fiducial magnetic field <pre>omegaB0</pre> <pre>float</pre> Fiducial cyclotron frequency <pre>algorithms</pre> <pre>current_filters</pre> <pre>ushort </pre>&gt;= 0 Number of current smoothing passes $0$ <pre>toggles</pre> <pre>fieldsolver</pre> <pre>bool</pre> Toggle for the field solver <pre>true</pre> <pre>deposit</pre> <pre>bool</pre> Toggle for the current deposition <pre>true</pre> <pre>timestep</pre> <pre>CFL</pre> <pre>float </pre>0.0 -&gt; 1.0 Courant-Friedrichs-Lewy numberCFL number determines the timestep duration. $0.95$ <pre>correction</pre> <pre>float</pre> Correction factor for the speed of light used in field solver $1.0$ <pre>dt</pre> <pre>float</pre> timestep duration <pre>gr</pre> <pre>pusher_eps</pre> <pre>float </pre>&gt; 0.0 Stepsize for numerical differentiation in GR pusher $10^{-6}$ <pre>pusher_niter</pre> <pre>ushort </pre>&gt; 0 Number of iterations for the Newton-Raphson method in GR pusher $10$ <pre>gca</pre> <pre>e_ovr_b_max</pre> <pre>float </pre>0.0 -&gt; 1.0 Maximum value for E/B allowed for GCA particles $0.9$ <pre>larmor_max</pre> <pre>float</pre> Maximum Larmor radius allowed for GCA particles (in physical units)When <pre>larmor_max</pre> == 0, the limit is disabled. $0.0$ <pre>synchrotron</pre> <pre>gamma_rad</pre> <pre>float </pre>&gt; 0.0 Radiation reaction limit gamma-factor for synchrotron[required] if one of the species has <pre>cooling = \"synchrotron\"</pre>. $1.0$ <pre>particles</pre> <pre>ppc0</pre> <pre>float </pre>&gt; 0.0 Fiducial number of particles per cell <pre>use_weights</pre> <pre>bool</pre> Toggle for using particle weights <pre>false</pre> <pre>clear_interval</pre> <pre>uint</pre> Timesteps between particle re-sorting (removing dead particles)Set to 0 to disable re-sorting. $100$ <pre>nspec</pre> <pre>uint</pre> Number of particle species <pre>species</pre> <pre>label</pre> <pre>string</pre> Label of the species<pre>&lt;INDEX&gt;</pre> is the index of the species in the list starting from 1. <pre>\"s&lt;INDEX&gt;\"</pre> <pre>mass</pre> <pre>float </pre>&gt;= 0.0 Mass of the species (in units of fiducial mass) <pre>charge</pre> <pre>float</pre> Charge of the species (in units of fiducial charge) <pre>maxnpart</pre> <pre>uint </pre>&gt; 0 Maximum number of particles per task <pre>pusher</pre> <pre>string</pre>\"Boris\", \"Vay\", \"Boris,GCA\", \"Vay,GCA\", \"Photon\", \"None\" Pusher algorithm for the species \"Boris\" [massive]; \"Photon\" [massless] <pre>n_payloads</pre> <pre>ushort</pre> Number of additional (payload) variables for each particle of the given species $0$ <pre>cooling</pre> <pre>string</pre>\"None\", \"Synchrotron\" Radiation reaction to use for the species <pre>\"None\"</pre> <pre>setup</pre> Parameters for specific problem generators and setups <pre>output</pre> <pre>format</pre> <pre>string</pre>\"disabled\", \"hdf5\", \"BPFile\" Output format <pre>\"hdf5\"</pre> <pre>interval</pre> <pre>uint </pre>&gt; 0 Number of timesteps between all outputsValue is overriden by output intervals for specific outputs. $1$ <pre>interval_time</pre> <pre>float</pre> Physical (code) time interval between all outputs* When <pre>interval_time</pre> &lt; 0, the output is controlled by <pre>interval</pre>, otherwise by <pre>interval_time</pre>.* Value is overriden by output intervals for specific outputs. $-1.0$ <pre>separate_files</pre> <pre>bool</pre> Whether to output each timestep into separate files <pre>true</pre> <pre>fields</pre> <pre>enable</pre> <pre>bool</pre> Toggle for the field output <pre>true</pre> <pre>quantities</pre> <pre>array&lt;string&gt;</pre>\"E\", \"B\", \"J\", \"divE\", \"Rho\", \"Charge\", \"N\", \"Nppc\", \"T0i\", \"Tij\", \"Vi\", \"D\", \"H\", \"divD\", \"A\" Field quantities to output* For <pre>T</pre>, you can use unspecified indices: <pre>Tij</pre>, <pre>T0i</pre>, or specific ones: <pre>Ttt</pre>, <pre>T00</pre>, <pre>T02</pre>, <pre>T23</pre>.* For <pre>T</pre>, in cartesian can also use \"x\" \"y\" \"z\" instead of \"1\" \"2\" \"3\".* By default, we accumulate moments from all massive species, one can specify only specific species: <pre>Ttt_1_2</pre>, <pre>Rho_1</pre>, <pre>Rho_3_4</pre>. [] <pre>custom</pre> <pre>array&lt;string&gt;</pre> Custom (user-defined) field quantities [] <pre>mom_smooth</pre> <pre>ushort</pre> Smoothing window for the output of moments (\"Rho\", \"Charge\", \"T\", ...) $0$ <pre>interval</pre> <pre>uint</pre> Number of timesteps between field outputs* When <pre>!= 0</pre>, overrides <pre>output.interval</pre>.* When <pre>== 0</pre>, <pre>output.interval</pre> is used. $0$ <pre>interval_time</pre> <pre>float</pre> Physical (code) time interval between field outputs* When <pre>&lt; 0</pre>, the output is controlled by <pre>interval</pre>.* When specified, overrides <pre>output.interval_time</pre>. $-1.0$ <pre>downsampling</pre> <pre>uint </pre> or <pre> array&lt;uint&gt; </pre>&gt;= 1 Downsample factor for the output of fields* The output is downsampled by the given factors in each direction.* If a scalar is given, it is applied to all directions. $[1, 1, 1]$ <pre>particles</pre> <pre>enable</pre> <pre>bool</pre> Toggle for the particles output <pre>true</pre> <pre>species</pre> <pre>array&lt;int&gt;</pre> Particle species indices to outputIf empty, all species are output. [] <pre>stride</pre> <pre>uint </pre>&gt; 1 Stride for the output of particles $100$ <pre>interval</pre> <pre>uint</pre> Number of timesteps between particle outputs* When <pre>!= 0</pre>, overrides <pre>output.interval</pre>.* When <pre>== 0</pre>, <pre>output.interval</pre> is used. $0$ <pre>interval_time</pre> <pre>float</pre> Physical (code) time interval between particle outputs* When <pre>&lt; 0</pre>, the output is controlled by <pre>interval</pre>.* When specified, overrides <pre>output.interval_time</pre>. $-1.0$ <pre>spectra</pre> <pre>enable</pre> <pre>bool</pre> Toggle for the spectra output <pre>true</pre> <pre>e_min</pre> <pre>float</pre> Minimum energy for the spectra output $10^{-3}$ <pre>e_max</pre> <pre>float</pre> Maximum energy for the spectra output $10^{3}$ <pre>log_bins</pre> <pre>bool</pre> Whether to use logarithmic bins <pre>true</pre> <pre>n_bins</pre> <pre>uint </pre>&gt; 0 Number of bins for the spectra output $200$ <pre>interval</pre> <pre>uint</pre> Number of timesteps between spectra outputs* When <pre>!= 0</pre>, overrides <pre>output.interval</pre>.* When <pre>== 0</pre>, <pre>output.interval</pre> is used. $0$ <pre>interval_time</pre> <pre>float</pre> Physical (code) time interval between spectra outputs* When <pre>&lt; 0</pre>, the output is controlled by <pre>interval</pre>.* When specified, overrides <pre>output.interval_time</pre>. $-1.0$ <pre>debug</pre> <pre>as_is</pre> <pre>bool</pre> Output fields \"as is\" without conversions <pre>false</pre> <pre>ghosts</pre> <pre>bool</pre> Output fields with values in ghost cells <pre>false</pre> <pre>stats</pre> <pre>enable</pre> <pre>bool</pre> Toggle for the stats output <pre>true</pre> <pre>interval</pre> <pre>uint </pre>&gt; 0 Number of timesteps between stat outputsOverriden if <pre>output.stats.interval_time != -1</pre>. $100$ <pre>interval_time</pre> <pre>float</pre> Physical (code) time interval between stat outputsWhen <pre>&lt; 0</pre>, the output is controlled by <pre>interval</pre>. $-1.0$ <pre>quantities</pre> <pre>array&lt;string&gt;</pre>\"B^2\", \"E^2\", \"ExB\", \"N\", \"Npart\", \"Charge\", \"Rho\", \"T00\", \"T0i\", \"Tij\" Field quantities to output* For particle moments, ...* ... same notation is used as for <pre>output.fields.quantities</pre>. [<pre>\"B^2\"</pre>, <pre>\"E^2\"</pre>, <pre>\"ExB\"</pre>, <pre>\"Rho\"</pre>, <pre>\"T00\"</pre>] <pre>custom</pre> <pre>array&lt;string&gt;</pre> Custom (user-defined) stats [] <pre>checkpoint</pre> <pre>interval</pre> <pre>uint </pre>&gt; 0 Number of timesteps between checkpoints $1000$ <pre>interval_time</pre> <pre>float </pre>&gt; 0 Physical (code) time interval between checkpointsWhen <pre>&lt; 0</pre>, the output is controlled by <pre>interval</pre>. $-1.0$ <pre>keep</pre> <pre>int</pre> Number of checkpoints to keep* 0 = disable checkpointing.* -1 = keep all checkpoints. $2$ <pre>walltime</pre> <pre>string</pre> Write a checkpoint once after a fixed walltime* The format is \"HH:MM:SS\".* Empty string or \"00:00:00\" disables this functionality.* Writing checkpoint at walltime does not stop the simulation. <pre>\"00:00:00\"</pre> <pre>write_path</pre> <pre>string</pre> Parent directory to write checkpoints toThe directory is created if it does not exist. <code>&lt;simname&gt;.ckpt</code> <pre>read_path</pre> <pre>string</pre> Parent directory to use when resuming from a checkpoint inherit <code>write_path</code> <pre>is_resuming</pre> <pre>bool</pre> Whether the simulation is resuming from a checkpoint <pre>start_step</pre> <pre>uint</pre> Timestep of the checkpoint used to resume <pre>start_time</pre> <pre>float</pre> Time of the checkpoint used to resume <pre>diagnostics</pre> <pre>interval</pre> <pre>int </pre>&gt; 0 Number of timesteps between diagnostic logs $1$ <pre>blocking_timers</pre> <pre>bool</pre> Blocking timers between successive algorithms <pre>false</pre> <pre>colored_stdout</pre> <pre>bool</pre> Enable colored stdout <pre>true</pre> <pre>log_level</pre> <pre>string</pre>\"VERBOSE\", \"WARNING\", \"ERROR\" Specify the log level\"VERBOSE\" prints all messages, \"WARNING\" prints only warnings and errors, \"ERROR\" prints only errors. <pre>\"VERBOSE\"</pre>"},{"location":"content/1-getting-started/4-units/","title":"Understanding the units","text":""},{"location":"content/1-getting-started/4-units/#physical-units","title":"Physical units","text":"<p>Note</p> <p>Here in the section we use \\(\\bm{E}\\), and \\(\\bm{B}\\) to denote the electric and magnetic fields in the orthonormal coordinate basis. For the purposes of this section flat space-time with trivial metric is assumed.</p> <p>Most of the time the user will only need to interact with quantities in the so-called physical units. For example, when specifying <code>larmor0 = 42.0</code> in the input file, the value <code>42.0</code> is in physical units (we discuss what <code>larmor0</code> means shortly). The <code>extent</code> parameter in the input, similarly, is the extent of the simulation box in physical units.</p> <p>In basic particle-in-cell algorithm we need to take care of three different equation sets: the Maxwell's equations on EM fields, equations of motion for the particles, and the current deposition. Let us write down all these in CGS (with all the vectors being defined in an orthonormal basis):</p> \\[ \\begin{aligned} \\text{Maxwell's equations}&amp; \\begin{cases} \\frac{\\partial\\bm{B}}{c\\partial t} = -\\nabla\\times\\bm{E}\\\\\\\\ \\frac{\\partial\\bm{E}}{c\\partial t} = \\nabla\\times\\bm{B} - \\frac{4\\pi}{c} \\bm{J} \\end{cases}\\\\\\\\ \\text{EoM for the {\\it i}-th particle}&amp; \\begin{cases} \\frac{d\\left(\\bm{\\beta}_i\\gamma_i\\right)}{c dt} = \\frac{q_i}{m_i c^2} \\left(\\bm{E} + \\bm{\\beta}_i\\times\\bm{B}\\right)\\\\\\\\ \\frac{d\\bm{x}_i}{c dt} = \\bm{\\beta}_i \\end{cases}\\\\\\\\ \\text{Deposited current}:&amp;~ \\bm{J}=\\frac{1}{V}\\sum\\limits_{i\\in V} q_i w_i \\bm{\\beta}_i c \\end{aligned} \\] <p>Here \\(q_i\\), \\(m_i\\), \\(w_i\\), \\(\\beta_i\\) are the charges, the masses, the weights, and the dimensionless three-velocities of the macroparticles.</p> <p>We now introduce the fiducial quantities which will help rescale all the equations from CGS to physical units. First of all, let us introduce a fiducial particle: one that has a charge \\(q_0&gt;0\\) and a mass \\(m_0\\). And let us also define \\(B_0\\) to be the fiducial magnetic field strength. If the fiducial particle moves in the uniform and constant magnetic field of strength \\(B_0\\) in the perpendicular plane with a velocity \\(\\bm{\\beta}\\gamma=1\\), then it's Larmor radius is: \\(\\rho_0=m_0 c^2/\\left(q_0 B_0\\right)\\). </p> <p>In Gaussian units there is a fundamental freedom to pick \\(q_0/m_0 \\equiv 1\\), and \\(c\\equiv 1\\). Thus, \\(B_0 \\equiv 1/\\rho_0\\).</p> <p>If we now have plasma consisting of static particles (ions) of charge \\(-q_0\\) and fiducial particles of charge \\(q_0\\) with both species having a number density \\(n_0\\) (fiducial number density), then this plasma will have a fundamental oscillation frequency, \\(\\omega_0^2 = 4\\pi n_0 q_0^2 / m_0\\), and an equivalent lengthscale (fiducial skin depth): \\(d_0 \\equiv 1/\\omega_0\\). Further we will see, that it is useful to define a fiducial current density as \\(J_0 \\equiv  4\\pi q_0 n_0\\).</p> <p>Because we are dealing with a discretized space, we also need to define a fiducial cell volume, \\(V_0\\), fiducial number of particles per cell, \\(\\texttt{PPC}_0\\). Then the fiducial number density from above can be chosen to be \\(n_0 \\equiv \\texttt{PPC}_0 / V_0\\).</p> Symbol Description Definition In the code \\(c\\) speed of light \\(\\equiv 1\\) -- \\(\\texttt{PPC}_0\\) fiducial number of p.p.c. fundamental <code>Simulation::params().ppc0()</code> \\(d_0\\) fiducial skin-depth fundamental <code>Simulation::params().skindepth0()</code> \\(\\rho_0\\) fiducial Larmor radius fundamental <code>Simulation::params().larmor0()</code> \\(V_0\\) fiducial volume size (see below) <code>Simulation::params().V0()</code> \\(n_0\\) fiducial number density \\(\\equiv\\texttt{PPC}_0 / V_0\\) <code>Simulation::params().n0()</code> \\(4\\pi q_0\\) fiducial particle charge \\(\\equiv \\left(n_0 d_0^2\\right)^{-1}\\) <code>Simulation::params().q0()</code> \\(m_0\\) fiducial particle masses \\(\\equiv q_0\\) \\(\\sigma_0\\) fiducial magnetization \\(\\equiv \\left(d_0/\\rho_0\\right)^2\\) <code>Simulation::params().sigma0()</code> \\(B_0\\) fiducial field strength \\(\\equiv \\rho_0^{-1}\\) <code>Simulation::params().B0()</code> \\(J_0\\) fiducial current density \\(\\equiv 4\\pi q_0 n_0\\) <p>We can then rewrite our equations in the \"dimensionless\" form, by renormalizing everything to fiducial units.</p> \\[ \\begin{aligned} \\text{with}~~~&amp;\\bm{e}\\equiv \\bm{E}/B_0,~~~\\bm{b}\\equiv \\bm{B}/B_0,~~~\\bm{j}\\equiv 4\\pi\\bm{J}/J_0,\\\\ &amp;\\tilde{q}_i \\equiv q_i/q_0,~~~\\tilde{m}_i \\equiv m_i/m_0,~~~q_0/m_0 \\equiv 1,~~~c\\equiv 1\\\\\\\\ \\text{Maxwell's equations}&amp; \\begin{cases} \\frac{\\partial\\bm{b}}{\\partial t} = -\\nabla\\times\\bm{e}\\\\\\\\ \\frac{\\partial\\bm{e}}{\\partial t} = \\nabla\\times\\bm{b} - \\frac{J_0}{B_0} \\bm{j} \\end{cases}\\\\\\\\ \\text{EoM for the {\\it i}-th particle}&amp; \\begin{cases} \\frac{d\\left(\\bm{\\beta}_i\\gamma_i\\right)}{dt} = \\frac{\\tilde{q}_i}{\\tilde{m}_i}B_0 \\left(\\bm{e} + \\bm{\\beta}_i\\times\\bm{b}\\right)\\\\\\\\ \\frac{d\\bm{x}_i}{dt} = \\bm{\\beta}_i \\end{cases}\\\\\\\\ \\text{Deposited current}:&amp;~ \\bm{j}=\\frac{V_0}{V}\\frac{1}{\\texttt{PPC}_0}\\sum\\limits_{i\\in V} \\tilde{q}_i w_i \\bm{\\beta}_i \\end{aligned} \\] <p>Note</p> <p>Fields and quantities contained in the data output are all normalized to their corresponding fiducial values. For a given physical setup, these normalized quantities are insensitive to either the resolution of the simulation, or the particle sampling (\\(\\texttt{PPC}_0\\)). In other words, if you measure 10 plasma oscillations in the time interval \\(0 &lt; t &lt; 1\\) with \\(\\texttt{PPC}_0 = 10\\), and resolution \\(128^3\\), then you will measure the same number of oscillations with the same amplitude for \\(\\texttt{PPC}_0 = 1000\\), and resolution \\(512^3\\). If you'd like to increase the \"physical\" density of your plasma, you should drop the value of \\(d_0\\). Similarly, if you want to weaken the strength of your field, you should increase \\(\\rho_0\\).</p>"},{"location":"content/1-getting-started/4-units/#fiducial-volume-and-number-density","title":"Fiducial volume and number density","text":"<p>To associate the number of simulation particles to the fiducial number density, \\(n_0\\), we need to define the fiducial cell volume \\(V_0\\). For Cartesian geometry, where all the cells have exactly the same size, it is defined simply as \\(V_0 \\equiv (\\Delta x)^D\\), with \\(D\\) being the dimension of the simulation. For spherical geometries, \\(V_0\\) is defined as the volume of the first cell near the pole: </p> \\[ V_0 \\equiv \\begin{cases} (\\Delta x)^D&amp;,~\\text{for Cartesian}\\\\\\\\ \\sqrt{\\det{h}}\\bigg\\rvert_{r=\\Delta r/2,~\\theta=\\Delta \\theta/2}&amp;,~\\text{for spherical} \\end{cases} \\] <p>The interpretation of this is quite simple: if you initialize \\(\\textrm{PPC}\\) particles per each cell with charges of \\(q = \\tilde{q}q_0\\), having an average weight of \\(w\\), and moving with an average velocity \\(\\beta^{\\hat{i}}\\), their number density and the current density they impose would be:</p> \\[ \\frac{n}{n_0} = \\frac{\\textrm{PPC}}{\\textrm{PPC}_0} \\frac{V_0}{\\sqrt{h}}w ,~~~\\frac{4\\pi J^{\\hat{i}}}{J_0} = \\underbrace{\\frac{\\textrm{PPC}}{\\textrm{PPC}_0}\\frac{V_0}{\\sqrt{h}}w}_{n/n_0} \\tilde{q} \\beta^{\\hat{i}}. \\] <p>Notice, that these quantities are independent of the resolution of the simulation, and the value of \\(\\textrm{PPC}_0\\). </p> <p>Important</p> <p>Knowing the exact value of \\(V_0\\) is not necessary for the end-user. All the factors of \\(V_0\\), \\(n_0\\) etc must be canceled when working in dimensionless units. If you find otherwise, double check your calculations.</p>"},{"location":"content/1-getting-started/4-units/#defining-the-physical-units","title":"Defining the physical units","text":"<p>The following parameters are parsed from the <code>input</code> file (under the <code>[units]</code> block) to define the fiducial physical quantities:</p> <ul> <li><code>larmor0</code>: fiducial Larmor radius, \\(\\rho_0\\), of a particle with charge \\(q_0\\) and mass \\(m_0\\) moving in a uniform magnetic field of strength \\(B_0\\) in the perpendicular plane with a velocity \\(\\bm{\\beta}\\gamma=1\\).</li> <li><code>skindepth0</code>: fiducial skin depth for plasma, \\(d_0\\), consisting of static particles (ions) of charge \\(-q_0\\) and fiducial particles of charge \\(q_0\\) with both species having a number density \\(n_0\\) (fiducial number density).</li> <li><code>ppc0</code>: fiducial number of particles per cell, \\(\\textrm{PPC}_0\\). If the domain of physical extent of size \\(1\\) is filled with \\(\\textrm{PPC}_0\\) simulation particles per each cell with weights \\(1\\), then the average number density of the simulation is \\(n/n_0 = 1\\).</li> </ul>"},{"location":"content/1-getting-started/4-units/#conversion-to-physical-units","title":"Conversion to physical units","text":"<p>Equations that rely on pure electromagnetism (e.g., no quantum effects) can be directly expressed through our basis parameters. For that you need to simply make the following substitutions </p> \\[ n\\to \\tilde{n} n_0,~~~m\\to \\tilde{m}m_0,~~~q\\to \\tilde{q}q_0\\\\ \\bm{B}\\to \\bm{b} B_0,~~~\\bm{E}\\to \\bm{e} B_0,~~~4\\pi\\bm{J}\\to \\bm{j} J_0\\\\ ct\\to t \\] <p>and then use equivalence relations in (1) to reduce all the fiducial unknowns to \\(\\rho_0\\), and \\(d_0\\). If done correctly, no additional factors should remain (e.g., \\(B_0\\), \\(c\\), or \\(n_0\\)). Most of the quantities either plotted in the GUI or written in the output are in normalized physical units. For example, the output contains \\(\\bm{b}\\) and \\(\\bm{e}\\) field components (for non-GR simulations these are defined in the orthonormal basis).</p> <p>Example</p> <p>For instance, we can compare the rest-mass energy density of plasma with number density \\(n_p\\) consisting of particles with masses \\(m_p\\) and charges \\(\\pm q_p\\) with the energy density of the magnetic field of strength \\(B\\):</p> \\[ \\frac{U_B}{\\rho_p c^2}\\equiv \\frac{B^2/(8\\pi)}{n_p m_p c^2} = \\frac{b}{2\\tilde{n}_p\\tilde{m}_p} \\left(\\frac{d_0}{\\rho_0}\\right)^2 = \\frac{b}{2\\tilde{n}_p\\tilde{m}_p} \\sigma_0, \\] <p>where \\(\\tilde{n}_p = n_p / n_0\\), \\(\\tilde{m}_p = m_p / m_0\\), and \\(b = B/B_0\\).</p> <p>As another example, suppose we try to estimate Goldreich-Julian number density for a magnetosphere with a light cylinder defined as \\(R_{\\rm LC}=c/\\Omega\\), a magnetic field of strength \\(B\\) near the surface and pair plasma of charge \\(\\pm q_p\\). </p> \\[ \\frac{n_{\\rm GJ}}{n_0}\\equiv \\frac{\\Omega B}{2\\pi c |q_p|} = \\frac{2 b}{|\\tilde{q}_p|}\\frac{d_0^2}{\\rho_0 R_{\\rm LC}} \\] <p>where \\(R_{\\rm LC}\\) is now measured in physical units (same units as for \\(\\rho_0\\) and \\(d_0\\)).</p>"},{"location":"content/1-getting-started/5-vis/","title":"Output &amp; visualization","text":"<p>To enable the runtime output of the simulation data, configure the code with the <code>-D output=ON</code> flag. As a backend <code>Entity</code> uses the open-source ADIOS2 library compiled in-place. The output is written in the HDF5 format, however, more formats will be added in the future. </p> <p>The output is configured using the following configurations in the <code>input</code> file:</p> <pre><code>[simulation]\n  name   = \"MySimulation\" # (5)!\n\n  # ...\n[output]\n  format = \"hdf5\" # (2)!\n  interval = 100 # (3)!\n  interval_time = 0.1 # (8)!\n  separate_files = true # (15)!\n\n  [output.fields]\n    quantities = [\"B\", \"E\", \"Rho_1_2\", \"...\"] # (1)!\n    stride = 2 # (9)!\n    mom_smooth = 2 # (4)!\n\n  [output.particles]\n    species = [1, 2, 4] # (7)!\n    stride = 10 # (6)!\n\n  [output.spectra]\n    e_min = 1e-2 # (12)!\n    e_max = 1e3\n    log_bins = true # (13)!\n\n  [output.stats]\n    quantities = [\"N\", \"Npart\", \"ExB\", \"J.E\"] # (14)!\n\n  [output.debug]\n    as_is = false # (10)!\n    ghosts = false # (11)!\n</code></pre> <ol> <li>fields to write</li> <li>output format (current supported: \"HDF5\", or \"disabled\" for no output)</li> <li>output interval (in the number of time steps)</li> <li>smoothing stencil size for moments (in the number of cells) [defaults to 1]</li> <li>title is used for the output filename</li> <li>stride used for particle output (write every <code>prtl_stride</code>-th particle) [defaults to 100]</li> <li>particle species to output</li> <li>output interval in time units (overrides <code>interval</code> if specified)</li> <li>stride used for field output (write every <code>fields_stride</code>-th cell) [defaults to 1]</li> <li>write the field quantities as-is (without conversion/interpolation) [defaults to false]</li> <li>write the ghost cells [defaults to false]</li> <li>Min/max energies for binning the energy distribution [default to 1e-3 -&gt; 1e3]</li> <li>whether to use logarithmic energy bins or linear</li> <li>box reduced quantities to output as stats</li> <li>whether to write in a single file or into separate files</li> </ol> <p>Output is written in the run directory either in a single <code>hdf5</code> file, <code>MySimulation.h5</code>, or in the directory with the same name <code>MySimulation</code>. </p> <p>Following is the list of all supported fields</p> Field name Description Normalization <code>E</code> Electric field (all components) \\(B_0\\) <code>B</code> Magnetic field (all components) \\(B_0\\) <code>D</code> GR: electric field (all components) \\(B_0\\) <code>H</code> GR: aux. magnetic field (all components) \\(B_0\\) <code>J</code> Current density (all components) \\(4\\pi q_0 n_0\\) <code>Rho</code> Mass density \\(m_0 n_0\\) <code>Charge</code> Charge density \\(q_0 n_0\\) <code>N</code> Number density \\(n_0\\) <code>V</code> 1.2.0 Mean 3-velocity dimensionless <code>Nppc</code> Raw number of particles per cell dimensionless <code>Nppc</code> Raw number of particles per cell dimensionless <code>Tij</code> Energy-momentum tensor (all components) \\(m_0 n_0\\) <code>divE</code> 1.2.0 Divergence of \\(E\\) arb. units <code>divD</code> 1.2.0 GR: divergence of \\(D\\) arb. units <code>A</code> GR: 2D vector potential \\(A_\\varphi\\) arb. units <p>and particle quantities</p> Particle quantity Description Units <code>X</code> Coordinates (all components) physical <code>U</code> Four-velocities (all components) dimensionless <code>W</code> Weights dimensionless <p> 1.2.0  The code also has an output of box-averaged stats into a <code>.csv</code> file, which are simply scalars per each output timestep. The following quantities can be computed</p> Box-reduced quantity Description Units <code>E^2</code> Mean \\(E^2\\) \\(B_0^2\\) <code>B^2</code> Mean \\(B^2\\) \\(B_0^2\\) <code>ExB</code> Mean \\(\\bm{E}\\times \\bm{B}\\) \\(B_0^2\\) <code>J.E</code> Mean \\(\\bm{J}\\cdot \\bm{E}\\) \\(4\\pi q_0 n_0 B_0\\) <code>N</code> Mean \\(n\\) \\(n_0\\) <code>Npart</code> Total # of particles dimensionless <code>Rho</code> Mean mass density \\(m_0 n_0\\) <code>Charge</code> Mean charge density \\(q_0 n_0\\) <code>Tij</code> Mean energy-momentum tensor (all components) \\(m_0 n_0\\) <p>\"Mean\" in this context refers to volume-averaging: i.e., $\\langle E_x^2 \\rangle = V^{-1}\\int \\sqrt{h} d^3 \\bm{x}~ E_x^2 $, or $\\langle T^{ij}\\rangle \\equiv V^{-1} \\int d^3\\bm{u} \\sqrt{h} d^3 \\bm{x} ~(u^i u^j / u^0) f(\\bm{u}) $, where \\(V\\equiv \\int \\sqrt{h} d^3\\bm{x}\\). As such, these values (except for <code>Npart</code>) are insensitive to the resolution of the grid or the number of particles per cell.</p> <p>Refining moments for the output</p> <p>One can specify particular components to output for the <code>Tij</code> fields/stats: <code>T0i</code> will output the <code>T00</code>, <code>T01</code>, and <code>T02</code> components, while <code>Tii</code> will output only the diagonal components: <code>T11</code>, <code>T22</code>, and <code>T33</code>, and <code>Tij</code> will output all the 6 components. For quantities computed from particles (moments of the distribution), one can also specify the particle species which will be used to compute the moments: <code>Rho_1</code> (density of species 1), <code>N_2_3</code> (number density of species 2 and 3), <code>Tij_1_3</code> (energy-momentum tensor for species 1 and 3), etc. </p> <p>All of the vector fields are interpolated to cell centers before the output, and converted to orthonormal basis. The particle-based moments are smoothed with a stencil (specified in the input file; <code>mom_smooth</code>) for each particle.</p> <p>In addition, one can write custom user-defined field quantities to the output with the fields or stats. Refer to the following section for more details.</p> <p>Can one track particles at different times?</p> <p>Particle tracking (outputting the same batch of particles at every timestep) is unfortunately not yet implemented, and will unlikely be available due to limitations imposed by the nature of GPU computations.</p>"},{"location":"content/1-getting-started/5-vis/#nt2py","title":"<code>nt2py</code>","text":"<p>We provide the <code>nt2py</code> python package to help easily access and manipulate the simulation data. <code>nt2py</code> package uses the <code>dask</code> and <code>xarray</code> libraries together with <code>h5py</code> and <code>h5pickle</code> to lazily load the output data and provide a convenient interface for the data analysis and quick visualization. </p> <p>To start using <code>nt2py</code>, it is recommended to create a python virtual environment and install the required packages:</p> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate # (1)!\npip install nt2py # (2)!\n</code></pre> <ol> <li>Now all the packages will be installed in the <code>.venv</code> directory which you can remove at any time without affecting the system.</li> <li>If you plan to use jupyter you might also need to run the following <code>pip install jupyterlab ipykernel</code>.</li> </ol> <p>Now simply import the <code>nt2</code> module and load the output data:</p> <pre><code>import nt2\n\ndata = nt2r.Data(path=\"MySimulation.h5\", single_file=True) # (1)!\n\n# or if saving as multiple files\ndata = nt2r.Data(path=\"MySimulation\")\n</code></pre> <ol> <li>Note, that even though the <code>h5</code> file can be quite large, the data is loaded lazily, so the memory consumption is minimal; data chunks are only loaded when they are actually needed for the analysis or visualization.</li> </ol>"},{"location":"content/1-getting-started/5-vis/#accessing-fields","title":"Accessing fields","text":"<p>Data selection is conveniently done with the <code>sel</code> and <code>isel</code> methods for the <code>xarray</code> Datasets (more info). For example, to select the <code>Rho</code> field around physical time <code>t=98</code>, one can do:</p> <pre><code>data.fields.Rho.sel(t=98, method=\"nearest\") # (1)!\n</code></pre> <ol> <li>The <code>method=\"nearest\"</code> is used to select the closest time step to the requested time.</li> </ol> <p> </p> <p>We can then plot the selected data using the <code>plot</code> method of the <code>xarray</code> Dataset:</p> <pre><code>data.fields.Rho\\\n  .sel(t=98, method=\"nearest\")\\\n  .plot(\n    norm=mpl.colors.Normalize(0, 1e2),  # (2)!\n    cmap=\"jet\") # (1)!\n</code></pre> <ol> <li>The <code>norm</code> and <code>cmap</code> arguments are used to set the colorbar limits and the colormap just like in normal <code>matplotlib</code> context.</li> <li>Make sure to also <code>module load matplotlib as mpl</code>.</li> </ol> <p>If the resolution is too high, one can also coarsen the data before plotting:</p> <pre><code>data.fields.Rho\\\n  .sel(t=98, method=\"nearest\")\\\n  .coarsen(x=16, y=4).mean()\\\n  .plot(\n    norm=mpl.colors.Normalize(0, 1e2),\n    cmap=\"jet\")\n</code></pre> <p>or downsample:</p> <pre><code>data.fields.Rho\\\n  .sel(t=98, method=\"nearest\")\\\n  .isel(x=slice(None, None, 16), y=slice(None, None, 4))\\ # (1)!\n  .plot(\n    norm=mpl.colors.Normalize(0, 1e2),\n    cmap=\"jet\")\n</code></pre> <ol> <li>The difference between <code>isel</code> and <code>sel</code> is that <code>isel</code> uses the integer indices along the given dimension, while <code>sel</code> uses the physical coordinates.</li> </ol> <p> </p> <p>One can also do more complicated things, such as building a 1D plot of the evolution of the mean \\(B^2\\) in the box:</p> <pre><code>data.fields.Bx**2 + data.fields.By**2 + data.fields.Bz**2\\\n  .mean((\"x\", \"y\"))\\\n  .plot()\n</code></pre> <p>or make \"waterfall\" plots, collapsing the quantity along one of the axis, and plotting vs the other axis and time:</p> <pre><code>(data.fields.Rho_2 - data.fields.Rho_1)\\\n  .mean(\"x\")\\\n  .plot(yincrease=False)\n</code></pre> <p>Particles and spectra can, in turn, be accessed via <code>data.particles[s]</code>, where <code>s</code> is the species index, and <code>data.spectra</code>.</p> <p><code>nt2py</code> documentation</p> <p>You can access the documentation of the <code>nt2py</code> functions and methods of the <code>Data</code> object by calling <code>nt2r.&lt;function&gt;?</code> in the jupyter notebook or <code>help(nt2r.&lt;function&gt;)</code> in the python console.</p>"},{"location":"content/1-getting-started/5-vis/#accessing-particles","title":"Accessing particles","text":"<p>Particles are stored in the same <code>data</code> object and are lazily preloaded when one calls <code>nt2.Data(...)</code>, as we did above. To access the particle data, use <code>data.particles</code>, which returns a python dictionary where the key is particles species index, and the value is an <code>xarray</code> Dataset with the particle data. For example, to access the <code>x</code> and <code>y</code> coordinates of the first species, one can do:</p> <pre><code>data.particles[1].x\ndata.particles[1].y\n</code></pre> <p>The shape of the returned dataset is number of particles times the number of time steps. To select the data at a specific time step, one can use the same <code>sel</code> or <code>isel</code> methods mentioned above. For example, to access the 10-th output timestep of the 3-rd species, one can do:</p> <pre><code>data.particles[3].isel(t=10).x\n</code></pre> <p>Scatter plotting the particles on a 2D plane is quite easy, since <code>xarray</code> has a built-in <code>plot.scatter</code> method:</p> <pre><code>species_3 = data.particles[3]\nspecies_4 = data.particles[4]\n\nspecies_3.isel(t=-1)\\\n  .plot.scatter(x=\"x\", y=\"y\", \n                label=species_3.attrs[\"label\"])\nspecies_4.isel(t=-1)\\\n  .plot.scatter(x=\"x\", y=\"y\", \n                label=species_4.attrs[\"label\"])\n</code></pre> scatter plot \\(\\{x,~y\\}\\) <p></p> <p><code>isel</code> indexing</p> <p><code>isel(t=-1)</code> selects the last time step.</p> <p>Or one can plot the same in phase space:</p> <pre><code>species_3.isel(t=-1)\\\n  .plot.scatter(x=\"ux\", y=\"uy\", \n                label=species_3.attrs[\"label\"])\nspecies_4.isel(t=-1)\\\n  .plot.scatter(x=\"ux\", y=\"uy\", \n                label=species_4.attrs[\"label\"])\n</code></pre> scatter plot \\(\\{u_x,~u_y\\}\\) <p></p>"},{"location":"content/1-getting-started/5-vis/#accessing-runtime-spectra","title":"Accessing runtime spectra","text":"<p>Distribution functions for all particle species in the box are also written with the data at specified timesteps. These can be accessed via <code>data.spectra</code>, which has several different fields. As in particles &amp; fields, you can access the data at different times using <code>data.spectra.isel(t=...)</code> or <code>data.spectra.sel(t=...)</code>. The energy bins are written into <code>data.spectra.e</code>; by default, the binning is done logarithmically in \\(\\gamma - 1\\) for massive particles and energy, \\(E\\), for the photons. Below is an example script to build a distribution function of electron-positron pairs at output step <code>t=450</code>:</p> <pre><code>sp = data.spectra.isel(t=450)\n\nplt.figure(figsize=(6, 3))\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.plot(sp.e, sp.n_1 + sp.n_2, c=\"r\")\nplt.ylim(10, 3e5)\nplt.xlabel(r\"$\\gamma - 1$\")\nplt.xlim(sp.e.min(), 1)\n</code></pre> particle spectra <p></p>"},{"location":"content/1-getting-started/5-vis/#exporting-movies","title":"Exporting movies","text":"<p>To produce animations, <code>nt2py</code> provides a shortcut helper function which saves the frames using multiple threads, and then calls <code>ffmpeg</code> to merge them into a video file. </p> <pre><code>def plot_frame(ti, data):\n    # function must take two parameters:\n    # - ti: output index\n    # - data: the dataset loaded with nt2.read\n    #\n    # any type data manipulation &amp; plotting routine goes here\n    # e.g.\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    (data.N_1 + data.N_2).isel(t=ti).plot(ax=ax, cmap=\"viridis\")\n    #                      ^\n    #                 selecting timestep by index\n\n# then simply pass this function to the routine:\ndata.makeMovie(plot_frame, num_cpus=8, framerate=\"10\", ...)\n#                                   ^\n#                 (optional) by default all available threads are used\n</code></pre> <p><code>makeMovie</code> also accepts a number of arguments used by <code>ffmpeg</code>, such as the framerate, the compression rate, etc. Run the following to see all the arguments:</p> <pre><code>import nt2.export as nt2e\nnt2e.makeMovie?\n</code></pre>"},{"location":"content/1-getting-started/6-checkpoints/","title":"Writing checkpoints &amp; restarting","text":"<p> 1.2.0 </p> <p>Oftentimes, large simulations cannot be run on a single go. For these cases, Entity provides a functionality to save a so-called \"checkpoint\" (essentially, a snapshot) of the simulation at a specific timestep, which can further be used to continue the simulation from where it was left off. </p>"},{"location":"content/1-getting-started/6-checkpoints/#writing-checkpoints","title":"Writing checkpoints","text":"<p>Since the checkpoint writing relies on the <code>ADIOS2</code> library, to be able to use checkpointing, the code has to be compiled with the <code>-D output=ON</code> flag (enabled by default). Configurations for the checkpoint writing are done via the <code>.toml</code> input file under the block named <code>[checkpoint]</code> (also see the input file documentation). The following parameters control how often the checkpoint is written, as well as how many snapshots are preserved as the simulation runs.</p> parameter description default <code>interval</code> # of timesteps between checkpoints <code>1000</code> <code>interval_time</code> code-unit time between checkpoints (overrides <code>interval</code> unless <code>interval_time &lt; 0</code>) <code>-1.0</code> <code>keep</code> # of checkpoints to keep (e.g., <code>2</code> will keep the latest and the one before it, removing older ones; <code>-1</code> = keep all, <code>0</code> = disable checkpoint writing) <code>-1</code> <code>walltime</code> forces the simulation to write an additional checkpoint after a certain walltime from the beginning of the simulation (<code>\"00:00:00\"</code> = disabled) <code>\"00:00:00\"</code> <code>write_path</code> path where the checkpoints will be written to <code>\"&lt;simname&gt;.ckpt\"</code> <code>read_path</code> path from where the checkpoints will be read same as <code>write_path</code> <p>Saving space</p> <p>Since snapshots can become quite large, to save storage space, it is recommended to set the <code>keep</code> parameter small. Sometimes, it is useful to have at least one backup checkpoint (i.e., <code>keep &gt;= 2</code>), as the simulation may crash (due to, e.g., time limit on clusters) during the checkpoint writing, in which case the latest checkpoint might become corrupted. </p> <p>The simulation will then produce checkpoints of <code>BP5</code> format written in the <code>&lt;simname&gt;.ckpt/</code> directory (or the <code>write_path</code> specified in the input). Together with the data, checkpoints will also store all the parameters of the simulation in the corresponding <code>.toml</code> file.</p> <p>Large simulations with limited time allocation</p> <p>When running large simulations on clusters with scheduling systems, it is often useful to ensure at least one checkpoint exists right before the allocated time expires. To ensure that a checkpoint is written before the exit, in <code>Entity</code> you can define a <code>walltime</code> parameter mentioned above. For instance, if the expected runtime is 24 hrs, one may specify <code>23:30:00</code> to enforce writing one checkpoint 23.5 hours after the simulation begins.</p>"},{"location":"content/1-getting-started/6-checkpoints/#continuing-restarting-from-a-checkpoint","title":"Continuing (restarting) from a checkpoint","text":"<p>To restart the simulation from the latest checkpoint, simply rerun the executable <code>./entity.xc ... &lt;ARG&gt;</code>, specifying one of the following command-line arguments: <code>-continue</code>, <code>-restart</code>, <code>-resume</code>, or <code>-checkpoint</code> (all of these are equivalent). The simulation will then automatically find the newest checkpoint and continue from it. </p> <p>While most of the simulation parameters will be read from the checkpoint itself, you may also provide an input file with updated parameters (e.g., if you wish to adjust the value of some parameters). Note, however, that not all the parameters can be changed when restarting the simulation. In particular, anything related to the metric, the box extent, the resolution, or units (i.e., <code>ppc0</code>, <code>larmor0</code>, <code>skindepth0</code>) cannot be altered. These immutable parameters, if changed in the new inputfile, will simply be ignored and instead overriden by those read from the checkpoint data. Likewise, Entity current does not support changing the domain decomposition for multi-domain (i.e., MPI) simulations when resuming from a checkpoint.</p>"},{"location":"content/1-getting-started/7-docs/","title":"Editing the documentation","text":"<p><code>entity</code> documentation is automatically generated using the <code>mkdocs</code> framework and the <code>Material for mkdocs</code> theme. When you commit/push to the <code>wiki</code> branch the static website is automatically compiled and pushed to the <code>gh-pages</code> branch of the main repository.</p> <p>Hint</p> <p>Documentations are created using <code>markdown</code> syntax which is then automatically parsed and converted into <code>html</code>. As such, any <code>html</code>/<code>css</code>/<code>js</code> code you write in the markdown file will be automatically rendered. </p> <p>To add global <code>css</code> styling (using <code>scss</code> syntax), add a file into <code>sass/</code> directory and import it in <code>style.scss</code> via <code>@use 'myfile'</code>. To add external javascript, e.g., in <code>file.js</code> file, simply create the file in <code>docs/js/scripts/</code> and include it in the header of the corresponding markdown file:</p> <pre><code>---\nscripts:\n  - file\n---\n</code></pre> <p>Some third-party libraries can be included on the given page in a similar way. Below is the full list of supported ones:</p> <pre><code>---\nlibraries:\n  - d3 #(1)!\n  - p5 #(2)!\n  - mermaid #(3)!\n  - three #(4)!\n  - highlight #(5)!\n  - tikzjax #(6)!\n---\n</code></pre> <ol> <li>vector graphics, schemes and diagrams with <code>d3js</code></li> <li>interactive visualizations and easy WebGL access via <code>p5js</code></li> <li>rendering of <code>mermaid</code> diagrams</li> <li>3D visualizations using WebGL in <code>threejs</code></li> <li>code highlighting with <code>highlight.js</code></li> <li><code>Tikz</code> diagrams via <code>TikzJax</code> framework</li> </ol> <p><code>Katex</code> (for rendering \\(\\LaTeX\\)) is automatically added to all pages.</p>"},{"location":"content/1-getting-started/7-docs/#workflow","title":"Workflow","text":"<ol> <li> <p>Pull the <code>wiki</code> branch of the main repository (it is recommended to do this in a separate directory from the main code).   <pre><code>git clone git@github.com:entity-toolkit/wiki.git entity-wiki\ncd entity-wiki\n</code></pre></p> </li> <li> <p>Create an isolated python virtual environment and activate it.   <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> </li> <li> <p>Install the dependencies (everything is installed locally in the <code>.venv</code> directory).   <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Install all the <code>node</code> packages using (from the root directory):   <pre><code>npm i\n</code></pre></p> <p>If you don't have <code>npm</code>, try the following instruction.</p> </li> <li> <p>Start the reactive server that will generate the website and will dynamically update any changes made to the documentation.   <pre><code>npm run dev\n</code></pre>   To access the documentation simply open the <code>http://127.0.0.1:8000/</code> in your browser. </p> <p>The <code>npm</code> command also compiles the <code>sass</code> styles into <code>css</code>, and minifies/copies the <code>js</code> libraries, placing them in the <code>docs/js/vendor</code> directory.</p> </li> <li> <p>When satisfied with all the changes made simply push them to the <code>master</code> branch.   <pre><code>git add .\ngit commit -m \"&lt;reasonable comment&gt;\"\ngit push\n</code></pre>   Shortly after that, <code>github-actions</code> will generate the website and push it to the <code>gh-pages</code> branch of the main repository, which will be accessible from the web.</p> </li> </ol>"},{"location":"content/1-getting-started/8-docker/","title":"Docker containers","text":"<p> 1.1.0 </p> <p><code>Entity toolkit</code> provides docker images for setting up a development environment with all the dependencies already preinstalled. To start a container with the proper image, simply go to the root of the source code, and run: </p> <pre><code>docker compose run entity-&lt;tag&gt;\n</code></pre> <p>where <code>&lt;tag&gt;</code> is the particular configuration (i.e., specific toolkit) you wish to utilize (see below). This command will also mount the current directory (i.e., the root of the source code) to the home directory inside the container.</p> <p>All the containers also come with a python environment set up and some of the necessary packages (e.g., <code>nt2py</code>, <code>jupyter</code>, etc.) already preinstalled, and it also forwards the port <code>8080</code> to the <code>localhost:8080</code>, so one could even run a post-processing server from within the container.</p> <p>Docker images are large</p> <p>Note that docker image sizes are typically quite large (few GB). So keep that in mind when pulling the image using <code>docker compose run</code> or <code>docker build</code>.</p>"},{"location":"content/1-getting-started/8-docker/#runtime","title":"Runtime","text":"<p>Often times you may want to simply compile the code within the container, and not run it (e.g. for testing purposes, or if your system does not have a GPU with the proper architecture). In that case there is no need to give docker access to the GPUs on your system, and thus no additional configuration is needed. You will simply use the tags with <code>_compilers</code> which do not require any custom runtimes installed. You may still be able to access the executable outside of the container and even run it.</p> <p>To actually run the code from within the container, you will need to provide docker access to the GPU via the corresponding container toolkit. For NVIDIA GPUs, you will need the NVIDIA Container Toolkit installed on your system. For AMD GPUs, you will need <code>amdgpu-dkms</code> which typically comes pre-packaged with ROCm.</p> <p>NVIDIA Container Toolkit on WSL</p> <p>When installing the toolkit via WSL, you will need to configure docker on the Windows side, not the WSL (since WSL simply uses the daemon from Windows). Use the following command, instead of the one specified in the instructions:</p> <pre><code>sudo nvidia-ctk runtime configure --runtime=docker --config /mnt/&lt;windows drive&gt;/Users/&lt;username&gt;/.docker/daemon.json\n</code></pre>"},{"location":"content/1-getting-started/8-docker/#tags","title":"Tags","text":"<p>We provide the following configurations (tags) as separate images:</p> tag compilers runtime download size <code>cuda</code> <code>gcc 11.4</code> + <code>CUDA toolkit 12.2</code> Y 4.55 GB <code>cuda-compilers</code> <code>gcc 11.4</code> + <code>CUDA toolkit 12.2</code> N 4.55 GB <code>rocm</code> <code>gcc 9.4</code> + <code>ROCm 6.1.2</code> Y 1.75 GB <code>rocm-compilers</code> <code>gcc 9.4</code> + <code>ROCm 6.1.2</code> N 1.75 GB <code>sycl</code> to-be-released <p><code>runtime</code> indicates that the particular container is also able to execute the code on the GPU (special docker toolkit needed, as noted above). All of the tags can be accessed through the docker hub under <code>morninbru/entity:&lt;tag&gt;</code>.</p>"},{"location":"content/1-getting-started/8-docker/#building-images-on-your-own","title":"Building images on your own","text":"<p>Images for these containers are all stored on the Docker hub. If you do not wish to use <code>docker compose</code> to download the pre-made ones from the Docker hub, you may also build the images yourself from the corresponding <code>Dockerfile</code>-s also provided with the source code. You can do that by going to the <code>dev/</code> directory in the root of the source code, and running: </p> <pre><code>docker build --no-cache -t myentity:&lt;toolkit&gt; -f Dockerfile.&lt;toolkit&gt; .\n</code></pre> <p>substituting one of the values for the <code>&lt;toolkit&gt;</code> mentioned above. You may then launch a container using the built image by running the following from the code source directory (or any other directory you wish to mount inside the container):</p> <pre><code>docker run -it \\\n# uncomment one of the two lines below to give container...\n# ... access to the GPU (see above). otherwise, if no runtime access needed...\n# ... leave the lines commented\n#  --runtime=nvidia --gpus all \\ # &lt; NVIDIA cards\n#  --device /dev/kfd --device /dev/dri --security-opt seccomp=unconfined \\ # &lt; AMD cards\n  --name myentity_&lt;tag&gt; \\\n  -p 8080:8080 \\\n  -v \"$(pwd)\":/home/&lt;MOUNT_DIRECTORY&gt;/ \\\n  myentity:&lt;tag&gt;\n</code></pre> <p>where <code>MOUNT_DIRECTORY</code> is the directory within the container, where the current directory on the host system will be mounted. The <code>&lt;runtime&gt;</code> has to correspond the tag being used. For example, for <code>CUDA</code> on NVIDIA GPUs you will typically use <code>--runtime=nvidia</code>.</p>"},{"location":"content/1-getting-started/9-faq/","title":"F.A.Q.","text":"<p>tl;dr</p> <p>Here we collect the most frequent questions that might occur. Please, make sure to inspect this section before filing a GitHub issue.</p>"},{"location":"content/1-getting-started/9-faq/#code-usage","title":"Code usage","text":"<p>I want to have a custom boundary/injection/driving/distribution function/output.</p> <p>All of that can be done via the tools provided by the problem generator. Please inspect carefully the section dedicated to that. Also have a look at the set of officially supported problem generators some of which might implement a variation of what your original intent is.</p>"},{"location":"content/1-getting-started/9-faq/#technical","title":"Technical","text":"<p>Running in a <code>docker</code> container with an AMD card</p> <p>AMD has a vary brief documentation on the topic. In theory the <code>docker</code> containers that come with the code should work. Just make sure you have the proper groups (<code>render</code> and <code>video</code>) defined and added to the current user. If it complains about access to <code>/dev/kfd</code>, You might have to run docker as a root.</p> <p>Compilation errors</p> <p>Before merging with the released stable version, the code is tested on CUDA and HIP GPU compilers, as well as few version of CPU compilers (GCC 9...11, and LLVM 13...17). If you are encountering compiler errors on GPUs, first thing to check is whether the compilers are set up properly (i.e., whether CMake indeed captures the right compilers). Here are a few tips:</p> <ul> <li> <p>CUDA @ NVIDIA GPUs: make sure you have a version of <code>gcc</code> which is supported by the version of CUDA you are using; check out this unofficial compatibility matrix. In particular, Intel compilers are not very compatible with CUDA, and it is recommended to use <code>gcc</code> instead (you won't gain much by using Intel anyway, since CUDA will be doing the heavy-lifting).</p> </li> <li> <p>HIP/ROCm @ AMD GPUs: ROCm library is a headache. The documentation is even more so. We have a dedicated section specifically discussing compilation with HIP. Make sure to check it before opening an issue.</p> </li> </ul> <p>If the code gives an error, how do I know whether the problem is with the Entity itself or with the other libraries it depends on (e.g., <code>Kokkos</code>, <code>ADIOS2</code>, <code>MPI</code>)?</p> <p>One good way of narrowing the problem down, is to run the so-called minimal examples, provided in the directory called <code>minimal/</code> in the source. It has detailed instructions on how to compile these examples, and should hopefully be able to verify whether all the installed dependencies work as expected, before looking for an issue in the Entity itself. </p> <p>Another good way is to compile the code with <code>-D TESTS=ON</code> flag, which will compile all the unit tests, and you can run them one-by-one. You may also compile the tests also with various flags, e.g., <code>-D mpi=ON</code>, <code>-D output=ON</code>, <code>-D precision=double</code>.</p>"},{"location":"content/2-code/1-pic/","title":"PIC Algorithm","text":""},{"location":"content/2-code/1-pic/#31-formalism","title":"3+1 Formalism","text":"<p>To facilitate both GR and non-GR equations in a single framework while still retaining the maximum level of generality, we employ the 3+1 formalism for projecting the space-time (see, e.g., Gourgoulhon, 2012).</p> <p>In the most general 3+1 formulation, the metric and its inverse in arbitrary coordinate system can be represented in the following form:</p> \\[\\begin{align*} g_{\\mu\\nu}&amp;=\\begin{bmatrix} -\\alpha^2+\\beta_k\\beta^k &amp; \\beta_i \\\\ \\beta_j &amp; h_{ij} \\end{bmatrix}\\\\\\\\ g^{\\mu\\nu}&amp;=\\begin{bmatrix} -1/\\alpha^2 &amp; \\beta^i/\\alpha^2 \\\\ \\beta^j/\\alpha^2 &amp; h^{ij}-\\beta^i\\beta^j/\\alpha^2 \\end{bmatrix} \\end{align*}\\] <p>where \\(\\alpha\\) is the metric lapse function, and \\(\\beta_i\\) is the metric shift vector. We will also denote \\(h \\equiv \\mathrm{det}{(h_{ij})}\\). Notice also, that \\(g\\equiv \\mathrm{det}{(g_{\\mu\\nu})} = -\\alpha^2 h\\), and \\(\\sqrt{-g}=\\alpha\\sqrt{h}\\).</p> <p>In this system, we can now express the curl of an arbitrary contravariant vector:</p> \\[ (\\nabla\\times A)^i \\equiv \\frac{1}{\\sqrt{h}}\\varepsilon^{ijk}\\partial_j A_k = \\frac{1}{\\sqrt{h}}\\varepsilon^{ijk}\\partial_j h_{kp}A^p \\] <p>where \\(A_k = h_{kp} A^p\\), and \\(\\varepsilon^{ijk}\\) is the Levi-Civita symbol.</p>"},{"location":"content/2-code/1-pic/#maxwells-equations","title":"Maxwell's equations","text":"<p>In the general 3+1 formulation (special and general relativistic) there are two \"physical\" fields (the ones measured by fiducial observers, FIDOs), \\(B^i\\), and \\(D^i\\). Evolution equations on these two fields can be written as:</p> \\[ \\begin{aligned} \\frac{\\partial D^i}{c\\partial t} &amp;= \\frac{1}{\\sqrt{h}} \\varepsilon^{ijk}\\partial_j H_k - \\frac{4\\pi}{c} J^i \\\\ \\frac{\\partial B^i}{c\\partial t} &amp;= -\\frac{1}{\\sqrt{h}} \\varepsilon^{ijk}\\partial_j E_k \\end{aligned} \\] <p>where \\(H_i\\), and \\(E_i\\) are the covariant auxiliary fields, defined as follows:</p> \\[ \\begin{aligned} H_i &amp;= \\alpha h_{ij} B^j - \\frac{1}{\\sqrt{h}}\\varepsilon_{ijk}\\beta^j D^k \\\\ E_i &amp;= \\alpha h_{ij} D^j + \\frac{1}{\\sqrt{h}}\\varepsilon_{ijk}\\beta^j B^k \\end{aligned} \\] <p>Flat space-time with diagonal metric</p> <p>In flat space-time with diagonal metric, \\(\\alpha=1\\), and \\(\\beta^i=0\\), we have \\(D_i = E_i = h_{ij} E^j\\), and \\(B_i = H_i = h_{ij} B^j\\). Maxwell's equations then reduce to a closed form:</p> \\[ \\begin{aligned} \\frac{\\partial E^i}{c\\partial t} &amp;= \\frac{1}{\\sqrt{h}} \\varepsilon^{ijk}\\partial_j B_k - \\frac{4\\pi}{c} J^i \\\\ \\frac{\\partial B^i}{c\\partial t} &amp;= -\\frac{1}{\\sqrt{h}} \\varepsilon^{ijk}\\partial_j E_k \\end{aligned} \\] <p>If we further assume that the coordinate system is also flat, \\(h_{ij}=\\delta_{ij}\\), we get the more familiar form:</p> \\[ \\begin{aligned} \\frac{\\partial E^i}{c\\partial t} &amp;= (\\nabla\\times \\bm{B})^i - \\frac{4\\pi}{c} J^i \\\\ \\frac{\\partial B^i}{c\\partial t} &amp;= -(\\nabla\\times\\bm{E})^i \\end{aligned} \\]"},{"location":"content/2-code/1-pic/#equations-of-motion-for-particles","title":"Equations of motion for particles","text":"<p>The equation of motion for relativistic particles in such a 3+1 formulation have the following form:</p> \\[ \\begin{aligned} \\frac{d x^i}{cd t} &amp;= \\alpha \\frac{1}{\\gamma}h^{ij} u_j - \\beta^i \\\\ \\frac{d u_i}{cd t} &amp;= \\underbrace{   -\\gamma \\partial_i \\alpha + u_j\\partial_i \\beta^j - \\frac{\\alpha}{2\\gamma} u_j u_k\\partial_i h^{jk} }_{\\text{``curvature'' force}} + \\underbrace{   \\frac{q}{m c^2}\\alpha\\left(h_{ij}D^j + \\frac{1}{\\sqrt{h}\\gamma}\\varepsilon_{ijk}h^{jl} u_l B^k\\right) }_{\\text{Lorentz force}} \\end{aligned} \\] <p>Here \\(u_i\\) are the three components of the particle's covariant four-velocity, \\(x^i\\) is its position, \\(\\gamma = \\sqrt{\\varepsilon+h^{ij}u_{i}u_j}\\) is the energy of the particle (\\(\\varepsilon\\equiv 1\\) for massive particles, and \\(\\varepsilon\\equiv 0\\) for photons), \\(h_{ij}\\), \\(h^{ij}\\), \\(\\sqrt{h}\\) as well as \\(\\alpha\\), and \\(\\beta^i\\) are the metric coefficients at particle's position. \\(D^i\\) and \\(B^i\\) are the contravariant field components also measured at particle's position.</p>"},{"location":"content/2-code/1-pic/#covariant-current-deposition","title":"Covariant current deposition","text":"<p>Charged particles deposit currents, \\(\\bm{J}^i\\), that go into Maxwell's equations as source terms. In general \\(\\bm{J}^i = \\rho \\bm{\\beta}^i c\\), where \\(\\rho\\) is the charge density measured in the lab frame. Coupled with the equations of motion and Maxwell's equations, this relation ensures exact charge conservation, i.e., \\(\\nabla\\cdot \\bm{J}^\\mu\\equiv \\bm{J}^\\mu_{;\\mu} = 0\\), where \\(\\bm{J}^\\mu\\) is the contravariant four-current with \\(J^0 = \\rho c\\). Substituting \\(\\tilde{\\rho}\\equiv \\sqrt{h}\\rho\\), and \\(\\bm{\\mathcal{J}}^i\\equiv \\sqrt{h}\\bm{J}^i\\), we can rewrite the charge conservation in a more concise form:</p> \\[ \\frac{\\partial \\tilde{\\rho}}{\\partial t} + \\partial_i \\bm{\\mathcal{J}}^i = 0 \\] <p>Thus, depositing currents as \\(\\bm{\\mathcal{J}}^i\\) and then converting to \\(\\bm{J}^i=\\bm{\\mathcal{J}}^i/\\sqrt{h}\\) one can ensure that the exact charge conservation is maintained.</p>"},{"location":"content/2-code/1-pic/#special-relativistic-pic","title":"Special-relativistic PIC","text":"<p>For the non-GR case we use an explicit leapfrog integrator for both fields and the particles. All the fields, as well as particle coordinates/velocities are defined in the general curvilinear (orthonormal) coordinate system.</p> 0. initial configuration1. first Faraday half-step2.1. particle velocity update2.2. particle coordinate update3. Current deposition4.1. second Faraday half-step4.2. Ampere substep5. final configuration <p></p> \\[ t=t^{(n)} \\] <p></p> \\[ \\frac{1}{c}\\frac{\\partial B^i}{\\partial t} = -\\frac{1}{\\sqrt{h}}\\varepsilon^{ijk}\\partial_j\\left(h_{kp} E^p\\right) \\] \\[ B^{(n-1/2)}\\xrightarrow[\\qquad E^{(n)}\\qquad]{\\Delta t/2} B^{(n)} \\] <p></p> \\[ u^{(n-1/2)}\\xrightarrow[\\qquad E(x^n),~B(x^n)\\qquad]{\\Delta t} u^{(n+1/2)} \\] <p></p> \\[ \\frac{\\mathrm{d} x_i}{\\mathrm{d} t} = \\frac{u_i}{\\gamma} \\] \\[ x^{(n)}\\xrightarrow[\\qquad u^{(n+1/2)}\\qquad]{\\Delta t} x^{(n+1)} \\] <p></p> <p></p> \\[ \\frac{1}{c}\\frac{\\partial B_i}{\\partial t} = -\\frac{1}{\\sqrt{h}}\\varepsilon^{ijk}\\partial_j\\left(h_{kp} E^p\\right) \\] \\[ B^{(n)}\\xrightarrow[\\qquad E^{(n)}\\qquad]{\\Delta t/2} B^{(n+1/2)} \\] <p></p> \\[ \\frac{1}{c}\\frac{\\partial E_i}{\\partial t} = \\frac{1}{\\sqrt{h}}\\varepsilon^{ijk}\\partial_j\\left(h_{kp} B^p\\right) - \\frac{4\\pi}{c} J^i \\] \\[ E^{(n)}\\xrightarrow[\\qquad B^{(n+1/2)},~J^{(n+1/2)}\\qquad]{\\Delta t} E^{(n+1)} \\] <p></p> \\[ t=t^{(n+1)} \\]"},{"location":"content/2-code/1-pic/#particle-pusher-in-sr","title":"Particle pusher in SR","text":"<p>The pseudocode below roughly illustrates the particle pusher algorithm in SR.</p> <pre><code>// em &lt;-- 4D array of e/b-fields (encode 3 dimensions and the component of the field)\n// species &lt;-- 1D array of species\n// metric &lt;-- metric object\n// dt &lt;-- timestep\n\n// prtl.x: coordinates in code units @ t^n (1)\n// prtl.u: 4-velocities in the global Cartesian basis @ t^(n-1/2)\n\nfor spec := range species {\n  q_ovr_m := spec.charge / spec.mass\n  for prtl := range spec.prtls { //(2)!\n    if !prtl.is_alive {\n      continue\n    }\n    if spec.is_massive { //(3)!\n      // 1. interpolate contravariant fields to particle position\n      eU, bU := interpolate(em, prtl.x)\n\n      // 2. convert to global XYZ coordinates \n      e, b := metric.transform_xyz[Idx::U, Idx::XYZ](eU, bU) // (4)!\n\n      // 3. update particle momentum (e.g., using Boris algorithm)\n      prtl.u = updateMomentum(prtl.u, e, b, q_ovr_m, dt)\n\n      // 4. get 3-velocity\n      v = prtl.u / sqrt(1 + prtl.u**2)\n    } else {\n      // 4. get 3-velocity\n      v = prtl.u / sqrt(prtl.u**2)\n    }\n    // 5. record the old position\n    prtl.x_old = prtl.x\n\n    // 6. convert the coordinates to Cartesian basis\n    x = metric.convert_xyz[Crd::Cd, Crd::XYZ](prtl.x)\n\n    // 7. update the position\n    x += v * dt\n\n    // 8. convert back to code basis\n    prtl.x = metric.convert_xyz[Crd::XYZ, Crd::Cd](x)\n\n    // 9. apply boundary conditions\n    prtl.x, prtl.u, prtl.x_old = boundary_conditions(prtl.x, prtl.u, prtl.x_old) // (5)!\n  }\n}\n\n// at the end of the loop we have\n// prtl.x_old: coordinates @ t^n\n// prtl.x: coordinates @ t^(n+1)\n// prtl.u: 4-velocities @ t^(n+1/2)\n</code></pre> <ol> <li> In the actual code, we store particle coordinates as an index of the cell the particle is in plus a displacement.</li> <li> In reality, we use a structure of arrays, instead of an array of structures. Here we use a simplified notation for clarity.</li> <li> In the code, of course, we minimize the amount of runtime <code>if</code> statements by using compile-time <code>constexpr if</code>-s and template arguments.</li> <li> If there are external forces acting on the particle, or a drag force, we include them in the next step.</li> <li> Here we include absorption, reflection from the axis, periodic boundaries, etc. Communication with other domains is happeing at a different place.</li> </ol>"},{"location":"content/2-code/1-pic/#general-relativistic-pic","title":"General-relativistic PIC","text":"0. initial configuration1.1. intermediate interpolation1.2. auxiliary field recovery1.3. auxiliary Faraday substep2. particle push3. current deposition4.1. auxiliary field recovery4.2. Faraday substep4.3. intermediate current interpolation4.4. auxiliary Ampere substep4.5. auxiliary field recovery4.6. Ampere substep5. final configuration \\[ t=t^{(n)} \\] \\[ \\begin{aligned} D^{(n-1/2)} &amp;= \\frac{1}{2}\\left(D^{(n-1)}+D^{(n)}\\right)\\\\\\\\ B^{(n-1)} &amp;= \\frac{1}{2}\\left(B^{(n-3/2)}+B^{(n-1/2)}\\right) \\end{aligned} \\] \\[ E^{(n-1/2)} = \\alpha D^{(n-1/2)} + \\beta\\times B^{(n-1/2)} \\] \\[ B^{(n-1)}\\xrightarrow[\\qquad E^{(n-1/2)}\\qquad]{\\Delta t} B^{(n)} \\] \\[ E^{(n)} = \\alpha D^{(n)} + \\beta\\times B^{(n)} \\] \\[ H^{(n)} = \\alpha B^{(n)} - \\beta\\times D^{(n)} \\] \\[ B^{(n-1/2)}\\xrightarrow[\\qquad E^{(n)}\\qquad]{\\Delta t} B^{(n+1/2)} \\] \\[ J^{(n)} = \\frac{1}{2}\\left(J^{(n-1/2)}+J^{(n+1/2)}\\right) \\] \\[ D^{(n-1/2)}\\xrightarrow[\\qquad H^{(n)},~J^{(n)}\\qquad]{\\Delta t} D^{(n+1/2)} \\] \\[ H^{(n+1/2)} = \\alpha B^{(n+1/2)} - \\beta\\times D^{(n+1/2)} \\] \\[ D^{(n)}\\xrightarrow[\\qquad H^{(n+1/2)},~J^{(n+1/2)}\\qquad]{\\Delta t} D^{(n+1)} \\] \\[ t=t^{(n+1)} \\]"},{"location":"content/2-code/1-pic/#particle-pusher-in-gr","title":"Particle pusher in GR","text":"<p>Particle pusher in GR is slightly more complicated, as we can no longer transform into a global Cartesian frame, and thus unavoidably have to deal with the \"geodesic\" term in the equation of motion.</p> <pre><code>// em &lt;-- 4D array of d/b-fields, with d defined @ t^n (1)\n// em0 &lt;-- 4D array of d/b-fields, with b defined @ t^n\n// species &lt;-- 1D array of species\n// metric &lt;-- metric object\n// dt &lt;-- timestep\n\n// prtl.x: coordinates in code units @ t^n\n// prtl.u: 4-velocities in code-covariant basis @ t^(n-1/2)\n\nfor spec := range species {\n  q_ovr_m := spec.charge / spec.mass\n  for prtl := range spec.prtls {\n    if !prtl.is_alive {\n      continue\n    }\n    if spec.is_massive {\n      // 1. interpolate contravariant fields to particle position\n      eU, bU := interpolate(em, em0, prtl.x)\n\n      // 2. convert to local tetrad basis\n      eT, bT := metric.transform[Idx::U, Idx::T](eU, bU)\n\n      // 3. get the velocity in tetrad basis\n      uT := metric.transform[Idx::D, Idx::T](prtl.u)\n\n      // 4. update particle momentum with electromagnetic-push (half step)\n      uT = updateMomentum(uT, eT, bT, q_ovr_m, dt / 2)\n\n      // 5. transform the velocity back to code-covariant basis\n      uD := metric.transform[Idx::T, Idx::D](uT)\n\n      // 6. update the momentum using a full geodesic push\n      uD = geodesicMomentumUpdate(prtl.x, uD, dt)\n\n      // 7. transform again to tetrad basis\n      uT = metric.transform[Idx::D, Idx::T](uD)\n\n      // 8. update with another half-step electromagnetic push\n      uT = updateMomentum(uT, eT, bT, q_ovr_m, dt / 2)\n\n      // 9. transform back to code-covariant basis\n      prtl.u = metric.transform[Idx::T, Idx::D](uT)\n\n      // 10. record the old position\n      prtl.x_old = prtl.x\n\n      // 11. update the coordinate using the geodesic equation\n      prtl.x = geodesicPositionUpdate(prtl.x, prtl.u, dt)\n\n      // 12. apply boundary conditions\n      prtl.x, prtl.u, prtl.x_old = boundary_conditions(prtl.x, prtl.u, prtl.x_old)\n    } else {\n      // 1. update the momentum using geodesic push\n      prtl.u = geodesicMomentumUpdate(prtl.x, prtl.u, dt)\n\n      // 2. update the coordinate using geodesic push\n      prtl.x = geodesicPositionUpdate(prtl.x, prtl.u, dt)\n\n      // 3. apply boundary conditions\n      prtl.x, prtl.u = boundary_conditions(prtl.x, prtl.u)\n    }\n  }\n}\n\n// at the end of the loop we have\n// prtl.x_old: coordinates @ t^n\n// prtl.x: coordinates @ t^(n+1)\n// prtl.u: 4-velocities @ t^(n+1/2)\n</code></pre> <ol> <li> To save memory, we store the fields at two time levels, and two different 4D arrays. Here we want to ensure we are passing the fields at time <code>t^n</code>. See step #2 in the diagram above.</li> </ol>"},{"location":"content/2-code/2-hierarchy/","title":"Hierarchy of structures","text":"<pre>\nclassDiagram\n  direction TB\n  class Metadomain~SimEngine, class~ {\n    see metadomain...*\n  }\n  class Domain~SimEngine, class~ {\n    see domain...*\n  }\n  class Mesh~class~ {\n    see mesh...*\n  }\n  class Grid~Dimension~ {\n    see grid...*\n  }\n  class MetricBase~Dimension~ {\n    see metrics...*\n  }\n  class ParticleSpecies {\n    see species...*\n  }\n  class Particles~Dimension, Coord~ {\n    see particles...*\n  }\n  class Fields~Dimension, SimEngine~ {\n    see fields...*\n  }\n\n  Domain --* Mesh : contains\n  Grid &lt;|-- Mesh : inherits\n  Mesh --* MetricBase : contains\n  Metadomain --* Domain : contains many\n  Metadomain --* Mesh : contains\n  Domain --* Fields : contains\n  Domain --* Particles : contains many\n  ParticleSpecies &lt;|-- Particles : inherits\n  Mesh --* MetricBase : contains\n\n\n</pre>"},{"location":"content/2-code/3-domains/","title":"Metadomain, subdomains and meshes","text":"<p>Relevant headers</p> <ul> <li><code>framework/domain/metadomain.h</code></li> <li><code>framework/domain/domain.h</code></li> <li><code>framework/domain/grid.h</code></li> <li><code>framework/domain/mesh.h</code></li> </ul> <p>The main object which contains all the data and configuration of the simulation, including domain decomposition, the mesh discretization, the metric, the fields and the particles is the <code>Metadomain&lt;S,M&gt;</code> class (where <code>S</code> is the simulation engine template argument, and <code>M</code> is the metric class). From the metadomain object, we can access all the subdomains <code>Metadomain&lt;S,M&gt;::subdomain(idx) -&gt; const Domain&lt;S,M&gt;&amp;</code> (<code>::subdomain_ptr(idx) -&gt; Domain&lt;S,M&gt;*</code>; <code>idx</code> is the index of the subdomain), the global mesh <code>Metadomain&lt;S,M&gt;::mesh() -&gt; const Mesh&lt;M&gt;&amp;</code>, and the global metric <code>Metadomain&lt;S,M&gt;::mesh().metric -&gt; const Metric&lt;D&gt;&amp;</code>. Each subdomain also contains a mesh and a metric object, which can be accessed with <code>Domain&lt;S,M&gt;::mesh</code> and <code>Domain&lt;S,M&gt;::mesh.metric</code>. Fields and particle species are stored for each subdomain: <code>Domain&lt;S,M&gt;::fields</code> and <code>Domain&lt;S,M&gt;::species</code>. </p> <p>Thus, the hierarchy of data objects is as follows:</p> <pre><code>Metadomain&lt;S,M&gt;\n\u251c\u2500\u2500 Mesh&lt;M&gt; : g_mesh\n\u2514\u2500\u2500 [Domain&lt;S,M&gt;, Domain&lt;S,M&gt;, ...] : g_subdomains\n     \u251c\u2500\u2500 Mesh&lt;M&gt; : mesh\n     \u2502   \u2514\u2500\u2500 Metric&lt;D&gt; : metric\n     \u251c\u2500\u2500 Fields&lt;D,S&gt; : fields\n     \u2514\u2500\u2500 [Particles&lt;D,S&gt;, ...] : species\n</code></pre> <p>Remember, that the local code-unit coordinates in Entity go from \\(0\\) to \\(n_i\\) (where \\(n_i\\) is the number of cells in the \\(i\\)-th direction). Because of that, metrics and coordinate systems on all subdomains can be different. However, the code guarantees that the transitions between the subdomains is continuous, and no vector transformations are needed when passing from one subdomain to another (coordinate transformations still have to be done).</p> <p>Local vs non-local subdomains</p> <p>While the metadomain object which exists on all MPI ranks contains information on all the subdomains of the global simulation, the data for fields and particles is only allocated for the so-called \"local\" subdomains, owned by the current MPI rank. To get the indices of the subdomains on the local MPI rank, use <code>Metadomain&lt;S,M&gt;::local_subdomain_indices() -&gt; std::vector&lt;unsigned int&gt;</code>. To run a specific function on all the local subdomains use the following lambda construct: <pre><code>metadomain.runOnLocalDomains([&amp;](auto&amp; loc_dom) {\n  // do something with loc_dom\n});\n</code></pre> If you do not plan to modify the local subdomains, you should use the <code>const</code> version of this function: <pre><code>metadomain.runOnLocalDomainsConst([&amp;](auto&amp; loc_dom) {\n  // do something with loc_dom without modifying it\n});\n</code></pre> Non-local subdomains are also referred to as placeholders, and one could check whether a given subdomain is a placeholder using the built-in <code>is_placeholder()</code> method.</p> <p>The diagram below demonstrates the structure of the metadomain object, the subdomains, and the mesh with all the contained variables and methods. Fields and particles are described in the following section, while the metrics are discussed here.</p> <pre>\nclassDiagram\n  class MetricBase~Dimension~ {\n    see metrics...*\n  }\n  class Fields~Dimension, SimEngine~ {\n    see fields...*\n  }\n  class Particles~Dimension, Coord~ {\n    see particles...*\n  }\n  class Metadomain~SimEngine, class~ {\n    +Dimension D$\n    -uint g_ndomains\n    -vector~int~ g_decomposition\n    -vector~uint~ g_ndomains_per_dim\n    -vector~vector~uint~~ g_domain_offsets\n    -map~vector~uint~, uint~ g_domain_offset2index\n    -vector~Domain~S, M~~ g_subdomains\n    -vector~uint~ g_local_subdomain_indices\n    -Mesh~M~ g_mesh\n    -const map~string, real_t~ g_metric_params\n    -const vector~ParticleSpecies~ g_species_params\n    -stats\\:\\:Writer g_stats_writer\n    +initialValidityCheck() void\n    +finalValidityCheck() void\n    +metricCompatibilityCheck() void\n    +createEmptyDomains() void\n    +redefineNeighbors() void\n    +redefineBoundaries() void\n    +CommunicateFields(Domain~S, M~ &amp;, CommTags ) void\n    +SynchronizeFields(Domain~S, M~ &amp;, CommTags ) void\n    +CommunicateParticles(Domain~S, M~ &amp;) void\n    +RemoveDeadParticles(Domain~S, M~ &amp;) void\n    +InitStatsWriter(SimulationParams &amp;, bool ) void\n    +WriteStats(SimulationParams &amp;, timestep_t , timestep_t , simtime_t , simtime_t ) bool\n    +setFldsBC(bc_in &amp;, FldsBC &amp;) void\n    +setPrtlBC(bc_in &amp;, PrtlBC &amp;) void\n    +ndomains() uint\n    +ndomains_per_dim() vector~uint~\n    +subdomain(uint idx) const Domain~S, M~&amp;\n    +subdomain_ptr(uint idx) Domain~S, M~*\n    +mesh() const Mesh~M~&amp;\n    +species_params() const vector~ParticleSpecies~&amp;\n    +l_subdomain_indices() vector~uint~\n    +l_npart_perspec() vector~npart_t~\n    +l_maxnpart_perspec() vector~npart_t~\n    +l_npart() size_t\n    +l_ncells() size_t\n    +species_labels() vector~string~\n  }\n  class Domain~SimEngine, class~ {\n    +Dimension D$\n    +Mesh~M~ mesh\n    +Fields~D, S~ fields\n    +vector~Particles~D, M\\:\\:CoordType~~ species\n    +random_number_pool_t random_pool\n    -uint m_index\n    -vector~uint~ m_offset_ndomains\n    -vector~ncells_t~ m_offset_ncells\n    -dir\\:\\:map_t~D, uint~ m_neighbor_idx\n    -int m_mpi_rank\n    +index() uint\n    +offset_ndomains() vector~uint~\n    +offset_ncells() vector~ncells_t~\n    +neighbor_idx_in(dir\\:\\:direction_t~D~ &amp; dir) uint\n    +is_placeholder() bool\n    +set_neighbor_idx(dir\\:\\:direction_t~D~ &amp; dir, uint idx) void\n  }\n  class Grid~Dimension~ {\n    #vector~ncells_t~ m_resolution\n    +i_min(in i) ncells_t\n    +i_max(in i) ncells_t\n    +n_active(in i) ncells_t\n    +n_active() vector~ncells_t~\n    +num_active() ncells_t\n    +n_all(in i) ncells_t\n    +n_all() vector~ncells_t~\n    +num_all() ncells_t\n    +rangeActiveCells() range_t~D~\n    +rangeAllCells() range_t~D~\n    +rangeCells(box_region_t~D~ &amp;) range_t~D~\n    +rangeCells(tuple_t~list_t~int, 2~, D~ &amp;) range_t~D~\n    +rangeActiveCellsOnHost() range_h_t~D~\n    +rangeAllCellsOnHost() range_h_t~D~\n    +rangeCellsOnHost(box_region_t~D~ &amp;) range_h_t~D~\n  }\n  class Mesh~class~ {\n    +bool is_mesh$\n    +Dimension D$\n    +M metric\n    -boundaries_t~real_t~ m_extent\n    -dir\\:\\:map_t~D, FldsBC~ m_flds_bc\n    -dir\\:\\:map_t~D, PrtlBC~ m_prtl_bc\n    +Intersection(boundaries_t~real_t~ box) boundaries_t~real_t~\n    +Intersects(boundaries_t~real_t~ box) bool\n    +ExtentToRange(boundaries_t~real_t~ box, boundaries_t~bool~ incl_ghosts) boundaries_t~ncells_t~\n    +extent(in i) real_t,real_t\n    +extent() boundaries_t~real_t~\n    +flds_bc() boundaries_t~FldsBC~\n    +prtl_bc() boundaries_t~PrtlBC~\n    +flds_bc_in(dir\\:\\:direction_t~D~ &amp; direction) FldsBC\n    +prtl_bc_in(dir\\:\\:direction_t~D~ &amp; direction) PrtlBC\n    +set_flds_bc(dir\\:\\:direction_t~D~ &amp; direction, FldsBC &amp; bc) void\n    +set_prtl_bc(dir\\:\\:direction_t~D~ &amp; direction, PrtlBC &amp; bc) void\n  }\n\n  Domain --* Mesh : contains\n  Grid &lt;|-- Mesh : inherits\n  Mesh --* MetricBase : contains\n  Metadomain --* Domain : contains many\n  Metadomain --* Mesh : contains\n  Domain --* Fields : contains\n  Domain --* Particles : contains many\n\n  note \"+: public\\n-: private\\n#: protected\\nunderline: static constexpr\\nitalic: virtual\"\n\n</pre>"},{"location":"content/2-code/4-fields_particles/","title":"Fields and particles","text":"<p>Relevant headers</p> <ul> <li><code>framework/containers/fields.h</code></li> <li><code>framework/containers/species.h</code></li> <li><code>framework/containers/particles.h</code></li> </ul> <p>To store the main data of the simulation, the fields and the particles, Entity provides container classes <code>Fields&lt;D,S&gt;</code> and <code>Particles&lt;D,C&gt;</code>, where <code>D</code> is a template argument for the dimension, <code>S</code> for the simulation engine, and <code>C</code> for the coordinate type. Notice, that none of these objects know anything about the geometry of the space-time (i.e., the metric), and thus they should be used in conjunction with the <code>Mesh&lt;M&gt;</code> object discussed here.</p> <p>Depending on the simulation engine, and the coordinate system, these object allocate some of the built-in arrays, while ignoring the others. Below is a full breakdown for all the arrays for both of these classes. We assume that \\(n_i\\) is the number of cells on the given subdomain in the \\(i\\)-th direction.</p>"},{"location":"content/2-code/4-fields_particles/#fields","title":"Fields","text":"Name Shape Description Allocated when... <code>em</code> \\(n_1[\\times n_2[\\times n_3]]\\times 6\\) main container for the electric and magnetic fields \\(E^i\\), \\(B^i\\) in SR and \\(D^i\\), \\(B^i\\) in GR always <code>bckp</code> \\(n_1[\\times n_2[\\times n_3]]\\times 6\\) used for intermediate operations (e.g., output) always <code>cur</code> \\(n_1[\\times n_2[\\times n_3]]\\times 3\\) current densities \\(J^i\\) always <code>buff</code> \\(n_1[\\times n_2[\\times n_3]]\\times 3\\) primarily used for storing intermediate values for the currents (e.g., for filtering) always <code>aux</code> \\(n_1[\\times n_2[\\times n_3]]\\times 6\\) auxiliary GR fields \\(E_i\\), \\(H_i\\) GR <code>em0</code> \\(n_1[\\times n_2[\\times n_3]]\\times 6\\) second set of electromagnetic fields staggered in time w.r.t. the main ones GR <code>cur0</code> \\(n_1[\\times n_2[\\times n_3]]\\times 3\\) current densities at the previous timestep GR <p>All of the field arrays have a type <code>real_t</code> which compilers to <code>float</code> when using single precision, and <code>double</code> when using double precision.</p> <p>Staggering</p> <p>Keep in mind that the field components stored in all these arrays are staggered not only in time, but also in space. Entity employs the Yee grid staggering, and this the electric fields are stored at the corresponding cell edges, while the magnetic fields are stored at the cell faces. To see how those fields are staggered in time, refer to the PIC algorithm section.</p> <p>Field loops</p> <p>Notice that fields have an additional dimension which stores the component. For convenience, Entity provides aliases to access those components: <code>em::ex1</code> (which maps to <code>0</code>), <code>em::bx3</code> (which maps to <code>5</code>), etc. A typical loop over all the fields on the local subdomain would look like this: <pre><code>// assume a 2D simulation\nauto fields = domain.fields;\nauto metric = domain.mesh.metric;\nKokkos::parallel_for(\"field_loop\",\n  domain.mesh.rangeActiveCells(),\n  Lambda(index_t i1, index_t i2) {\n    // get the code-unit coordinate of the cell corner\n    const auto i1_ = COORD(i1);\n    const auto i2_ = COORD(i2);\n    // get the sqrt_det_h at (i + 1/2, j)\n    const auto sqrtdeth = metric.sqrt_det_h({i1_ + HALF, i2_});\n    // do something with the Ex1 field (just an example)\n    fields.em(i1, i2, em::ex1) *= ONE / sqrtdeth;\n  });\n</code></pre></p>"},{"location":"content/2-code/4-fields_particles/#particles","title":"Particles","text":"Name Type Description Allocated when... <code>i1</code> <code>int</code> cell index of the particle in \\(x_1\\) always <code>i2</code> <code>int</code> cell index of the particle in \\(x_2\\) 2D or 3D <code>i3</code> <code>int</code> cell index of the particle in \\(x_3\\) 3D <code>dx1</code> <code>prtldx_t</code> displacement of the particle in the cell in \\(x_1\\) always <code>dx2</code> <code>prtldx_t</code> displacement of the particle in the cell in \\(x_2\\) 2D or 3D <code>dx3</code> <code>prtldx_t</code> displacement of the particle in the cell in \\(x_3\\) 3D <code>ux1</code> <code>real_t</code> velocity of the particle (see comment below) always <code>ux2</code> <code>real_t</code> velocity of the particle (see comment below) always <code>ux3</code> <code>real_t</code> velocity of the particle (see comment below) always <code>weight</code> <code>real_t</code> particle weights always <code>tag</code> <code>short</code> particle tag always <code>phi</code> <code>real_t</code> \\(\\phi\\) coordinate of the particle 2D non-Cartesian <code>i1_prev</code> <code>int</code> same as <code>i1</code> but for the previous step always <code>i2_prev</code> <code>int</code> same as <code>i2</code> but for the previous step 2D or 3D <code>i3_prev</code> <code>int</code> same as <code>i3</code> but for the previous step 3D <code>dx1_prev</code> <code>prtldx_t</code> same as <code>dx1</code> but for the previous step always <code>dx2_prev</code> <code>prtldx_t</code> same as <code>dx2</code> but for the previous step 2D or 3D <code>dx3_prev</code> <code>prtldx_t</code> same as <code>dx3</code> but for the previous step 3D <code>pld</code> <code>real_t</code> custom particle payloads (2D array) as needed (defined in the input) <p><code>prtldx_t</code> is a type alias for <code>real_t</code> which is used for the displacement of the particle w.r.t. the corner of the cell (this can be changed to be half-precision). Notice, that we additionally store the <code>phi</code> coordinate for the particles in non-Cartesian 2D simulations. While in GR this is totally optional, in SR it is required to keep track of the full particle coordinate, to be able to convert to and from the global Cartesian metric.</p> <p>Particle velocities</p> <p>Velocities, stored in <code>ux1</code>, <code>ux2</code>, and <code>ux3</code> have different meanings in different configurations. For SR simulations, particle velocities are stored in the global Cartesian basis, while in GR, these velocities correspond to the covariant components \\(u_i\\) in code units.</p> <p>All of the particle arrays have shape of <code>maxnpart</code>, which is set at the beginning of the simulation from the input file. To indicate the number of active particles, one can use the <code>npart()</code> method of the <code>Particles</code> object. That being said, remember that particles can always leave the simulation domain, or be sent to another subdomain (while its index would still be less than <code>npart()</code>). For that, we use the <code>tag</code> field, which can be set to <code>ParticleTag::dead</code> to indicate that the particle is no longer active.</p> <p>Particle loops</p> <p>A typical loop over all the particles on the local subdomain would look like this: <pre><code>// for example, taking the first species\nauto particles = domain.species[0];\nKokkos::parallel_for(\"prtl_loop\",\n  particles.rangeActiveParticles(),\n  Lambda(index_t p) {\n    if (particles.tag(p) == ParticleTag::dead) {\n      return;\n    }\n    // do something with the particle\n  });\n</code></pre></p> <p>Below is the diagram showing the structure of the fields and particles objects, and how they are stored in the <code>Domain</code> object.</p> <pre>\nclassDiagram\n  class Domain~SimEngine, class~ {\n    see domain...*\n  }\n  class ParticleSpecies {\n    #const spidx_t m_index\n    #const string m_label\n    #const float m_mass\n    #const float m_charge\n    #npart_t m_maxnpart\n    #const PrtlPusher m_pusher\n    #const bool m_use_gca\n    #const Cooling m_cooling\n    #const ushort m_npld\n    +index() spidx_t\n    +label() string\n    +mass() float\n    +charge() float\n    +maxnpart() npart_t\n    +pusher() PrtlPusher\n    +use_gca() bool\n    +cooling() Cooling\n    +npld() ushort\n  }\n  class Particles~Dimension, Coord~ {\n    -npart_t m_npart\n    -bool m_is_sorted\n    +array_t~int*~ i1\n    +array_t~int*~ i2\n    +array_t~int*~ i3\n    +array_t~prtldx_t*~ dx1\n    +array_t~prtldx_t*~ dx2\n    +array_t~prtldx_t*~ dx3\n    +array_t~real_t*~ ux1\n    +array_t~real_t*~ ux2\n    +array_t~real_t*~ ux3\n    +array_t~real_t*~ weight\n    +array_t~int*~ i1_prev\n    +array_t~int*~ i2_prev\n    +array_t~int*~ i3_prev\n    +array_t~prtldx_t*~ dx1_prev\n    +array_t~prtldx_t*~ dx2_prev\n    +array_t~prtldx_t*~ dx3_prev\n    +array_t~short*~ tag\n    +array_t~real_t**~ pld\n    +array_t~real_t*~ phi\n    +rangeActiveParticles() range_t~1D~\n    +rangeAllParticles() range_t~1D~\n    +npart() npart_t\n    +is_sorted() bool\n    +ntags() size_t\n    +memory_footprint() size_t\n    +NpartsPerTagAndOffsets() vector~npart_t~,array_t~npart_t*~\n    +set_npart(npart_t n) void\n    +set_unsorted() void\n    +RemoveDead() void\n    +SyncHostDevice() void\n  }\n  class Fields~Dimension, SimEngine~ {\n    +ndfield_t~D, 6~ em\n    +ndfield_t~D, 6~ bckp\n    +ndfield_t~D, 3~ cur\n    +ndfield_t~D, 3~ buff\n    +ndfield_t~D, 6~ aux\n    +ndfield_t~D, 6~ em0\n    +ndfield_t~D, 3~ cur0\n    +memory_footprint() size_t\n  }\n\n  Domain --* Particles : contains many\n  Domain --* Fields : contains\n  ParticleSpecies &lt;|-- Particles : inherits\n\n  note \"+: public\\n-: private\\n#: protected\\nunderline: static constexpr\\nitalic: virtual\"\n\n</pre>"},{"location":"content/2-code/5-metrics/","title":"Metrics","text":"<p>Relevant headers</p> <ul> <li><code>metrics/metric_base.h</code></li> <li><code>metrics/minkowski.h</code></li> <li><code>metrics/spherical.h</code></li> <li><code>metrics/qspherical.h</code></li> <li><code>metrics/kerr_schild.h</code></li> <li><code>metrics/qkerr_schild.h</code></li> <li><code>metrics/kerr_schild_0.h</code></li> </ul> <p>Metrics are key objects of the Entity framework. Superimposed on the discretized mesh, they define the spacetime geometry of the simulation and provide necessary functions for converting coordinates and transforming vectors from one basis to another.</p>"},{"location":"content/2-code/5-metrics/#coordinate-systems-and-bases","title":"Coordinate systems and bases","text":"<p>To understand how metrics are implemented in Entity, one must first understand the different coordinate systems and bases the Entity works with. For each metric, there are two or three levels of coordinates.</p> <ul> <li>Physical, \\(x^I\\): this is the coordinate system in which the metric is originally defined (e.g., for Kerr-Schild -- it's spherical, for Minkowski -- Cartesian).</li> <li>Linear: this is the coordinate system in which the grid discretization is uniform. It is often stretched or squeezed w.r.t. the original physical (e.g., for Q-Kerr-Schild -- it's quasi-spherical, for Kerr-Schild -- it's the same as the physical);</li> <li>Code-unit, \\(x^i\\): finally, this is the coordinate system the code works with; it takes the linear coordinate system and remaps it to the interval of \\([0, n_i)\\), where \\(n_i\\) is the number of grid points in the \\(i\\)-th direction on the given domain.</li> </ul> <p>Qspherical coordinates</p> <p>All qspherical metrics (<code>QSpherical</code>, <code>QKerrSchild</code>) take spherical coordinates as their base \\((r,\\theta,\\phi)\\), then stretch them to quasi-spherical coordinates \\((\\xi,\\eta,\\phi)\\), and finally map them to code-unit coordinates \\((x^1,x^2,x^3)\\).</p> \\[ \\begin{CD}  (r,\\theta,\\phi) @&gt;\\text{stretch}&gt;&gt; (\\xi,\\eta,\\phi) @&gt;\\text{map}&gt;&gt; (x^1,x^2,x^3) \\end{CD} \\] <p>where stretching is done via \\(\\xi=\\log{(r-r_0)}\\), \\(\\theta = x_2 + 2h \\eta (\\pi - 2 \\eta) (\\pi - \\eta) / \\pi^2\\) (\\(\\eta\\) set implicitly), and mapping -- via \\(x^1 = (\\xi - \\xi_{\\rm min})/n_1\\), etc.</p> <p>The diagram below demonstrates the stretching of the quasi-spherical coordinates in the \\(\\xi\\) direction, and how that maps to both the physical and code-unit coordinates. Here we stretch \\(r=[1, 90)\\) logarithmically \\(\\xi=\\ln{r}\\), and map it to \\(x^1=[0, 18)\\) which coincides with our discretization. The result is a non-uniformly discretized grid with more cells focused towards the origin.</p> <p></p> <p>Vectors in Entity can also be defined in different bases. We typically define covariant and contravariant vectors in code-, \\(x^i\\), and physical-, \\(x^I\\), generally non-orthonormal coordinates: \\(u_i\\), \\(u^i\\), and \\(u_I\\), \\(u^I\\). We can also define a locally-flat orthonormal basis, \\(u_{\\hat{i}}\\equiv u^{\\hat{i}}\\equiv u^{\\hat{I}}\\equiv u_{\\hat{I}}\\), also called the tetrad basis. For special-relativistic metrics, tetrad basis is exactly the same in all points of space, and is thus global. For general-relativistic metrics, the tetrad basis is defined locally, and is different in each point of space.</p>"},{"location":"content/2-code/5-metrics/#metric-classes","title":"Metric classes","text":"<p>Each metric has a number of distinct attributes. These are:</p> <ul> <li><code>D</code>: the dimensionality of the metric:<ul> <li><code>Dim::_1D</code>, <code>Dim::_2D</code>, <code>Dim::_3D</code></li> </ul> </li> <li><code>Label</code>: a string that identifies the metric;</li> <li><code>CoordType</code>: the type of coordinates used in the metric;<ul> <li><code>Coord::Cart</code>, <code>Coord::Sph</code>, <code>Coord::Qsph</code></li> </ul> </li> <li> <p><code>PrtlDim</code>: the dimensionality of the particle coordinates. <code>PrtlDim == Dim::_3D</code> for SR spherical metrics, and <code>== D</code> otherwise; (1) </p> <ol> <li> In 2D axisymmetric SR simulations, particles carry all three coordinates to recover their full Cartesian position, and transform fields to/from the global Cartesian basis.</li> </ol> </li> </ul>"},{"location":"content/2-code/5-metrics/#methods","title":"Methods","text":"method description arguments returns <code>h_ metric components $h_{ij}$ <code>coord_t&lt;D&gt; x_C</code> <code>real_t</code> <code>h inverse metric components $h^{ij}$ <code>coord_t&lt;D&gt; x_C</code> <code>real_t</code> <code>alpha</code> lapse function $\\alpha$ <code>coord_t&lt;D&gt; x_C</code> <code>real_t</code> <code>beta1</code> shift vector component $\\beta^1$ <code>coord_t&lt;D&gt; x_C</code> <code>sqrt_det_h</code> $\\sqrt{\\det{h_{ij}}}$ <code>coord_t&lt;D&gt; x_C</code> <code>real_t</code> <code>sqrt_det_h_tilde</code> $\\sqrt{\\det{h_{ij}}} / \\sin{\\theta}$ <code>coord_t&lt;D&gt; x_C</code> <code>real_t</code> <code>sqrt_h_ $\\sqrt{h_{ij}}$ <code>coord_t&lt;D&gt; x_C</code> <code>real_t</code> <code>polar_area</code> $A_{\\theta\\phi}$ <code>real_t x1_C</code> <code>real_t</code> <code>convert converts the $i$-th component of a coordinate to another basis <code>real_t</code> <code>real_t</code> <code>convert converts the full $D$-dimensional coordinate to another basis <code>coord_t&lt;D&gt;, &amp;coord_t&lt;D&gt;</code> <code>convert_xyz explicitly converts to/from a Cartesian basis <code>coord_t&lt;PrtlDim&gt;, &amp;coord_t&lt;PrtlDim&gt;</code> <code>transform transforms the $i$-th component of a vector to another frame <code>coord_t&lt;D&gt; x_C, real_t</code> <code>real_t</code> <code>transform transforms the full $3D$ vector to another frame <code>coord_t&lt;D&gt;, vec_t&lt;3D&gt;, &amp;vec_t&lt;3D&gt;</code> <code>transform_xyz explicitly transforms to/from a Cartesian frame <code>coord_t&lt;PrtlDim&gt;, vec_t&lt;3D&gt;, &amp;vec_t&lt;3D&gt;</code> <p> = only defined for SR metrics</p> <p> = only defined for GR metrics</p> <p> = only defined for spherical metrics</p> <p>The <code>in</code> and <code>out</code> template arguments for the <code>convert&lt;&gt;</code> and <code>transform&lt;&gt;</code> functions are, respectively, the coordinate bases (stored as an enum <code>Crd::</code>) and the vector reference frame (stored as enum <code>Idx::</code>). The members of these enums are:</p> description <code>Crd::Cd</code> code-unit coordinates: $x^i$ <code>Crd::XYZ</code> cartesian coordinates: $x^{x}$ <code>Crd::Sph</code> spherical coordinates: $x^{r}$ <code>Crd::Ph</code> \"physical\" coordinates: $x^{I}$ <p><code>Crd::Ph</code></p> <p>Depending on the metric, <code>Crd::Ph</code> is equivalent to <code>Crd::XYZ</code> or <code>Crd::Sph</code>. </p> description <code>Idx::U</code> upper (contravariant) basis (code units): $u^i$ <code>Idx::D</code> lower (covariant) basis (code units): $u_i$ <code>Idx::T</code> tetrad (orthonormal) basis $u^{\\hat{i}} \\equiv u_{\\hat{i}} \\equiv u^{\\hat{I}} \\equiv u_{\\hat{I}}$ <code>Idx::XYZ</code> global Cartesian basis: $u^{\\hat{x}}\\equiv u_{\\hat{x}}$ <code>Idx::Sph</code> global spherical basis: $u^{\\hat{r}}\\equiv u_{\\hat{r}}$ <code>Idx::PU</code> physical contravariant basis: $u^I$ <code>Idx::PD</code> physical covariant basis: $u_I$ <p> = only possible in SR</p> <p>Note</p> <p>For Cartesian metric, transforming to <code>Idx::T</code> is equivalent to converting to <code>Idx::XYZ</code>, and similarly for the spherical metric, transforming to <code>Idx::T</code> is equivalent to converting to <code>Idx::Sph</code>.</p> <p>Code</p> <p>Metric component \\(1,1\\) in the covariant basis, \\(h_{11}\\), can be accessed via in the certain position <code>x_Code</code> in code units by calling (assuming <code>D</code> is the dimension of the metric):  <pre><code>coord_t&lt;D&gt; x_Code;\n// define x_Code\nmetric.template h_&lt;1, 1&gt;(x_Code)\n</code></pre> To convert <code>x_Code</code> to physical coordinates: <pre><code>coord_t&lt;D&gt; x_Ph { ZERO }; // init with zero so the compiler doesn't complain\nmetric.template convert&lt;Crd::Cd, Crd::Ph&gt;(x_Code, x_Ph);\n</code></pre> To transform a vector <code>v_Cov</code> from the covariant basis to the tetrad basis: <pre><code>vec_t&lt;Dim::_3D&gt; v_Cov;\n// define v_Cov\nvec_t&lt;Dim::_3D&gt; v_T { ZERO };\nmetric.template transform&lt;Idx::D, Idx::T&gt;(x_Code, v_Cov, v_T);\n</code></pre> For SR metrics you can also explicitly convert coordinates and transform vectors to/from the cartesian basis: <pre><code>coord_t&lt;PrtlDim&gt; x_Code;\n// define x_Code (notice, that for 2D spherical metrics this will be 3D)\nmetric.template convert_xyz&lt;Crd::Cd, Crd::XYZ&gt;(x_Code, x_Cart);\n\nvec_t&lt;Dim::_3D&gt; v_Cntrv;\n// define v_Cntrv\nvec_t&lt;Dim::_3D&gt; v_XYZ { ZERO };\nmetric.template transform_xyz&lt;Idx::U, Idx::XYZ&gt;(x_Code, v_Cntrv, v_XYZ);\n</code></pre></p> <p>Below is a diagram demonstrating all the possible transformations.</p>"},{"location":"content/2-code/5-metrics/#metric-structure-and-hierarchy","title":"Metric structure and hierarchy","text":"<p>Schematics below shows the structure of the metric classes and their inheritance hierarchy with all the private/public variables and methods.</p> <pre>\nclassDiagram\n  direction LR\n  class Mesh~class~ {\n    see mesh...*\n  }\n  class MetricBase~Dimension~ {\n    +bool is_metric$\n    +Dimension Dim$\n    #real_t dx_min\n    #const real_t nx1\n    #const real_t nx2\n    #const real_t nx3\n    #const real_t x1_min\n    #const real_t x1_max\n    #const real_t x2_min\n    #const real_t x2_max\n    #const real_t x3_min\n    #const real_t x3_max\n    +find_dxMin() real_t\n    +totVolume() real_t\n    +dxMin() real_t\n    +set_dxMin(real_t dxmin) void\n  }\n\n  class Minkowski~Dimension~ {\n    -const real_t dx\n    -const real_t dx_inv\n    +const char *       Label$\n    +Dimension PrtlDim$\n    +ntt\\:\\:Metric MetricType$\n    +ntt\\:\\:Coord CoordType$\n    +find_dxMin() real_t override\n    +totVolume() real_t override\n    +sqrt_det_h(coord_t~D~ &amp;) Inline auto\n  }\n  class Spherical~Dimension~ {\n    -const real_t dr\n    -const real_t dtheta\n    -const real_t dphi\n    -const real_t dr_inv\n    -const real_t dtheta_inv\n    -const real_t dphi_inv\n    -const bool small_angle\n    +const char *       Label$\n    +Dimension PrtlDim$\n    +ntt\\:\\:Metric MetricType$\n    +ntt\\:\\:Coord CoordType$\n    +find_dxMin() real_t override\n    +totVolume() real_t override\n    +sqrt_det_h(coord_t~D~ &amp; x) Inline auto\n    +sqrt_det_h_tilde(coord_t~D~ &amp; x) Inline auto\n    +polar_area(real_t &amp; x1) Inline auto\n  }\n  class QSpherical~Dimension~ {\n    -const real_t r0\n    -const real_t h\n    -const real_t chi_min\n    -const real_t eta_min\n    -const real_t phi_min\n    -const real_t dchi\n    -const real_t deta\n    -const real_t dphi\n    -const real_t dchi_inv\n    -const real_t deta_inv\n    -const real_t dphi_inv\n    -const bool small_angle\n    +const char *       Label$\n    +Dimension PrtlDim$\n    +ntt\\:\\:Metric MetricType$\n    +ntt\\:\\:Coord CoordType$\n    +find_dxMin() real_t override\n    +totVolume() real_t override\n    +sqrt_det_h(coord_t~D~ &amp; x) Inline auto\n    +sqrt_det_h_tilde(coord_t~D~ &amp; x) Inline auto\n    +polar_area(real_t &amp; x1) Inline auto\n    -dtheta_deta(real_t &amp; eta) Inline auto\n    -eta2theta(real_t &amp; eta) Inline auto\n    -theta2eta(real_t &amp; theta) Inline auto\n  }\n  class KerrSchild~Dimension~ {\n    -const real_t a\n    -const real_t rg_\n    -const real_t rh_\n    -const real_t dr\n    -const real_t dtheta\n    -const real_t dphi\n    -const real_t dr_inv\n    -const real_t dtheta_inv\n    -const real_t dphi_inv\n    +const char *       Label$\n    +Dimension PrtlDim$\n    +ntt\\:\\:Coord CoordType$\n    +ntt\\:\\:Metric MetricType$\n    -Delta(real_t &amp; r) Inline auto\n    -Sigma(real_t &amp; r, real_t &amp; theta) Inline auto\n    -A(real_t &amp; r, real_t &amp; theta) Inline auto\n    -z(real_t &amp; r, real_t &amp; theta) Inline auto\n    +spin() Inline auto\n    +rhorizon() Inline auto\n    +rg() Inline auto\n    +find_dxMin() real_t override\n    +totVolume() real_t override\n    +alpha(coord_t~D~ &amp; x) Inline auto\n    +dr_alpha(coord_t~D~ &amp; x) Inline auto\n    +dt_alpha(coord_t~D~ &amp; x) Inline auto\n    +beta1(coord_t~D~ &amp; x) Inline auto\n    +dr_beta1(coord_t~D~ &amp; x) Inline auto\n    +dt_beta1(coord_t~D~ &amp; x) Inline auto\n    +dr_h11(coord_t~D~ &amp; x) Inline auto\n    +dr_h22(coord_t~D~ &amp; x) Inline auto\n    +dr_h33(coord_t~D~ &amp; x) Inline auto\n    +dr_h13(coord_t~D~ &amp; x) Inline auto\n    +dt_Sigma(real_t &amp; theta) Inline auto\n    +dt_A(real_t &amp; r, real_t &amp; theta) Inline auto\n    +dt_h11(coord_t~D~ &amp; x) Inline auto\n    +dt_h22(coord_t~D~ &amp; x) Inline auto\n    +dt_h33(coord_t~D~ &amp; x) Inline auto\n    +dt_h13(coord_t~D~ &amp; x) Inline auto\n    +sqrt_det_h(coord_t~D~ &amp; x) Inline auto\n    +sqrt_det_h_tilde(coord_t~D~ &amp; x) Inline auto\n    +polar_area(real_t &amp; x1) Inline auto\n  }\n  class QKerrSchild~Dimension~ {\n    -const real_t a\n    -const real_t rg_\n    -const real_t rh_\n    -const real_t r0\n    -const real_t h0\n    -const real_t chi_min\n    -const real_t eta_min\n    -const real_t phi_min\n    -const real_t dchi\n    -const real_t deta\n    -const real_t dphi\n    -const real_t dchi_inv\n    -const real_t deta_inv\n    -const real_t dphi_inv\n    +const char *       Label$\n    +Dimension PrtlDim$\n    +ntt\\:\\:Metric MetricType$\n    +ntt\\:\\:Coord CoordType$\n    -Delta(real_t &amp; r) Inline auto\n    -Sigma(real_t &amp; r, real_t &amp; theta) Inline auto\n    -A(real_t &amp; r, real_t &amp; theta) Inline auto\n    -z(real_t &amp; r, real_t &amp; theta) Inline auto\n    +spin() Inline auto\n    +rhorizon() Inline auto\n    +rg() Inline auto\n    +find_dxMin() real_t override\n    +totVolume() real_t override\n    +alpha(coord_t~D~ &amp; x) Inline auto\n    +dr_alpha(coord_t~D~ &amp; x) Inline auto\n    +dt_alpha(coord_t~D~ &amp; x) Inline auto\n    +beta1(coord_t~D~ &amp; x) Inline auto\n    +dr_beta1(coord_t~D~ &amp; x) Inline auto\n    +dt_beta1(coord_t~D~ &amp; x) Inline auto\n    +dr_h11(coord_t~D~ &amp; x) Inline auto\n    +dr_h22(coord_t~D~ &amp; x) Inline auto\n    +dr_h33(coord_t~D~ &amp; x) Inline auto\n    +dr_h13(coord_t~D~ &amp; x) Inline auto\n    +dt_Sigma(real_t &amp; eta) Inline auto\n    +dt_A(real_t &amp; r, real_t &amp; eta) Inline auto\n    +dt_h11(coord_t~D~ &amp; x) Inline auto\n    +dt_h22(coord_t~D~ &amp; x) Inline auto\n    +dt_h33(coord_t~D~ &amp; x) Inline auto\n    +dt_h13(coord_t~D~ &amp; x) Inline auto\n    +sqrt_det_h(coord_t~D~ &amp; x) Inline auto\n    +sqrt_det_h_tilde(coord_t~D~ &amp; x) Inline auto\n    +polar_area(real_t &amp; x1) Inline auto\n    -dtheta_deta(real_t &amp; eta) Inline auto\n    -eta2theta(real_t &amp; eta) Inline auto\n    -dx_dt(real_t &amp; eta) Inline auto\n    -theta2eta(real_t &amp; theta) Inline auto\n  }\n  class KerrSchild0~Dimension~ {\n    -const real_t dr\n    -const real_t dtheta\n    -const real_t dphi\n    -const real_t dr_inv\n    -const real_t dtheta_inv\n    -const real_t dphi_inv\n    -const real_t a\n    -const real_t rg_\n    -const real_t rh_\n    +const char * Label$\n    +Dimension PrtlDim$\n    +ntt\\:\\:Metric MetricType$\n    +ntt\\:\\:Coord CoordType$\n    +spin() Inline auto\n    +rhorizon() Inline auto\n    +rg() Inline auto\n    +find_dxMin() real_t override\n    +totVolume() real_t override\n    +alpha(coord_t~D~ &amp;) Inline auto\n    +dr_alpha(coord_t~D~ &amp; x) Inline auto\n    +dt_alpha(coord_t~D~ &amp; x) Inline auto\n    +beta1(coord_t~D~ &amp;) Inline auto\n    +dr_beta1(coord_t~D~ &amp; x) Inline auto\n    +dt_beta1(coord_t~D~ &amp; x) Inline auto\n    +dr_h11(coord_t~D~ &amp; x) Inline auto\n    +dr_h22(coord_t~D~ &amp; x) Inline auto\n    +dr_h33(coord_t~D~ &amp; x) Inline auto\n    +dr_h13(coord_t~D~ &amp; x) Inline auto\n    +dt_h11(coord_t~D~ &amp; x) Inline auto\n    +dt_h22(coord_t~D~ &amp; x) Inline auto\n    +dt_h33(coord_t~D~ &amp; x) Inline auto\n    +dt_h13(coord_t~D~ &amp; x) Inline auto\n    +sqrt_det_h(coord_t~D~ &amp; x) Inline auto\n    +sqrt_det_h_tilde(coord_t~D~ &amp; x) Inline auto\n    +polar_area(real_t &amp; x1) Inline auto\n  }\n\n  MetricBase &lt;|-- Minkowski : implements\n  MetricBase &lt;|-- Spherical : implements\n  MetricBase &lt;|-- QSpherical : implements\n  MetricBase &lt;|-- KerrSchild : implements\n  MetricBase &lt;|-- QKerrSchild : implements\n  MetricBase &lt;|-- KerrSchild0 : implements\n  Mesh --* MetricBase : contains\n\n  note \"+: public\\n-: private\\n#: protected\\nunderline: static constexpr\\nitalic: virtual\"\n\n</pre>"},{"location":"content/2-code/6-problem_generators/","title":"Problem generators","text":"<p>Relevant headers</p> <ul> <li><code>setups/**/pgen.hpp</code></li> <li><code>archetypes/problem_generator.h</code></li> <li><code>archetypes/field_setter.h</code></li> <li><code>archetypes/energy_dist.h</code></li> <li><code>archetypes/spatial_dist.h</code></li> <li><code>archetypes/particle_injector.h</code></li> </ul> <p>Problem generators describe a specific simulation setup (e.g., initial conditions) for the Entity engines to use to run the simulation. All problem generators are stored in the <code>setups</code> diectory each in a separate parent directory and are all named <code>pgen.hpp</code>. It is a good practice to also store a sample <code>*.toml</code> input file and a <code>*.py</code> visualization file corresponding to that problem generator. Problem generators are chosen at compile time using the <code>-D pgen=...</code> flag, where <code>...</code> is the relative path where the problem generator is stored. For instance, to pick the <code>setups/srpic/langmuir/pgen.hpp</code> problem generator, one would use the following command:</p> <pre><code>cmake ... -D pgen=srpic/langmuir\n</code></pre> <p>All problem generators contain a namespace <code>user::</code> and a structure named <code>Pgen&lt;S, M&gt;</code> which must inherit from the <code>arch::ProblemGenerator&lt;S, M&gt;</code> class, where <code>S</code> is the simulation engine and <code>M</code> is the metric. A typical dummy problem generator will look like this:</p> <pre><code>#ifndef PROBLEM_GENERATOR_H\n#define PROBLEM_GENERATOR_H\n\n#include \"enums.h\"\n#include \"global.h\"\n\n#include \"arch/traits.h\"\n#include \"archetypes/problem_generator.h\"\n#include \"framework/domain/metadomain.h\"\n#include \"framework/parameters.h\"\n\nnamespace user {\n  using namespace ntt; // (2)!\n\n  template &lt;SimEngine::type S, class M&gt;\n  struct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n    // enumerate which engines/metrics/dimensions are compatible (1)\n    static constexpr auto engines {\n      traits::compatible_with&lt;SimEngine::SRPIC, SimEngine::GRPIC&gt;::value\n    };\n    static constexpr auto metrics {\n      traits::compatible_with&lt;Metric::Minkowski,\n                              Metric::Spherical,\n                              Metric::QSpherical,\n                              Metric::Kerr_Schild,\n                              Metric::QKerr_Schild,\n                              Metric::Kerr_Schild_0&gt;::value\n    };\n    static constexpr auto dimensions {\n      traits::compatible_with&lt;Dim::_1D, Dim::_2D, Dim::_3D&gt;::value\n    };\n\n    // ... additional definitions ..\n\n    inline PGen(const SimulationParams&amp; p, const Metadomain&lt;S, M&gt;&amp;)\n      : arch::ProblemGenerator&lt;S, M&gt; { p } \n      // ... any additional initialization ...\n      {}\n\n    // ... additional methods ...\n  };\n\n} // namespace user\n\n#endif // PROBLEM_GENERATOR_H\n</code></pre> <ol> <li>This is done not only for the runtime sanity check, but also to shorten the compile time, as the compiler will not generate the code for the incompatible engines/metrics/dimensions.</li> <li>To avoid using <code>ntt::</code> everywhere</li> </ol> <p>There are three special definitions one may provide in the problem generator that will allow the simulation engine to call custom routines at the beginning of the simulation or at the end of each timestep.</p> <p>Units</p> <p>In all of the functions and classes described below, it is assumed that the end-user designing the problem generator has no knowledge of the inner workings of the code units. All the quantities provided by the user are thus in the natural physical units (i.e., global physical coordinates for the positions and local tetrad basis for the vectors). All the conversions, staggering etc. is done automatically under the hood.</p>"},{"location":"content/2-code/6-problem_generators/#initializing-fields","title":"Initializing fields","text":"<p>To initialize electromagnetic fields to specific values, one may provide a custom class called <code>init_flds</code>:</p> <pre><code>template &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // ...\n\n  // the name of the class may be arbitrary, but the instance must be named `init_flds`\n  FancyFieldInitializer&lt;S, M&gt; init_flds;\n};\n</code></pre> <p>This class in turn may have an arbitrarily complex constructor, but it must have any number of the methods <code>ex1()</code>, <code>ex2()</code>, ... <code>dx1()</code>, <code>dx2()</code>, etc., which set the corresponding field components. For instance, to set the electric field in the \\(x_2\\) direction to a constant value, one would write: <pre><code>template &lt;Dimension D&gt;\nstruct NotSoFancyFieldInitializer {\n  NotSoFancyFieldInitializer(real_t myval)\n    : myvalue { myval } {}\n\n  Inline auto ex2(const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    return myvalue;\n  }\n  // you may skip other field components if you don't need them\n\nprivate:\n  const real_t myvalue;\n};\n</code></pre></p> <p>Notice, that <code>ex2</code> takes a single argument of type <code>coord_t&lt;D&gt;</code> (coordinate vector), which in this case is empty, since we are not using it. In general, one may define fields as functions of the coordinate. </p> <pre><code>template &lt;Dimension D&gt;\nstruct SinusoidalField {\n  SinusoidalField(real_t kx, real_t ampl)\n    : kx { kx }, amplitude { ampl } {}\n\n  Inline auto bx1(const coord_t&lt;D&gt;&amp; x_Ph) const -&gt; real_t {\n    return amplitude * math::sin(kx * x_Ph[1]); // function of x2 coordinate\n  }\n  // you may skip other field components if you don't need them\n\nprivate:\n  const real_t kx, amplitude;\n};\n\n// and then use it in the problem generator\ntemplate &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // ...\n\n  SinusoidalField&lt;S, M&gt; init_flds;\n\n  // initialize the `init_flds` by passing the parameters from the input\n  inline PGen(const SimulationParams&amp; p, const Metadomain&lt;S, M&gt;&amp;)\n      : arch::ProblemGenerator&lt;S, M&gt; { p }\n      , init_flds { p.template get&lt;real_t&gt;(\"setup.kx\"), \n                    p.template get&lt;real_t&gt;(\"setup.amplitude\") } \n      {}\n};\n</code></pre> <p>Fields must be returned in the local tetrad (orthonormal) basis, while the passed coordinates are the physical coordinates. Conversion to code units and staggering of each corresponding field component is done automatically under the hood.</p>"},{"location":"content/2-code/6-problem_generators/#initializing-particles","title":"Initializing particles","text":"<p>Similar to initializing the fields, one can also initialize particles with a given energy or spatial distribution. This is done by providing a custom method of the <code>PGen</code> class called <code>InitPrtls(Domain&lt;S, M&gt;&amp;)</code> which takes a reference to the local subdomain as a parameter. In principle, one can manually initialize the particles in any way they want, but it is recommended to use the built-in routines from the <code>arch::</code> (archetypes) namespace.</p> <p>For instance, to initialize a uniform Maxwellian of a given temperature, one can use the <code>arch::Maxwellian</code> and <code>arch::UniformInjector</code> classes together with the <code>InjectUniform</code> method:</p> <pre><code>// don't forget to include the proper headers\n#include \"archetypes/energy_dist.h\"\n#include \"archetypes/particle_injector.h\"\n\n// ...\n\ntemplate &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // \n  inline void InitPrtls(Domain&lt;S, M&gt;&amp; local_domain) {\n    const auto energy_dist = arch::Maxwellian&lt;S, M&gt;(\n                                  local_domain.mesh.metric, \n                                  local_domain.random_pool, \n                                  temperature);\n    const auto injector = arch::UniformInjector&lt;S, M, arch::Maxwellian&gt;(\n                                  energy_dist, { 1, 2 });\n                                         //      ^^^^^\n                                         //  species to inject    \n    arch::InjectUniform&lt;S, M, arch::UniformInjector&lt;S, M, arch::Maxwellian&gt;&gt;(\n                          params,\n                          local_domain,\n                          injector,\n                          1.0); // &lt;-- this is the number density in units of `n0`\n  }\n};\n</code></pre> <p>To initialize a non-uniform distribution and/or an arbitrary energy distribution, we will need to provide our own classes, which in turn must inherit from the <code>arch::SpatialDistribution&lt;S, M&gt;</code> and <code>arch::EnergyDistribution&lt;S, M&gt;</code>. For instance, let us initialize a distribution of two particle species counter-streaming in opposing direction with their velocities depending on the \\(x_2\\) (\\(y\\)) coordinate, distributed in space according to a Gaussian profile. We first need to define the energy distribution:</p> <pre><code>template &lt;SimEngine::type S, class M&gt;\nstruct CounterstreamEnergyDist : public arch::EnergyDistribution&lt;S, M&gt; {\n  CounterstreamEnergyDist(const M&amp; metric, real_t v_max, real_t sx2)\n    : arch::EnergyDistribution&lt;S, M&gt; { metric }\n    , v_max { v_max }\n    , kx2 { static_cast&lt;real_t&gt;(constant::TWO_PI) / sx2 } {}\n\n  // three arguments passed here are\n  // x_Ph: global physical coordinates of the particle\n  // v: the velocity of the particle to-be-set in the tetrad basis\n  // sp: species index\n  Inline void operator()(const coord_t&lt;M::Dim&gt;&amp; x_Ph,\n                         vec_t&lt;Dim::_3D&gt;&amp;       v,\n                         unsigned short         sp) const {\n    if (sp == 1) {\n      v[0] = v_max * math::sin(kx2 * x_Ph[1]);\n    } else {\n      v[0] = -v_max * math::sin(kx2 * x_Ph[1]);\n    }\n  }\n\nprivate:\n  const real_t v_max, kx2;\n};\n</code></pre> <p>We then need to define the spatial distribution, which takes a coordinate as an argument and returns the probability of a particle to be injected at that point. In our case, we will use a Gaussian profile:</p> <pre><code>template &lt;SimEngine::type S, class M&gt;\nstruct GaussianDist : public arch::SpatialDistribution&lt;S, M&gt; {\n  GaussianDist(const M&amp; metric, real_t x1c, real_t x2c, real_t dr)\n    : arch::SpatialDistribution&lt;S, M&gt; { metric }\n    , x1c { x1c }\n    , x2c { x2c }\n    , dr { dr } {}\n\n  // to properly scale the number density, the probability should be normalized to 1\n  Inline auto operator()(const coord_t&lt;M::Dim&gt;&amp; x_Ph) const -&gt; real_t {\n    return math::exp(-(SQR(x_Ph[0] - x1c) + SQR(x_Ph[1] - x2c)) / SQR(dr));\n  }\n\nprivate:\n  const real_t x1c, x2c, dr;\n};\n</code></pre> <p>We can then pass the instances of these classes to the <code>arch::InjectNonUniform</code> method called from within the <code>InitPrtls</code> method of the problem generator:</p> <pre><code>// definition of CounterstreamEnergyDist and GaussianDist classes\n// ...\n\ntemplate &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // internal variables to-be-used in the constructor\n  const real_t temperature, v_max, sx2;\n  const real_t x1c, x2c, dr;\n\n  // read the parameters from the input\n  inline PGen(const SimulationParams&amp; p, const Metadomain&lt;S, M&gt;&amp; global_domain)\n    : arch::ProblemGenerator&lt;S, M&gt; { p }\n    , temperature { p.template get&lt;real_t&gt;(\"setup.temperature\") }\n    , v_max { p.template get&lt;real_t&gt;(\"setup.v_max\") }\n    , sx2 { global_domain.mesh().extent(in::x2).second - global_domain.mesh().extent(in::x2).first } // (1)!\n    , x1c { p.template get&lt;real_t&gt;(\"setup.x1c\") }\n    , x2c { p.template get&lt;real_t&gt;(\"setup.x2c\") }\n    , dr { p.template get&lt;real_t&gt;(\"setup.dr\") }\n    {}\n\n  inline void InitPrtls(Domain&lt;S, M&gt;&amp; local_domain) {\n    const auto energy_dist  = CounterstreamEnergyDist&lt;S, M&gt;(\n                                        local_domain.mesh.metric,\n                                        v_max,\n                                        sx2);\n    const auto spatial_dist = GaussianDist&lt;S, M&gt;(domain.mesh.metric,\n                                                 x1c,\n                                                 x2c,\n                                                 dr);\n    const auto injector = \n      arch::NonUniformInjector&lt;S, M, CounterstreamEnergyDist, GaussianDist&gt;(\n        energy_dist,\n        spatial_dist,\n        { 1, 2 });\n\n    arch::InjectNonUniform&lt;S, M, arch::NonUniformInjector&lt;S, M, CounterstreamEnergyDist, GaussianDist&gt;&gt;(\n            params,\n            domain,\n            injector,\n            1.0); // &lt;-- injected density in units of `n0` (2)!\n  }\n\n};\n</code></pre> <ol> <li><code>x_2</code> extent of the global domain can be directly read from the metadomain instance passed to the constructor.</li> <li>Here, the value of <code>1.0</code> corresponds to the probability of <code>1.0</code> returned by the spatial distribution class.</li> </ol>"},{"location":"content/2-code/6-problem_generators/#boundary-conditions","title":"Boundary Conditions","text":"<p>Entity supports a number of boundary conditions for fields and particles which we will list in the following section. We currently provide:</p> <pre><code>[grid.boundaries]\n  # one of: [\"PERIODIC\", \"MATCH\", \"FIXED\", \"ATMOSPHERE\", \"CUSTOM\", \"HORIZON\", \"CONDUCTOR\"]\n  fields = \"\" \n\n  # one of: [\"PERIODIC\", \"ABSORB\", \"ATMOSPHERE\", \"CUSTOM\", \"REFLECT\", \"HORIZON\"]\n  particles = \"\"\n</code></pre>"},{"location":"content/2-code/6-problem_generators/#periodic-boundaries","title":"Periodic boundaries","text":"<p>This boundary condition is valid for both fields and particles and is quite self-explanatory. It simply maps fields and particles outside of the domain on one side into the domain on the other side.</p>"},{"location":"content/2-code/6-problem_generators/#atmospheric-boundaries","title":"Atmospheric boundaries","text":"<p>There is a special type of boundary condition named \"atmosphere,\" which applies an additional \"gravitational\" force to particles and automatically replenishes the plasma to a given target level, while also resetting the fields to a specific value. For Cartesian geometry this boundary condition can be applied in the arbitrary direction, while for spherical/qspherical coordinates, it is only applicable in the \\(-\\hat{x}_1\\) (same as \\(-\\hat{r})\\) dimension. Thes boundary conditions are specified just like any other ones, via the <code>fields</code> and <code>particles</code> input parameters of the <code>[grid.boundaries]</code> section of the input file. </p> <p>The injected particle distribution is in Boltzmann-equilibrium with the gravity: \\(\\bm{u}\\cdot\\nabla_{\\bm{x}} f + m \\bm{g}\\cdot \\nabla_{\\bm{u}} f = 0\\); the scale-height and the temperature of of the atmosphere are configurable from the input file using the <code>grid.boundaries.atmosphere</code> parameters. The user also has a control over the peak density of the atmosphere, the extent to which the force is acting, as well as the particle species that are being injected. </p> <pre><code>[grid.boundaries.atmosphere]\n# @required: if ATMOSPHERE is one of the boundaries\n  # Temperature of the atmosphere in units of m0 c^2\n  #   @type: float\n  temperature = \"\"\n  # Peak number density of the atmosphere at base in units of n0\n  #   @type: float\n  density = \"\"\n  # Pressure scale-height in physical units\n  #   @type: float\n  height = \"\"\n  # Species indices of particles that populate the atmosphere\n  #   @type: array of ints of size 2\n  species = \"\"\n  # Distance from the edge to which the gravity is imposed in physical units\n  #   @type: float\n  #   @default: 0.0\n  #   @note: 0.0 means no limit\n  ds = \"\"\n</code></pre> <p>While the particle atmospheric boundaries are handled automatically, when field boundaries are set to <code>ATMOSPHERE</code>, the user must also provide target electromagnetic fields which will be used to reset the field values below the atmosphere. To do this, one needs to provide a <code>FieldDriver</code> method in their problem generator, which takes <code>time</code> as an argument and returns an arbitrary class with methods: <code>ex1</code>, <code>ex2</code>, ... etc. An example of such a class is shown below:</p> <pre><code>template &lt;Dimension D&gt;\nstruct AtmFields {\n\n  // functions take the physical coordinate as an argument\n  Inline auto ex1(const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    // return something\n  }\n\n  // ex2, ex3\n\n  Inline auto bx1(const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    // return something ...\n  }\n\n  // bx2, bx3\n};\n\ntemplate &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // ...\n\n  auto FieldDriver(real_t time) const -&gt; AtmFields&lt;D&gt; {\n    return AtmFields&lt;D&gt; { /* ... params ... */ };\n  }\n};\n</code></pre> <p>Below is a diagram which indicates how the atmospheric boundary conditions operate (in the example, they are applied in \\(-\\hat{\\bm{x}}\\) direction). Note, that when the fields are enforced, the normal components of the electric field, and the tangential components of the magnetic field are only enforced below the last (first) cell of the region, whereas the tangential components of the electric field and the normal components of the magnetic field are enforced everywhere (including in the last (first) cell of the region).</p>"},{"location":"content/2-code/6-problem_generators/#conductor-boundaries","title":"Conductor boundaries","text":"<p> 1.2.0 </p> <p>Another kind of special boundary condition is the perfect (magnetic) conductor boundary. This boundary should be used in combination with reflecting boundaries for particles. You can think of the perfect conductor as introducing a mirror charge outside the boundary.</p> <p>An example usage can be found in the <code>srpic/shock/shock.toml</code>:</p> <pre><code>[grid.boundaries]\n    fields = [[\"CONDUCTOR\", \"MATCH\"], [\"PERIODIC\"]]\n    particles = [[\"REFLECT\", \"ABSORB\"], [\"PERIODIC\"]]\n</code></pre> <p>A perfect conductor satisfies the equations \\(\\hat{\\boldsymbol{n}} \\times \\boldsymbol{B} = 0\\) and \\(\\hat{\\boldsymbol{n}} \\cdot \\boldsymbol{E} = 0\\), where \\(\\hat{\\boldsymbol{n}}\\) is the surface normal of the boundary. This is achieved by setting \\(E\\)- and \\(B\\)-field at distance \\(x\\) from the boundary to:</p> E-field B-field \\(E_\\perp(-\\vec{x}) = - E_\\perp(+\\vec{x})\\) \\(B_\\parallel(-\\vec{x}) = - B_\\parallel(+\\vec{x})\\) \\(E_\\perp(0) = 0\\) \\(B_\\parallel(0) = 0\\) \\(E_\\parallel(-\\vec{x}) = E_\\parallel(+\\vec{x})\\) \\(B_\\perp(-\\vec{x}) = B_\\perp(+\\vec{x})\\) <p>where \\(\\perp\\) is the component perpendicular to \\(\\hat{\\boldsymbol{n}}\\) (tangential to the conductor boundary), while \\(||\\) component is along \\(\\hat{\\boldsymbol{n}}\\) (perpendicular to the conductor boundary).</p> <p>Note that the current densities which might propagate beyond the particle stencil due to filtering, should be reflected in the same way as the electric fields; in the <code>Entity</code> this is handled at the current filtering kernel. </p> <p>These boundary conditions do not require any additional input paremeters.</p>"},{"location":"content/2-code/6-problem_generators/#match-boundaries","title":"Match boundaries","text":"<p> 1.2.0 </p> <p>If you want to drive the fields at your boundary to a given value you can do so using the <code>MATCH</code> boundary conditions. You can define the width across which the code drives the fields the target values with the <code>grid.boundaries.match.ds</code> parameter:</p> <pre><code>[grid.boundaries.match]\n  # Size of the matching layer in each direction for fields in physical (code) units:\n  #   @type: float or array of tuples\n  #   @default: 1% of the domain size (in shortest dimension)\n  #   @note: In spherical, this is the size of the layer in r from the outer wall\n  #   @example: ds = 1.5 (will set the same for all directions)\n  #   @example: ds = [[1.5], [2.0, 1.0], [1.1]] (will duplicate 1.5 for +/- x1 and 1.1 for +/- x3)\n  #   @example: ds = [[], [1.5], []] (will only set for x2)\n  ds = \"\"\n</code></pre> <p>Note</p> <p>Under the hood, the matching boundary conditions simply apply the following condition to the field components: $$   A^{\\rm new} = A^{\\rm old}s + A^{\\rm target} (1-s),~~~ s\\equiv \\tanh{\\left(\\frac{|x^i - x^i_b|}{ds_i/4}\\right)} $$ where \\(x^i\\) is the physical coordinate in the direction \\(i\\), \\(x^i_b\\) -- is the corresponding boundary in that direction, and \\(A\\) is one of the \\(E\\) (or \\(D\\) in GR) or \\(B\\) components. The user is responsible for supplying the target values (in the problem generator, see below), and the values of \\(ds_i\\) (for all directions) from the input file.</p> <p>In the problem generator one only needs to define a <code>MatchFields</code> method which should inherit from a <code>struct</code> that defines the field components you want to drive (the name of the struct can be arbitrary, as long as the name of the function returning it is <code>MatchFields</code>). </p> <pre><code>template &lt;Dimension D&gt;\nstruct MyBoundaryFields {\n  /*\n    Defines the fields you want to drive your boundary towards\n  */\n\n  // functions take the physical coordinate as an argument\n  Inline auto ex1(const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    // return something\n  }\n\n  // ex2, ex3\n\n  Inline auto bx1(const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    // return something ...\n  }\n\n  // bx2, bx3\n};\n\ntemplate &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // ...\n\n  // This function is called within the BC kernel\n  // (potentially, bc_flds may depend on time)\n  auto MatchFields(simtime_t time) const -&gt; MyBoundaryFields&lt;D&gt; {\n    const auto bc_flds = BoundaryFields&lt;D&gt;{};\n    return bc_flds;\n  }\n};\n</code></pre> <p>All the coordinate conversions and field staggering is performed automatically, so all specified coordinates are in physical units, while the field values are normalized to \\(B_0\\).</p> <p>If the <code>struct</code> does not explicitly define certain components (in the example above, <code>ex2</code>, <code>ex3</code>, <code>bx2</code>, <code>bx3</code> are omitted), the code will not apply any specific boundaries to them. If you wish to damp the fields to zero, you will have to explicitly specify it, e.g.,</p> <pre><code>  // ...\n  Inline auto ex2(const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    return ZERO;\n  }\n</code></pre> <p>Matching boundaries in different directions</p> <p>You might need to have separate matching boundaries (i.e., fields being matched to different values) in different directions. For example, you may have one set of BCs in \\(\\pm x\\), while completely different conditions in \\(\\pm y\\). This can be achieved by specifying separately <code>MatchFieldsInX1</code> (for \\(\\pm x\\)) and <code>MatchFieldsInX2</code> (in \\(\\pm y\\)) instead of <code>MatchFields</code>. The function will still take time as the argument and return a class defining BCs in each distinct direction.</p>"},{"location":"content/2-code/6-problem_generators/#fixed-field-boundaries","title":"Fixed field boundaries","text":"<p> 1.2.0 </p> <p>With <code>FIXED</code> boundary conditions you can explicitly set the field components at the boundary cells to a given predefined value. For this you need to define a method <code>FixFieldsConst(const bc_in&amp;, const em&amp; comp)</code> which should return a pair of the value you want to set, and a bool if the component should be set or not.</p> <p>In this example, the \\(E^2\\), and \\(E^3\\) components are set to zero in the boundary, while all the other components remain untouched:</p> <pre><code>// ...\n\ntemplate &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // ...\n\n  // This function is called within the BC kernel\n  // the first element in the pair ...\n  // ... determines the value to which the component is set\n  // while the second bool component ...\n  // ... determines whether that component is updated at all\n  // ... i.e., if bool is false, the first component is simply ignored\n  auto FixFieldsConst(const bc_in&amp;, const em&amp; comp) const\n      -&gt; std::pair&lt;real_t, bool&gt; {\n      if (comp == em::ex2) {\n        return { ZERO, true };\n      } else if (comp == em::ex3) {\n        return { ZERO, true };\n      } else {\n        return { ZERO, false };\n      }\n    }\n};\n</code></pre> <p>Changing BCs at runtime</p> <p>One can change the boundary conditions at runtime by directly accessing the global metadomain, for example, in the <code>CustomPostStep</code> routine. Below is an example of how to do that (chaging boundaries to <code>MATCH</code> for fields and <code>ABSORB</code> for particles in \\(\\pm x\\) and \\(\\pm y\\) at a certain time): <pre><code>template &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // ...\n  Metadomain&lt;S, M&gt;&amp; metadomain;\n  bool bc_opened { false };\n  // ...\n  inline PGen(const SimulationParams&amp; p, Metadomain&lt;S, M&gt;&amp; m) : \n    : arch::ProblemGenerator&lt;S, M&gt;(p)\n    , metadomain { m } {}\n\n  void CustomPostStep(timestep_t, simtime_t time, Domain&lt;S, M&gt;&amp;) {\n    if ((time &gt; t_open) and (not bc_opened)) { // (1)!\n      bc_opened = true;\n      metadomain.setFldsBC(bc_in::Mx1, FldsBC::MATCH); // (2)!\n      metadomain.setPrtlBC(bc_in::Mx1, PrtlBC::ABSORB);\n      metadomain.setFldsBC(bc_in::Px1, FldsBC::MATCH);\n      metadomain.setPrtlBC(bc_in::Px1, PrtlBC::ABSORB);\n    }\n    // ...\n  }\n};\n</code></pre></p> <ol> <li>check if not already opened &amp; open after certain time</li> <li>first argument is the direction in which to change the boundary; <code>M</code>/<code>P</code> stand for minus/plus</li> </ol>"},{"location":"content/2-code/6-problem_generators/#custom-post-timestep-routines","title":"Custom post-timestep routines","text":"<p>Often times, one needs to intervene to the simulation process to perform some custom operations by updating the fields or the particles (for instance, to apply special boundary conditions, inject particles etc.). The safest way of performing this is at the end of each timestep, when all the quantities have already been computed and stored. For that, Entity allows users to define another special method in the problem generator called <code>CustomPostStep</code>. It accepts the current timestep, the current physical time, and the local subdomain as a parameter. For instance, to inject particles at a given rate, one can write:</p> <pre><code>template &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // ... \n\n  void CustomPostStep(std::size_t, long double, Domain&lt;S, M&gt;&amp; domain) {\n    // ... some energy/spatial distribution &amp; injector here (see above) ...\n    arch::InjectNonUniform&lt;S, M, /* ... */&gt;( /* ... */ );\n  }\n};\n</code></pre> <p>Or you may also manually access the fields and particles through the <code>domain.fields</code> and <code>domain.species[...]</code> objects, respectively, and perform any operations you need. Be mindful, however, that all the raw quantities stored within the <code>domain</code> object are in the code units (for more details, see the fields and particles section; for ways to convert from one system/basis to another, see the metric section).</p>"},{"location":"content/2-code/6-problem_generators/#particle-purging","title":"Particle purging","text":"<p>The custom post-timestep can also be used to purge particles within a given region of the domain. This can be useful to inject fresh plasma in the part of the domain and make sure that the old plasma has been removed before replenishing.</p> <p>Conserving the currents</p> <p>Keep in mind that removing charged particles in general will violate charge conservation, unless the \\(E\\)-fields are then explicitly forced to obey \\(\\nabla\\cdot\\boldsymbol{E}=4\\pi \\rho\\). For that reason, in the example below, along with purging particles we will also reset the fields in that region.</p> <p>Below we use the example from the shock setup to illustrate particle purging routine. Here, all plasma particles that have \\(x&gt;x_{\\rm min}\\) (denoted <code>xmin</code> in the code) is purged and the fields are reset before new plasma is injected.</p> <pre><code>template &lt;Dimension D&gt;\nstruct InitFields {\n  /**\n   * Defines the fields at initialisation\n  **/\n\n  Inline auto ex1(const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    return /*...*/; //(1)!\n  }\n\n  Inline auto bx1(const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    return /*...*/;\n  }\n};\n\ntemplate &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  InitFields&lt;D&gt; init_flds;\n\n  void CustomPostStep(timestep_t step, simtime_t time, Domain&lt;S, M&gt;&amp; domain) {\n    /**\n     * tag particles inside the injection zone as dead\n    **/\n    const auto&amp; mesh = domain.mesh;\n    // loop over particle species\n    for (auto s { 0u }; s &lt; 2; ++s) {\n      // get particle properties\n      auto&amp; species = domain.species[s];\n      auto  i1      = species.i1;\n      auto  dx1     = species.dx1;\n      auto  tag     = species.tag;  \n\n      Kokkos::parallel_for(\n          \"RemoveParticles\",\n          species.rangeActiveParticles(),\n          Lambda(index_t p) {\n            // check if the particle is already dead\n            if (tag(p) == ParticleTag::dead) {\n              return;\n            }\n            // convert particle position to grid coordinates\n            const auto x_Cd = static_cast&lt;real_t&gt;(i1(p)) +\n                              static_cast&lt;real_t&gt;(dx1(p));\n            // convert grid coordinates to physical coordinates\n            const auto x_Ph = mesh.metric.template convert&lt;1, Crd::Cd, Crd::XYZ&gt;(\n              x_Cd);\n\n            // if the particle position is to the right of xmin, tag it as dead\n            if (x_Ph &gt; xmin) {\n              tag(p) = ParticleTag::dead;\n            }\n          });\n      }\n\n    /*\n      Reset the fields inside the purged region\n    */\n    // define indices range to reset fields\n    // (not including ghost zones in either direction)\n    boundaries_t&lt;bool&gt; incl_ghosts;\n    for (auto d = 0; d &lt; M::Dim; ++d) {\n      incl_ghosts.push_back({ false, false });\n    }\n\n    // define the rectangular box region where fields are reset\n    boundaries_t&lt;real_t&gt; purge_box;\n    // loop over all dimension\n    for (auto d = 0u; d &lt; M::Dim; ++d) {\n      if (d == 0) {\n        purge_box.push_back({ xmin, global_xmax });\n      } else {\n        purge_box.push_back(Range::All);\n      }\n    }\n\n    // convert physical extent to a range of cells\n    const auto extent = domain.mesh.ExtentToRange(purge_box, incl_ghosts);\n    // record the range min/max boundaries in each dimension\n    tuple_t&lt;std::size_t, M::Dim&gt; x_min { 0 }, x_max { 0 };\n    for (auto d = 0; d &lt; M::Dim; ++d) {\n      x_min[d] = extent[d].first;\n      x_max[d] = extent[d].second;\n    }\n\n    Kokkos::parallel_for(\"ResetFields\",\n                         CreateRangePolicy&lt;M::Dim&gt;(x_min, x_max),\n                         arch::SetEMFields_kernel&lt;decltype(init_flds), S, M&gt; {\n                           domain.fields.em,\n                           init_flds,\n                           domain.mesh.metric });\n\n  }\n};\n</code></pre> <ol> <li>because we essentially remove the particles, the returned E-fields must have zero divergence.</li> </ol>"},{"location":"content/2-code/6-problem_generators/#custom-external-force","title":"Custom external force","text":"<p>Similar to all the other custom routines, one may also define a custom external force which will optionally be applied to the particles together with the electromagnetic pusher. This is done by defining an arbitrary class with an instance named <code>ext_force</code>, which implements three methods: <code>fx1()</code>, <code>fx2()</code>, <code>fx3()</code>. For instance, to apply a force in the \\(x_1\\) direction decaying over time, one would write:</p> <pre><code>template &lt;Dimension D&gt;\nstruct PushDaTempo {\n  // specify which species to apply the force to\n  const std::vector&lt;unsigned short&gt; species { 1, 2 };\n\n  PushDaTempo(real_t f, real_t t) : force { f }, tau { t } {}\n\n  Inline auto fx1(const unsigned short&amp;,\n                  const real_t&amp; time,\n                  const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    return force * math::exp(-time / tau);\n  }\n\n  Inline auto fx2(const unsigned short&amp;,\n                  const real_t&amp;,\n                  const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    return ZERO;\n  }\n\n  Inline auto fx3(const unsigned short&amp;,\n                  const real_t&amp;,\n                  const coord_t&lt;D&gt;&amp;) const -&gt; real_t {\n    return ZERO;\n  }\nprivate:\n  const real_t force, tau;\n};\n\n// and then in the problem generator class\ntemplate &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // ...\n  PushDaTempo&lt;S, M&gt; ext_force;\n  // and read the parameters from the input\n  inline PGen(const SimulationParams&amp; p, const Metadomain&lt;S, M&gt;&amp; global_domain)\n    : arch::ProblemGenerator&lt;S, M&gt; { p }\n    , ext_force { p.template get&lt;real_t&gt;(\"setup.force\"),\n                  p.template get&lt;real_t&gt;(\"setup.tau\") }\n    {}\n};\n</code></pre> <p>Again, as everything else in the problem generator, the force (rather, the acceleration) must be returned in the local tetrad basis and the passed coordinates are the physical coordinates.</p> <p>All functions are optional</p> <p>Note, that among the functions mentioned throughout this section, you may specify only the ones you actually need, and ignore the ones you don't (i.e., there is no need to provide dummy functions that return zero), as the code will automatically determine at compile-time which functions are present.</p>"},{"location":"content/2-code/6-problem_generators/#custom-external-current","title":"Custom external current","text":"<p> 1.2.0 </p> <p>There are specific instances, where one needs to apply a source term to the Ampere's law as additional (external) currents (e.g., driven turbulence). This can easily be achieved by defining an arbitrary class instance called <code>ext_current</code>, which implements 3 methods: <code>jx1()</code>, <code>jx2()</code>, <code>jx3()</code> -- each returning the corresponding external current component in units of \\(j_0\\). For instance, to apply a constant sinusoidal current \\(j_3\\) as a function of \\(x_1\\), one could write:</p> <pre><code>template &lt;Dimension D&gt;\nstruct ImmaRealLiveWire { //(1)!\n  ImmaRealLiveWire(real_t amplitude, real_t k)\n    : amp { amplitude }\n    , k { k } {};\n\n  Inline auto jx1(const coord_t&lt;D&gt;&amp; x_Ph) const -&gt; real_t {\n      return ZERO;\n    }\n\n  Inline auto jx2(const coord_t&lt;D&gt;&amp; x_Ph) const -&gt; real_t {\n      return ZERO;\n    }\n\n  Inline auto jx3(const coord_t&lt;D&gt;&amp; x_Ph) const -&gt; real_t {\n      return amp * math::sin(k * x_Ph[0]);\n    }\n\n  private:\n    const real_t amp, k; \n};\n\ntemplate &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  ImmaRealLiveWire&lt;D&gt; ext_current;\n\n  inline PGen(const SimulationParams&amp; p, const Metadomain&lt;S, M&gt;&amp; global_domain)\n    : arch::ProblemGenerator&lt;S, M&gt; { p }\n    , ext_current { p.template get&lt;real_t&gt;(\"setup.amplitude\"),\n                    p.template get&lt;real_t&gt;(\"setup.k\") } //(2)!\n    {}\n};\n</code></pre> <ol> <li> <p>name of the class is not important, as long as its instance declared in the problem generator class itself is called <code>ext_current</code>.</p> </li> <li> <p>directly initialize the <code>ext_current</code> in the <code>PGen</code> constructor by passing values read from the input.</p> </li> </ol> <p>Keep in mind, that in contrast to the external force, all of the components of the current have to be defined in the structure, even if they return zero. </p> <p>External current</p> <p>The external current routine is currently only limited to work in Minkowski space. </p>"},{"location":"content/2-code/6-problem_generators/#custom-field-output","title":"Custom field output","text":"<p>The code also allows for custom-defined fields to be written together with other field quantities during the output. To enable that, simply define the name of your field in the input file:</p> <pre><code>[output.fields]\n  ...\n  custom = [\"my_field\"]\n  ...\n</code></pre> <p>There can be as many custom fields as one needs. And then in the problem generator, populate the corresponding field by defining the following function:</p> <pre><code>void CustomFieldOutput(const std::string&amp;   name,//(1)!\n                       ndfield_t&lt;M::Dim, 6&gt; buffer,//(2)!\n                       index_t              index,//(3)!\n                       timestep_t,//(4)!\n                       simtime_t,//(5)!\n                       const Domain&lt;S, M&gt;&amp;  domain) {//(6)!\n  if (name == \"my_field\") {\n    // 1D example (can be easily generalized)\n    if constexpr (M::Dim == Dim::_1D) {\n      const auto&amp; EM = domain.fields.em;\n      Kokkos::parallel_for(\n        \"MyField\",\n        domain.mesh.rangeActiveCells(),\n        Lambda(index_t i1) {\n          const auto      i1_ = COORD(i1);\n          coord_t&lt;M::Dim&gt; x_Ph { ZERO };\n          // convert coordinate to physical basis:\n          metric.template convert&lt;Crd::Cd, Crd::Ph&gt;({ i1_ }, x_Ph);\n          // compute whatever needs to be written\n          // ... may also depend on the EM fields from the `domain`\n          // ... in this example -- output Ex * x^2\n          buffer(i1, index) = SQR(x_Ph[0]) *\n                              metric.template transform&lt;1, Idx::U, Idx::T&gt;(\n                                { i1_ + HALF },\n                                EM(i1, em::ex1));\n          // here we also convert Ex1(i + 1/2) to Tetrad basis\n        });\n    }\n  } else {\n    raise::Error(\"Custom output not provided\", HERE);\n  }\n}\n</code></pre> <ol> <li>the same name that went into the input file</li> <li>buffer array where the field is going to be written into</li> <li>an index of the buffer array where the field is written into</li> <li>completed step index</li> <li>completed step time in physical units</li> <li>reference of the local subdomain</li> </ol> <p>Alternatively, you can precompute the desired quantity in the <code>CustomPostStep</code> function and then simply copy to the buffer in the same function:</p> <pre><code>// assuming 2D and that the desired quantity is saved in `cbuff`\ntemplate &lt;SimEngine::type S, class M&gt;\nstruct PGen : public arch::ProblemGenerator&lt;S, M&gt; {\n  // ...\n\n  array_t&lt;real_t**&gt; cbuff;\n\n  // ...\n\n  void CustomPostStep(timestep_t step, simtime_t, Domain&lt;S, M&gt;&amp; domain) {\n    if (step == 0) {\n      // allocate the array at time = 0\n      cbuff = array_t&lt;real_t**&gt;(\"cbuff\",\n                                domain.mesh.n_all(in::x1),\n                                domain.mesh.n_all(in::x2));\n    }\n    // populate the buffer (can be done at specific timesteps)\n    Kokkos::parallel_for(\n      \"FillCbuff\",\n      domain.mesh.rangeActiveCells(),\n      Lambda(index_t i1, index_t i2) {\n        // ...\n      });\n  }\n\n  void CustomFieldOutput(const std::string&amp;    name,\n                         ndfield_t&lt;M::Dim, 6&gt; buffer,\n                         index_t              index,\n                         timestep_t,\n                         simtime_t,\n                         const Domain&lt;S, M&gt;&amp;) {\n    if (name == \"my_field\") {\n      Kokkos::deep_copy(Kokkos::subview(buffer, Kokkos::ALL, Kokkos::ALL, index), cbuff);\n    } else {\n      // ...\n    }\n  }\n};\n</code></pre> <p>Keep in mind that the custom field output is written as-is, i.e., no additional interpolation or transformation is applied. So make sure the quantity you output is covariant (i.e., does not depend on the resolution or the stretching of coordinates; essentially, always output \"physical\" covariant/contravariant vectors or transform them to the tetrad basis).</p>"},{"location":"content/2-code/6-problem_generators/#custom-stats","title":"Custom stats","text":"<p>One can also write custom user-defined stats.</p> <pre><code>[output.stats]\n  ...\n  custom = [\"my_stat\"]\n  ...\n</code></pre> <p>Just like in the case with custom fields, you specify a special function in the problem generator class which has a name <code>CustomStat</code> and returns a single real value:</p> <pre><code>auto CustomStat(const std::string&amp;   name,//(1)!\n                timestep_t,\n                simtime_t,\n                const Domain&lt;S, M&gt;&amp;  domain) -&gt; real_t {//(2)!\n  if (name == \"my_stat\") {\n    return 42.0;\n  } else {\n    raise::Error(\"Custom stat not provided\", HERE);\n  }\n}\n</code></pre> <p>Reduction from all meshblocks of the custom stat is done automatically (the values are summed).</p> <ol> <li>the same name that went into the input file</li> <li>reference of the local subdomain</li> </ol>"},{"location":"content/2-code/7-guidelines/","title":"Code guidelines","text":"<p>Two general places to find information on C++-specific questions are cppreference and learncpp. For Kokkos-related questions, one can refer to the Kokkos documentation as well as the Kokkos tutorials for practical examples. For ADIOS2-related issues, refer to the ADIOS2 documentation, and the examples on their github.</p> <p>When not sure what a specific function does, or how to include a particular module, first check the documentation (you can do a keyword search). Another good option to figure things out on your own, is to look at how the particular modules/functions in questions are used in the unit tests (in the corresponding <code>tests/</code> directories). If non of that answers your questions, please feel free to open a github issue.</p> <p>Submitting a github issue</p> <p>When submitting a github issue, please make sure to include all the parameters of the simulation as well as the code version tag and the git hash of the commit you are using. These are automatically printed into <code>stdout</code> at the beginning of the simulation, and are also saved into <code>&lt;simname&gt;.info</code> file. If you believe the problem is coming from your problem generator, please also include the problem generator file. </p>"},{"location":"content/2-code/7-guidelines/#codestyle-guide","title":"Codestyle guide","text":""},{"location":"content/2-code/7-guidelines/#clang-format","title":"<code>clang-format</code>","text":"<p>To maintain coherence throughout the source code, we use <code>clang-format</code> to enforce a uniform style. A corresponding <code>.clang-format</code> file with all the style-related settings can be found in the root directory of the code. To use this, one needs to have the <code>clang-format</code> executable (typically provided with the <code>llvm</code> package). After installing the <code>clang-format</code> itself (check by running <code>clang-format --version</code>), you can use it either manually by running <code>clang-format .</code> in the route directory of the code, or attach it to your favorite code editor to run on save. For VSCode, the recommended extension is <code>xaver.clang-format</code>, for vim -- <code>rhysd/vim-clang-format</code>, for nvim -- <code>stevearc/conform.nvim</code>, for emacs.</p>"},{"location":"content/2-code/7-guidelines/#general","title":"General","text":"<ul> <li> <p>Use <code>const</code> and <code>auto</code> declarations where possible.</p> </li> <li> <p>For real-valued literals, use <code>ONE</code>, <code>ZERO</code>, <code>HALF</code> etc. instead of <code>1.0</code>, <code>0.0</code>, <code>0.5</code> to ensure the compiler will not need to cast. If the value is not defined as a macro, use <code>static_cast&lt;real_t&gt;(123.4)</code>.</p> </li> <li> <p>In problem generators, it is usually a good practice to enumerate all the code configurations the code works with. For instance, if your code only works with 3D Minkowski metric in SRPIC, include the following in your <code>PGen</code> class:   <pre><code>template &lt;SimEngine::type S, class M&gt;\nstruct PGen : public ProblemGenerator&lt;S, M&gt; {\n  static constexpr auto engines {\n    traits::compatible_with&lt;SimEngine::SRPIC&gt;::value\n  };\n  static constexpr auto metrics {\n    traits::compatible_with&lt;Metric::Minkowski&gt;::value\n  };\n  static constexpr auto dimensions {\n    traits::compatible_with&lt;Dim::_3D&gt;::value\n  };\n  // ...\n};\n</code></pre>   This will allow the code to throw a compile-time error if the problem generator is used with an incompatible configuration.</p> </li> </ul>"},{"location":"content/2-code/7-guidelines/#developers","title":"Developers","text":"<ul> <li>Use <code>{}</code> in declarations to signify a null (placeholder) value for the given variable:   <pre><code>auto a { -1 }; // &lt;- value of `a` will be changed later (-1 is a placeholder)\nauto b = -1; // &lt;- value of `b` is known at the time of declaration (but may change later)\nconst auto b = -1; // &lt;- value of `b` is not expected to change later\n</code></pre></li> <li> <p>Each header file has to have a description at the top, consisting of the following fields:</p> <ul> <li><code>@file</code> [required] the name of the file (as it should be included in other files)</li> <li><code>@brief</code> [required] brief description of what the file contains</li> <li><code>@implements</code> list of class/function/macros implementations<ul> <li>structs/classes in this section have no prefix (templates are marked with <code>&lt;&gt;</code>)</li> <li>functions are marked with their return type, e.g. <code>-&gt; void</code></li> <li>type aliases have a prefix <code>type</code></li> <li>enums or enum-like objects are marked with <code>enum</code></li> <li>macros have a prefix <code>macro</code></li> <li>all of the above are also marked with their respective namespaces (if any): <code>namespace::</code></li> </ul> </li> <li><code>@cpp:</code> list of cpp files that implement the header</li> <li><code>@namespaces:</code> list of namespaces defined in the file</li> <li><code>@macros:</code> list of macros that the file depends on</li> <li><code>@note</code> any additional notes (stack as many as necessary)</li> </ul> <p>Example</p> <pre><code>/**\n  * @file output/particles.h\n  * @brief Defines the metadata for particle output\n  * @implements\n  *   - out::OutputParticle\n  * @cpp:\n  *   - particles.cpp\n  */\n</code></pre> </li> <li> <p><code>#ifdef</code> macros should be avoided. Use C++17 type traits or <code>if constexpr ()</code> expressions to specialize functions and classes instead (ideally, specialize them explicitly). <code>#ifdef</code>-s are only acceptable in platform/library-specific parts of the code (e.g., <code>MPI_ENABLED</code>, <code>GPU_ENABLED</code>, <code>DEBUG</code>, etc.).</p> </li> <li> <p>Header files should start with <code>#ifndef ... #define ...</code> and end with <code>#endif</code>; do not use <code>#pragma</code> guards. The name of the macro should be the same as the name of the file in uppercase, with underscores instead of dots and slashes. For example, for <code>global/utils/formatting.h</code>, the macro should be <code>GLOBAL_UTILS_FORMATTING_H</code>.</p> </li> <li> <p>There is no difference between <code>.h</code> and <code>.hpp</code> files as both indicate C++ header files. As a consistency convention, we use <code>.h</code> for common headers which may be included from multiple <code>.cpp</code> files (e.g., metrics), while <code>.hpp</code> are very specific headers for only a single (or a couple of) <code>.cpp</code> file (e.g. kernels).</p> </li> </ul>"},{"location":"content/2-code/7-guidelines/#recommendations","title":"Recommendations","text":"<ul> <li> <p>Do assertions on parameters and quantities whenever possible. Outside the kernels, use <code>raise::Error(message, HERE)</code> and <code>raise::ErrorIf(condition, message, HERE)</code> to throw exceptions. Inside the kernels, use <code>raise::KernelError(HERE, message, **args)</code>. To enable compile-time errors, use <code>static_assert(condition, message)</code>. The <code>HERE</code> keyword is macro that includes the filename and line number in the error message.</p> </li> <li> <p>When writing class or function templates, it is always a good practice to ensure the template argument is valid (depending on the context). When doing that, use SFINAE (see, e.g., <code>arch/traits.h</code>) to test whether the type is valid. For example:   <pre><code>template &lt;typename T&gt;\nusing foo_t = decltype(&amp;T::foo);\n\ntemplate &lt;typename T&gt;\nusing b_t = decltype(&amp;T::b);\n\ntemplate &lt;class B&gt;\nclass A {\n  // compile-time fail if B does not have a `foo()` method or a `b` member\n  static_assert(traits::has_method&lt;foo_t, B&gt;::value, \"B must have a `foo()` method\");\n  static_assert(traits::has_member&lt;b_t, B&gt;::value, \"B must have a `b` member\");\n};\n</code></pre></p> </li> </ul>"},{"location":"content/2-code/datalayout/","title":"Datalayout","text":"<p>Code</p> <p>Field quantities and particle coordinates are stored in the simulation object with following structure.</p> <pre><code>// fields (1)\nSimulation::Meshblock.em(i, j, k, fld::ex1);\nSimulation::Meshblock.em(i, j, k, fld::ex2);\nSimulation::Meshblock.em(i, j, k, fld::ex3);\n\nSimulation::Meshblock.em(i, j, k, fld::bx1);\nSimulation::Meshblock.em(i, j, k, fld::bx2);\nSimulation::Meshblock.em(i, j, k, fld::bx3);\n//                          ^  ^\n//                          |   \\\n//                          |    not present in 1D/2D\n//                  not present in 1D\n\n// particles (2)\nSimulation::Meshblock.particles[species_id].i1(prtl_id);\nSimulation::Meshblock.particles[species_id].i2(prtl_id);  // not present in 1D\nSimulation::Meshblock.particles[species_id].i3(prtl_id);  // not present in 1D/2D\nSimulation::Meshblock.particles[species_id].dx1(prtl_id);\nSimulation::Meshblock.particles[species_id].dx2(prtl_id); // not present in 1D\nSimulation::Meshblock.particles[species_id].dx3(prtl_id); // not present in 1D/2D\n\nSimulation::Meshblock.particles[species_id].ux1(prtl_id);\nSimulation::Meshblock.particles[species_id].ux2(prtl_id);\nSimulation::Meshblock.particles[species_id].ux3(prtl_id);\n</code></pre> <ol> <li> Even though we employ <code>(i, j, k)</code> indexing in the code the field components are staggered not only in time but also spatially.</li> <li> Particle velocities are staggered in time w.r.t. the coordinates.</li> </ol>"},{"location":"content/2-code/testpage/","title":"Testpage","text":"<p>TEST PAGE</p>"},{"location":"content/events/sceecs2024/instructions/","title":"SCEECS Summer School 2024","text":"<p>There are two environments for running the code: the WUSTL cluster, and the CCA (Flatiron Inst. Cluster). Either should work ok, but the resources on the CCA cluster might be limited. Below are the instructions for both of these:</p> WUSTL clusterCCA cluster <ol> <li> <p>First, open a terminal, and connect to the WUSTL cluster using     <pre><code># enter your username here which starts with `WG-`\nssh &lt;USERNAME&gt;@compute1-client-1.ris.wustl.edu\n</code></pre></p> </li> <li> <p>Make sure you have the proper <code>LSF_DOCKER_VOLUMES</code> environment variable set, by either running the following once:     <pre><code>mkdir -p /storage1/fs1/workshops/Active/ExtremePlasmas/$USER\nexport LSF_DOCKER_VOLUMES=\"/storage1/fs1/workshops/Active/ExtremePlasmas/$USER:/scratch $HOME:$HOME\"\n</code></pre>     or adding it to your <code>~/.bashrc</code> file and running <code>source ~/.bashrc</code>.</p> </li> <li> <p>Launch the following job that requests a GPU with at least 16GB of VRAM (GPU memory), and 64 GB of RAM (CPU memory):     <pre><code>thpc-terminal -q general-interactive -R 'gpuhost rusage[mem=64GB]' -gpu 'num=1:gmem=16G' -a 'docker(morninbru/arch-entity:cuda)' /bin/bash\n</code></pre></p> <p>Notice that we are using the <code>general-interactive</code> queue, as the <code>workshop</code> queue does not allow custom containers.</p> </li> <li> <p>Clone the code from the github repository by running:     <pre><code>git clone -b v1.0.0rc --recursive https://github.com/entity-toolkit/entity.git\ncd entity\n</code></pre></p> </li> </ol> <ol> <li> <p>Go to the following website in your browser: binder.flatironinstitute.org. </p> </li> <li> <p>Click \"Sign in with Google\" and... well... sign in with Google.</p> </li> <li> <p>Under \"Choose project\" enter <code>hhakobyan</code> for the \"Owner,\" and <code>entity</code> for the \"Project,\" and hit \"Launch.\"</p> </li> <li> <p>You should be greeted with a Jupyter lab window, and the <code>entity</code> is already cloned to the home directory.</p> </li> <li> <p>You may open the terminal from the Launcher.</p> </li> </ol> <p>GPU architecture</p> <p>You will need to know the architecture of the GPU (in this case it will be an NVIDIA GPU). You can find that out by running <code>nvidia-smi</code> in the shell. It should either be <code>Tesla V100</code> or <code>Ampere A100</code>/<code>A40</code>. You will only need the first letter: <code>A</code> or <code>V</code> -- this indicates the microarchitecture of the GPU which you need to tell the code to optimize compilation for that specific device.</p>"},{"location":"content/events/sceecs2024/instructions/#compiling-and-running-tests","title":"Compiling and running tests","text":"<p>The code is prepared for running in two steps: first you configure it with whatever settings you need, and then compile. The command for configuring the code in test mode is the following:</p> <pre><code>#   directory for the compilation files\n#          |\n#          v\ncmake -B build -D TESTS=ON -D output=ON -D Kokkos_ENABLE_CUDA=ON -D Kokkos_ARCH_AMPERE80=ON\n#                   ^             ^                 ^                        ^\n#                   |             |                 |                        |\n#         enable test mode        |      enable CUDA (GPU support)           |\n#                         enable the output                         microarchitecture of the GPU\n</code></pre> <p>Compilation is always done with the following command: <pre><code>cmake --build build -j 4\n</code></pre></p> <p>If you are running this on the <code>A100</code>/<code>A40</code> GPUs, specify <code>Kokkos_ARCH_AMPERE80=ON</code> as shown above, otherwise if compiling on <code>V100</code> GPUs, use <code>Kokkos_ARCH_VOLTA70=ON</code>. </p> <p>This might take some time, as <code>CUDA</code> compiler is typically quite slow, and test mode compiles the code with different configurations. Once it is done (you should see <code>100%</code> written in green). You can now run the tests with the following command:</p> <pre><code>ctest --test-dir build\n</code></pre> <p>All tests (there are about 33 of them) should complete with a \"Passed\" mark. </p>"},{"location":"content/events/sceecs2024/instructions/#exercise-1","title":"Exercise #1","text":"<p>In this exercise we will inspect the so-called Weibel/filamentation instability in pair-plasmas. The initial setup is quite simple: two cold (\\(T\\ll m_e c^2\\)) neutral (\\(n_+=n_-\\)) pair-plasma beams counterstream in \\(\\pm z\\) direction with relativistic velocities \\(\\pm\\bm{u}_d\\). The net current in the system is exactly \\(0\\). The simulation is performed in the \\(xy\\), so only \\(k_{xy}\\) perturbation modes can be excited. The system with a kinetic equilibrium, which is, however unstable, as any perturbation in the current density in \\(z\\) will drive a magnetic field in \\(xy\\) plane, which in turn will tend to amplify the current density in \\(z\\).</p>"},{"location":"content/events/sceecs2024/instructions/#preliminaries","title":"Preliminaries","text":"<p>The problem generator is located in <code>setups/srpic/weibel/pgen.hpp</code> of the Entity repository. To configure/compile use the following command: <pre><code>cmake -B build -D pgen=srpic/weibel -D output=ON -D Kokkos_ENABLE_CUDA=ON -D Kokkos_ARCH_AMPERE80=ON\n#                        ^               ^                    ^                           ^\n#                        |               |                    |                           |\n#                        |        enable code output          |                    architecture of the GPU\n#             problem generator to use             enable GPU support with CUDA    (A100/A40/etc. use **_AMPERE80, V100 -- use **_VOLTA70)\n#                                                                                  to check the architecture -- run `nvidia-smi`\ncmake --build build -j 4\n</code></pre></p> <p>Once the compilation is done, you should have the executable file in <code>build/src/entity.xc</code>. The other ingredient required to run the code is the input file, located together with the problem generator: <code>setups/srpic/weibel/weibel.toml</code>. To run the code, copy both the executable and the input <code>toml</code> file to the same directory:</p> <pre><code># example:\ncd run-directory\ncp /path/to/entity/build/src/entity.xc .\ncp /path/to/entity/setups/srpic/weibel/weibel.toml .\n# then run\n./entity.xc -input weibel.toml\n</code></pre> <p>Carefully inspect the <code>weibel.toml</code>. We will be exploring the dependency of the instability growth rate on two parameters: the plasma skin-depth, \\(d_\\pm\\), and the relativistic drift velocity \\(u_d\\). Run the simulation with several values of these parameters, and compare the growth rate with the analytic expression: \\(\\Gamma = \\omega_\\pm \\beta_d \\sqrt{2/\\gamma_d}\\), where \\(\\gamma_d = \\sqrt{1+u_d^2}\\) and \\(\\beta_d = u_d / \\gamma_d\\).</p>"},{"location":"content/events/sceecs2024/instructions/#exercise-2","title":"Exercise #2","text":"<p>The problem generator that sets up a double-periodic Harris layer can be found in <code>setups/wip/reconnection/pgen.hpp</code> and can be turned on using the <code>-D pgen=wip/reconnection</code>. Compile and run the reconnection setup similar to the exercise above. Plot the density, the current density, and the magnetic field at the beginning and at late time of the simulation (or better yet, make a movie!). Measure the quasi-steady-state rate of the reconnection, by plotting the \\(y\\) (vertical) component of the \\(\\bm{E}\\times\\bm{B} / |\\bm{B}|^2\\) (averaged in \\(x\\) at a specific distance in \\(y\\) from either of the current layers) over time. </p>"},{"location":"content/fun/coords/","title":"Axisymmetric coordinates","text":"<p>Below for visualization purposes we demonstrate three different axisymmetric systems: a regular spherical \\((r,\\theta,\\phi)\\), an equal area and a \"quasi-spherical\" \\((\\xi,\\eta,\\phi)\\), where </p> \\[ \\text{equal area} =          \\begin{cases}             \\xi = \\log{(r)}, \\\\             \\eta = -\\cos{\\theta}, \\\\             \\phi = \\phi         \\end{cases} ~~~~ \\text{quasi-spherical} =          \\begin{cases}             \\xi = \\log{(r - r_0)}, \\\\             \\eta:~\\theta = \\eta + 2h \\eta (\\pi - 2 \\eta) (\\pi - \\eta) / \\pi^2, \\\\             \\phi = \\phi         \\end{cases} \\] <p>with \\(r_0\\) and \\(h\\) being user-controlled parameters. The interactive plot below demonstrates the difference between grids, uniformly discretized in each of these coordinate systems.</p>"},{"location":"content/fun/cubed_sphere/","title":"Cubed-sphere geometry","text":"<p>Hint</p> <p>The panel is interactive</p>"},{"location":"content/fun/particle-shapes/","title":"2D particle shapes","text":"<p>This tool shows the intersection of 2D particle shapes with the different field components defined on the grid. Can be useful when debugging grid-particle interpolation routines, as well as current depositions (for the latter, it also shows initial and final positions of the particle). The distance between the initial and final particle position is limited to \\(\\Delta x / \\sqrt{2}\\) (CFL condition). Both shape functions (initial and final) can be dragged. </p>"},{"location":"content/fun/pushers/","title":"Particle pushers","text":"<p>Under construction</p> <p>Some of the functionality of this tool is still in development! </p> <p>This demo tool allows you to integrate particle trajectories in the analytically prescribed electric and magnetic fields. You may vary the fields themselves below (fields can be non-uniform and even time-varying), inject particles of different charge-to-mass ratios (by either dragging on the screen or using an injector at the bottom), and pick different parameters of the simulation (like the integration timestep, and the algorithm). You may also visualize your fields by ticking the checkboxes next to the corresponding components (the colormap is automatically rescaled to accomodate for its dynamic range).</p> <p>The tool uses natural units, in which \\(c = 1\\), and all the particle velocities are four-velocities, i.e., \\(\\bm{u}\\equiv \\gamma\\bm{\\beta}\\). When dragging on the screen with the right click, the charge-to-mass for the injected particle is \\(q/m=1\\) (red), while the right click (or shift + left click if you're using touchpad) injects a particle with \\(q/m=-1\\) (blue). Electric and magnetic fields can be either picked from a limited number of presets using the dropdown, or you can type them in in a functional form (as a function of \\(x\\), \\(y\\), \\(z\\) position and time, \\(t\\)) using the boxes below (the evaluator accepts all sorts of expressions outlined here).</p> Field presets:  E-cross-B drift betatron drift grad-B drift magnetic dipole magnetic mirror magnetic noise Pusher:  Boris Vay Implicit $x$ $y$ $z$ $\\bm{E}$ $\\bm{B}$ $x$:  $y$:  $u_x$:  $u_y$:  $u_z$:  $q/m$:"},{"location":"content/useful/cluster-setups/","title":"Cluster setups","text":"<p>This section goes over some instructions on how to compile &amp; run the <code>Entity</code> on some of the most widely utilized clusters. While the main libraries we rely on, <code>Kokkos</code> and <code>ADIOS2</code> can be built in-tree (i.e., together with the code when you launch the compiler), it is nonetheless recommended to pre-install them separately (if not already installed on the cluster) and use them as external dependencies, since that will significantly cut down the compilation time.</p> <p>Contribute!</p> <p>If you don't see a cluster you are running the code on here, please be kind to those that will come after us and contribute instructions for that specific cluster. <code>Entity</code> is only as strong as the community supporting it, and by contributing a few sentences, you may have an immense effect in the longrun.</p> <code>Stellar</code> (Princeton)<code>Zaratan</code> (UMD)<code>Rusty</code> (CCA)<code>Vista</code> (TACC)<code>DeltaAI</code> (NCSA)<code>Perlmutter</code> (NERSC)<code>Frontier</code> (ORNL)(IAS)<code>Aurora</code> (ANL)<code>LUMI</code> (CSC)<code>Trillium</code> (SciNet, Canada) <p><code>Stellar</code> cluster at Princeton University has 6 nodes with 2 NVIDIA A100 GPUs (Ampere 8.0 microarchitecture) each and 128-core AMD EPYC Rome CPUs (Zen2 microarchitecture).</p> <p>Installing the dependencies</p> <p>The most straightforward way to set things up on the <code>Stellar</code> cluster, is to use <code>spack</code> as described here. After downloading and initializing the shell-env, load the proper modules to-be-used during compilation: <pre><code>module load gcc-toolset/10\nmodule load cudatoolkit/12.5\nmodule load openmpi/gcc-toolset-10/4.1.0\n</code></pre></p> <p>Then manually add the following two entries to <code>~/.spack/packages.yaml</code>: <pre><code>packages:\n  cuda:\n    buildable: false\n    externals:\n    - spec: cuda@12.5\n      prefix: /usr/local/cuda-12.5\n  openmpi:\n    buildable: false\n    externals:\n    - spec: openmpi@4.1.0\n      prefix: /usr/local/openmpi/4.1.0/gcc-toolset-10\n</code></pre></p> <p>And run <code>spack compiler add</code> and <code>spack external find</code>. Since the login nodes (on which all the libraries will be compiled) are different from the compute nodes on <code>Stellar</code>, you will need to allow spack to compile for non-native CPU architectures by running: <pre><code>spack config add concretizer:targets:host_compatible:false\n</code></pre></p> <p>Now we can install 3 libraries we will need: <code>HDF5</code>, <code>ADIOS2</code> and <code>Kokkos</code>. First create and activate a new environment: <pre><code>spack env create entity-env\nspack env activate entity-env\n</code></pre></p> <p>To install the packages within the spack environment, run the following commands: <pre><code>spack add hdf5 +mpi +cxx target=zen2\nspack add adios2 +hdf5 +pic target=zen2\nspack add kokkos +cuda +wrapper cuda_arch=80 +pic +aggressive_vectorization target=zen2\n</code></pre></p> <p>You might want to first run these commands with <code>spec</code> instead of <code>add</code> to make sure spack recognizes the correct <code>cuda</code> &amp; <code>openmpi</code> (they should be marked as <code>[e]</code> and should point to a local directory specified above). After <code>add</code>-ing you can launch the installer via <code>spack install</code> and wait until all installations are done.</p> <p>Compiling &amp; running the code</p> <p>To compile the code, first activate the environment (if not already), then manually using both <code>modules</code> and <code>spack</code> load all the necessary libraries:  <pre><code>module load gcc-toolset/10\nmodule load cudatoolkit/12.5\nmodule load openmpi/gcc-toolset-10/4.1.0\nspack load gcc cuda openmpi kokkos adios2\n</code></pre></p> <p>During the compilation, passing any <code>-D Kokkos_***</code> or <code>-D ADIOS2_***</code> flags is not necessary, while <code>-D mpi=ON/OFF</code> is still needed, since in theory the code can also be compiled without MPI.</p> <p>To run the code, the submit script should look something like this: <pre><code>#!/bin/bash\n#SBATCH -n 4 (1)\n#SBATCH -t 00:30:00\n#SBATCH -J entity-run\n#SBATCH --gres=gpu:2 (2)\n#SBATCH --gpus=4 (3)\n# .. other sbatch directives\n\nmodule load gcc-toolset/10\nmodule load cudatoolkit/12.5\nmodule load openmpi/gcc-toolset-10/4.1.0\n. &lt;HOME&gt;/spack/share/spack/setup-env.sh\nspack env activate entity-env\nspack load gcc cuda openmpi kokkos adios2\n\nsrun entity.xc -input &lt;INPUT&gt;.toml\n</code></pre></p> <ol> <li>total number of tasks (GPUs)</li> <li>requesting nodes with 2 GPUs per node</li> <li>total number of GPUs</li> </ol> <p>Last updated: 4/28/2025</p> <p><code>Zaratan</code> cluster at the University of Maryland has 20 nodes with 4 NVIDIA A100 GPUs (Ampere 8.0) and a 128-core AMD EPYC (Zen2) CPUs each, as well as 8 nodes with NVIDIA H100 GPUs (Hopper 9.0) and Intel Xeon Platinum 8468 (Sapphire Rapids). Below, we describe how to run the code on the A100 nodes; for the H100 nodes the procedure is similar with the only exception being that different flags need to be specified when installing the <code>Kokkos</code> library (plus, you might need to manually specify <code>target=&lt;CPUARCH&gt;</code> as the login nodes have a different microarchitecture than the H100 compute nodes).</p> <p>Installing the dependencies</p> <p>We will rely on spack to compile on Zaratan. But first of all, the correct compiler should be loaded:</p> <p><pre><code>module load gcc/11.3.0\n</code></pre> After that, add the following to <code>~/.spack/packages.yaml</code>: <pre><code>packages:\n  cuda:\n    buildable: false\n    externals:\n    - prefix: /cvmfs/hpcsw.umd.edu/spack-software/2023.11.20/linux-rhel8-x86_64/gcc-11.3.0/cuda-12.3.0-fvfg7yyq63nunqvkn7a5fzh6e77quxty\n      spec: cuda@12.3\n    - modules:\n      - cuda/12.3\n      spec: cuda@12.3\n  cmake:\n     buildable: false\n     externals:\n     - prefix: /usr\n       spec: cmake@3.26.5\n</code></pre> Next, you should create a virtual environment and activate it <pre><code>spack env create entity-env\nspack env activate entity-env\n</code></pre> From within the environment, install the required packages within the environment running the following command:  <pre><code>spack add hdf5 +mpi +cxx\nspack add adios2 +hdf5 +pic\nspack add kokkos +cuda +wrapper cuda_arch=80 +pic +aggressive_vectorization\nspack add openmpi +cuda\n</code></pre> After that, install the packages with <code>spack install</code>. Now, to load the packages within the environment, do: <pre><code>spack load hdf5 adios2 kokkos openmpi\n</code></pre></p> <p>Compiling &amp; running the code</p> <p>Compilation of the code is performed as usual, and there is no need for any additional <code>-D Kokkos_***</code> or <code>-D ADIOS2_***</code> flags. The batch script for submitting the job should look like this: <pre><code>#!/bin/bash\n#SBATCH -p gpu\n#SBATCH -t 00:30:00\n#SBATCH -n 1\n#SBATCH -c 1\n#SBATCH --gpus=a100_1g.5gb:1\n#SBATCH --output=test.out\n#SBATCH --error=test.err\n\nmodule load gcc/11.3.0\n. &lt;HOME&gt;/spack/share/spack/setup-env.sh\nspack env activate entity-env\nspack load hdf5 kokkos adios2 cuda openmpi\n\nmpirun ./entity.xc -input &lt;INPUT&gt;.toml \n</code></pre></p> <p>Last updated: 4/29/2025</p> <p><code>Rusty</code> cluster at Flatiron Institute has 36 nodes with 4 NVIDIA A100-80GB each and 36 nodes with 4 NVIDIA A100-40GB GPUs each and 64-core Icelake CPUs. It also has 18 nodes with 8 NVIDIA H100-80GB GPUs each and same CPU architecture. We will use nodes with A100 GPUs for the example below.</p> <p>Installing the dependencies</p> <p>The most straightforward way to set things up on the <code>Rusty</code> cluster, is to use <code>spack</code> as described here. After downloading and initializing the <code>spack</code> shell-env, start an interactive session to make compilation faster: <pre><code>srun -C a100 -p gpu -N1 -n1 -c32 --gpus-per-task=1 --pty bash -i\n</code></pre></p> <p>Next, you can create a virtual environment and activate it: <pre><code>spack env create entity-env\nspack env activate entity-env\n</code></pre></p> <p>Then load the proper modules to-be-used during compilation, add compiler to spack, and find external libraries: <pre><code>module purge\nml modules/2.4-20250724 gcc/13.3.0 cuda/12.5.1 openmpi/cuda-4.1.8 hdf5/mpi-1.12.3\nspack compiler add\nspack external find\nspack external find cuda\nspack external find openmpi\nspack external find hdf5\n</code></pre> You can check that the corrrect external libraries were found by <code>spack spec [library]</code>.</p> <p>Now we can install 2 libraries we will need: <code>Kokkos</code> and <code>ADIOS2</code>. <pre><code>spack add kokkos +cuda +wrapper cuda_arch=80 +pic +aggressive_vectorization\nspack add adios2 +hdf5 +pic\nspack install\n</code></pre></p> <p>Note: if you wish to install this from the login nodes, you need to allow compilation on non-native architectures and specify target architecture (<code>linux-rocky8-icelake</code> for A100 GPUs nodes on <code>Rusty</code>): <pre><code>spack config add concretizer:targets:host_compatible:false\nspack add kokkos +cuda +wrapper cuda_arch=80 +pic +aggressive_vectorization target=linux-rocky8-icelake \nspack add adios2 +hdf5 +pic target=linux-rocky8-icelake \n</code></pre></p> <p>You might want to first run these commands with <code>spec</code> instead of <code>add</code> to make sure spack recognizes the correct <code>gcc</code>, <code>cuda</code>, <code>openmpi</code>, and <code>hdf5</code> (they should be marked as <code>[e]</code> and should point to a local directory specified above).</p> <p>Compiling &amp; running the code</p> <p>To compile the code, load the modules and activate the spack enviroment:  <pre><code>module purge\nml modules/2.4-20250724 gcc/13.3.0 cuda/12.5.1 openmpi/cuda-4.1.8 hdf5/mpi-1.12.3\nspack env activate entity-env\n</code></pre></p> <p>During the compilation, passing any <code>-D Kokkos_***</code> or <code>-D ADIOS2_***</code> flags is not necessary, while <code>-D mpi=ON/OFF</code> is still needed, since in theory the code can also be compiled without MPI.</p> <p>To run the code, the submit script should look something like this: <pre><code>#!/bin/bash\n#SBATCH -p gpu\n#SBATCH --gpus-per-task=1\n#SBATCH --cpus-per-task=16\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodes=1 (*)\n#SBATCH --gres=gpu:4\n#SBATCH --constraint=a100-80gb\n#SBATCH --time=00:30:00\n\n# .. other sbatch directives\n\nmodule purge\nml modules/2.4-20250724 gcc/13.3.0 cuda/12.5.1 openmpi/cuda-4.1.8 hdf5/mpi-1.12.3\n. &lt;HOME&gt;/spack/share/spack/setup-env.sh\nspack env activate entity-env\nexport LD_PRELOAD=/mnt/sw/fi/cephtweaks/lib/libcephtweaks.so\nexport CEPHTWEAKS_LAZYIO=1\n\nsrun entity.xc -input &lt;INPUT&gt;.toml\n</code></pre></p> <p>(*) total number of nodes</p> <p>Last updated: 10/21/2025</p> <p><code>Vista</code> cluster is a part of TACC research center. It consists of 600 Grace Hopper nodes, each hosting H100 GPU and 72 Grace CPUs. </p> <p>Installing the dependencies</p> <p><code>Vista</code> does not require any specific modules to be installed. Before compiling, the following modules should be loaded:</p> <pre><code>module load nvidia/24.7\nmodule load cuda/12.5\nmodule load kokkos/4.5.01-cuda\nmodule load openmpi/5.0.5\nmodule load adios2/2.10.2\nmodule load phdf5/1.14.4\nmodule load ucx/1.18.8\n</code></pre> <p>Compiling &amp; running the code</p> <p>The code can be then configured with the following command:</p> <p><pre><code>cmake -B build -D mpi=ON -D pgen=&lt;YOUR_PGEN&gt;  -D output=ON -D Kokkos_ENABLE_CUDA=ON -D Kokkos_ARCH90=ON -D ADIOS2_USE_CUDA=ON -D ADIOS2_USE_MPI=ON\n</code></pre> While the <code>hdf5</code> output format works on <code>Vista</code>, we advise to use <code>BPFile</code>, as currently <code>hdf5</code> write is extremely slow with MPI for 2- and 3-dimensional problems.  The sample submit script should look similar to this:</p> <pre><code>#!/bin/bash\n#SBATCH -A &lt;PROJECT NUMBER&gt;\n#SBATCH -p gh\n#SBATCH -t 16:00:00 #the code will run for 16 hours\n#SBATCH -N 64       # 64 nodes will be used\n#SBATCH -n 64       # 64 tasks in total will be launched\n#SBATCH -J your_job_name\n#SBATCH --output=test.out\n#SBATCH --error=test.err\nexport UCX_MEMTYPE_CACHE=n\nexport UCX_TLS=rc,cuda_copy\nexport UCX_IB_REG_METHODS=rcache,direct\nexport UCX_RNDV_MEMTYPE_CACHE=n\necho \"Launching application...\"\nibrun ./entity.xc -input &lt;INPUT&gt;.toml\n</code></pre> <p>Last updated: 6/19/2025</p> <p><code>DeltaAI</code> uses GH200 nodes. These are NVIDIA superchip nodes with 4x H100 GPUs and 4x ARM CPUs with 72 cores each.</p> <p>Installing the dependencies</p> <p>This makes the setup a bit more tedious, but luckily most dependencies are already installed.</p> <p>You can load the installed dependencies with</p> <pre><code>module restore\nmodule unload gcc-native\nmodule load gcc-native/12\nmodule load craype-accel-nvidia90\nmodule load cray-hdf5-parallel\n</code></pre> <p>We would recommend installing <code>Kokkos</code> and <code>ADIOS2</code> from source with the following settings:</p> <pre><code># Kokkos\ncmake -B build  \\\n    -D CMAKE_CXX_STANDARD=17 \\\n    -D CMAKE_CXX_EXTENSIONS=OFF \\\n    -D CMAKE_POSITION_INDEPENDENT_CODE=TRUE \\\n    -D CMAKE_C_COMPILER=cc \\\n    -D CMAKE_CXX_COMPILER=CC \\\n    -D Kokkos_ARCH_ARMV9_GRACE=ON \\\n    -D Kokkos_ARCH_HOPPER90=ON \\\n    -D Kokkos_ENABLE_CUDA=ON \\\n    -D Kokkos_ENABLE_DEBUG=ON \\\n    -D CMAKE_INSTALL_PREFIX=/path/to/install/location/for/kokkos &amp;&amp; \\\ncmake --build build -j &amp;&amp; \\\ncmake --install build \n\n# ADIOS2\ncmake -B build  \\\n    -D CMAKE_CXX_STANDARD=17 \\\n    -D CMAKE_CXX_EXTENSIONS=OFF \\\n    -D CMAKE_POSITION_INDEPENDENT_CODE=TRUE \\\n    -D BUILD_SHARED_LIBS=ON \\\n    -D ADIOS2_USE_HDF5=ON \\\n    -D ADIOS2_USE_Python=OFF \\\n    -D ADIOS2_USE_Fortran=OFF \\\n    -D ADIOS2_USE_ZeroMQ=OFF \\\n    -D BUILD_TESTING=ON \\\n    -D CMAKE_C_COMPILER=cc \\\n    -D CMAKE_CXX_COMPILER=CC \\\n    -D ADIOS2_BUILD_EXAMPLES=OFF \\\n    -D ADIOS2_USE_MPI=ON \\\n    -D ADIOS2_USE_BLOSC=ON \\\n    -D HDF5_ROOT=/opt/cray/pe/hdf5-parallel \\\n    -D CMAKE_INSTALL_PREFIX=/path/to/install/location/for/adios2 &amp;&amp; \\\ncmake --build build -j &amp;&amp; \\\ncmake --install build\n</code></pre> <p>You can then add module files for both libraries (as described here) or add them to your path directly. Just be sure to export the relevant <code>kokkos</code> settings. <pre><code># in the kokkos module file\nsetenv  Kokkos_ENABLE_CUDA              ON\nsetenv  Kokkos_ARCH_ARMV9_GRACE         ON\nsetenv  Kokkos_ARCH_HOPPER90            ON\n</code></pre></p> <p>Compiling &amp; running the code</p> <p><code>DeltaAI</code>'s <code>mpich</code> seems to not be <code>CUDA</code> aware (or it's bugged), so you will always need to add the flag <code>gpu_aware_mpi=OFF</code>.</p> <p>Your <code>cmake</code> setting should look something like this: <pre><code>cmake -B build -D pgen=&lt;PGEN&gt; -D mpi=ON -D CMAKE_CXX_COMPILER=CC -D CMAKE_C_COMPILER=cc -D gpu_aware_mpi=OFF\n</code></pre></p> <p>Finally an example <code>SLURM</code> script using the full node looks like this:</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=72\n#SBATCH --gpus-per-node=4\n#SBATCH --partition=ghx4\n#SBATCH --time=48:00:00\n#SBATCH --gpu-bind=verbose,closest\n#SBATCH --job-name=example\n#SBATCH -o ./log/%x.%j.out\n#SBATCH -e ./log/%x.%j.err\n#SBATCH --account=your-account\n\nmodule restore\nmodule unload gcc-native\nmodule load gcc-native/12\nmodule load craype-accel-nvidia90\nmodule use --append /path/to/your/.modfiles\nmodule load kokkos/4.6.00\nmodule load entity/cuda\nmodule load adios2/2.10.2\nmodule list\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport MPICH_OFI_VERBOSE=1\n\nsrun ./entity.xc -input &lt;INPUT&gt;.toml\n</code></pre> <p>Last updated: 4/28/2025</p> <p><code>Perlmutter</code> is a DoE cluster in LBNL with 4x NVIDIA A100 and a AMD EPYC 7763 CPU on each node. Note, that two different GPU configurations are available with 40 and 80 GB of VRAM respectively.</p> <p>Installing the dependencies</p> <p>The easiest way to use the code here is to compile and install your own modules manually. First, load the modules you will need for that:</p> <pre><code>module load PrgEnv-gnu cray-hdf5-parallel cmake/3.24.3\n</code></pre> <p>Download the <code>Kokkos</code> source code, configure/compile and install it (in this example, we install it in the <code>~/opt</code> directory.</p> <pre><code>wget https://github.com/kokkos/kokkos/releases/download/4.6.01/kokkos-4.6.01.tar.gz\ntar xvf kokkos-4.6.01.tar.gz\ncd kokkos-4.6.01\ncmake -B build -D CMAKE_CXX_STANDARD=17 \\\n    -D CMAKE_BUILD_TYPE=Release \\\n    -D CMAKE_CXX_EXTENSIONS=OFF \\\n    -D CMAKE_POSITION_INDEPENDENT_CODE=TRUE \\\n    -D CMAKE_CXX_COMPILER=CC \\\n    -D Kokkos_ENABLE_CUDA=ON \\\n    -D Kokkos_ENABLE_IMPL_CUDA_MALLOC_ASYNC=OFF \\\n    -D Kokkos_ARCH_ZEN3=ON \\\n    -D Kokkos_ARCH_AMPERE80=ON \\\n    -D CMAKE_INSTALL_PREFIX=$HOME/opt/kokkos/4.6.01/\ncmake --build build -j\ncmake --install build\n</code></pre> <p>Now the <code>ADIOS2</code>:</p> <pre><code>wget https://github.com/ornladios/ADIOS2/archive/refs/tags/v2.10.2.tar.gz\ntar xvf v2.10.2.tar.gz\ncd ADIOS2-2.10.2\ncmake -B build -D CMAKE_CXX_STANDARD=17 \\\n    -D CMAKE_CXX_EXTENSIONS=OFF \\\n    -D CMAKE_POSITION_INDEPENDENT_CODE=TRUE \\\n    -D BUILD_SHARED_LIBS=ON \\\n    -D ADIOS2_USE_HDF5=ON \\\n    -D ADIOS2_USE_Python=OFF \\\n    -D ADIOS2_USE_Fortran=OFF \\\n    -D ADIOS2_USE_ZeroMQ=OFF \\\n    -D BUILD_TESTING=OFF \\\n    -D ADIOS2_BUILD_EXAMPLES=OFF \\\n    -D ADIOS2_USE_MPI=ON \\\n    -D ADIOS2_USE_BLOSC=ON \\\n    -D LIBFABRIC_ROOT=/opt/cray/libfabric/1.15.2.0/ \\\n    -D CMAKE_INSTALL_PREFIX=$HOME/opt/adios2/v2.10.2 \\\n    -D MPI_ROOT=/opt/cray/pe/craype/2.7.30\ncmake --build build -j\ncmake --install build\n</code></pre> <p>For simplicity, it is recommended to also create the module files (e.g., in <code>~/modules</code> directory):</p> <p>For <code>kokkos</code>: <pre><code>#%Module1.0######################################################################\n##\n## Kokkos @ Zen3 @ Ampere80 modulefile\n##\n#################################################################################\nproc ModulesHelp { } {\n  puts stderr \"\\tKokkos\\n\"\n}\n\nmodule-whatis      \"Sets up Kokkos @ Zen3 @ Ampere80\"    \n\nconflict           kokkos\n\nset                basedir      /global/homes/h/&lt;USER&gt;/opt/kokkos/4.6.01\nprepend-path       PATH         $basedir/bin\nsetenv             Kokkos_DIR   $basedir\nsetenv             Kokkos_ARCH_ZEN3 ON\nsetenv             Kokkos_ARCH_AMPERE80 ON\nsetenv             Kokkos_ENABLE_CUDA ON\n</code></pre></p> <p>For <code>ADIOS2</code>: <pre><code>#%Module1.0######################################################################\n##\n## ADIOS2 modulefile\n##\n#################################################################################\nproc ModulesHelp { } {\n  puts stderr \"\\tADIOS2\\n\"\n}\n\nmodule-whatis      \"Sets up ADIOS2\"    \n\nconflict           adios2\n\nset                basedir      /global/homes/h/&lt;USER&gt;/opt/adios2/v2.10.2\nprepend-path       PATH         $basedir/bin\nsetenv             ADIOS2_DIR   $basedir\n\nsetenv ADIOS2_USE_HDF5      ON\nsetenv ADIOS2_USE_MPI       ON\nsetenv ADIOS2_HAVE_HDF5_VOL ON\nsetenv MPI_ROOT             /opt/cray/pe/mpich/8.1.30/ofi/gnu/12.3\nsetenv HDF5_ROOT            /opt/cray/pe/hdf5-parallel/1.14.3.1/gnu/12.3\n\nprereq cray-mpich/8.1.30 cray-hdf5-parallel/1.14.3.1\n</code></pre></p> <p>Make sure to explicitly set the paths, instead of using <code>~</code> or <code>$HOME</code>.</p> <p>Compiling &amp; running the code</p> <p>When compiling <code>entity</code> itself, explicitly pass the <code>cc</code> and <code>CC</code> as compilers, i.e.: <pre><code>cmake -B build ... -D CMAKE_C_COMPILER=cc -D CMAKE_CXX_COMPILER=CC\n</code></pre></p> <p>Use the following submit script for the slurm (example for 8 GPUs on 2 nodes; details on available resources): <pre><code>#!/bin/bash\n#SBATCH --account=&lt;ALLOCATION&gt;\n#SBATCH --constraint=gpu\n#SBATCH --qos=&lt;QUEUE&gt;\n#SBATCH -t &lt;TIME&gt;\n#SBATCH -N 2\n#SBATCH -c 1\n#SBATCH -n 8\n#SBATCH --gpus=8\n#SBATCH --gpus-per-task=1\n#SBATCH --gpu-bind=none\n\n# load all the modules here\n\nexport MPICH_NO_BUFFER_ALIAS_CHECK=1\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport MPICH_OFI_NIC_POLICY=GPU\nexport SLURM_CPU_BIND=\"cores\"\n\nsrun ./entity.xc -input cfg.toml &gt;report 2&gt;error\n</code></pre></p> <p>Last updated: 6/19/2025</p> <p>WIP</p> <p>WIP</p> <p><code>Aurora</code> uses Intel PVC nodes with 6 GPUs/node. Each PVC has 128GB of memory and is split into 2 tiles. It is recommended to use 1 MPI rank per tile, so 2 per GPU and 12 per node. Development of entity for <code>Aurora</code> is currently ongoing. Use the following docs with caution and check in with <code>@LudwigBoess</code> on potential changes.</p> <p>Modules to load</p> <p>You can load the installed dependencies with</p> <pre><code>module load adios2\nmodule load autoconf cmake\n</code></pre> <p>The <code>adios2</code> module automatically loads the related <code>kokkos</code> module. Please note that the <code>adios2</code> module provided by ALCF does not support HDF5.</p> <p>I would recommend saving the module configuration for easy loading within the PBS job: <pre><code>module save entity\n</code></pre></p> <p>You can compile <code>entity</code> with:</p> <pre><code>cmake -B build -D pgen=&lt;your_pgen&gt; -D precision=single -D mpi=ON -D output=ON -DCMAKE_C_COMPILER=mpicc -DCMAKE_CXX_COMPILER=mpicxx\n</code></pre> <p>Running entity</p> <p>Aurora uses PBS for workload management. The Intel PVC GPUs are split into two tiles each and it is recommended to launch one MPI rank per tile.</p> <pre><code>#!/bin/bash -l\n#PBS -A &lt;project_name&gt;\n#PBS -N &lt;job_name&gt;\n#PBS -l select=1                # number of nodes to use\n#PBS -l walltime=00:05:00\n#PBS -l filesystems=flare       # replace with the filesystem of your project\n#PBS -k doe\n#PBS -l place=scatter\n#PBS -q debug\n\nNTOTRANKS=12        # 2*6*N_nodes - updated with your requested number\nNRANKS_PER_NODE=12  # 2*6  - always the same\n\n# change to directory from which job was submitted\ncd $PBS_O_WORKDIR\n\n# load all modules defined above\nmodule restore entity\n\n# only relevant for CPU pinning and to avoid Kokkos complaints\nexport OMP_PROC_BIND=spread\n\nmpiexec --envall -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} ./gpu_tile_compact.sh ./entity.xc -input weibel.toml\n</code></pre> <p>To run it you need to define a script <code>gpu_tile_compact.sh</code> in the same folder as your executable. It should look like this:</p> <pre><code>#!/bin/bash -l\nnum_gpu=6\nnum_tile=2\ngpu_id=$(( (PALS_LOCAL_RANKID / num_tile ) % num_gpu ))\ntile_id=$((PALS_LOCAL_RANKID % num_tile))\nexport ZE_ENABLE_PCI_ID_DEVICE_ORDER=1\nexport ZE_AFFINITY_MASK=$gpu_id.$tile_id\n\n# reports the GPU tile pinning\necho \u201cRANK= $PALS_RANKID LOCAL_RANK= $PALS_LOCAL_RANKID gpu= $gpu_id.$tile_id\u201d\n# runs the actual job\nexec \"$@\"\n</code></pre> <p>Last updated: 8/11/2025</p> <p><code>LUMI</code> cluster is located in Finland. It is equipped with 2978 nodes with 4 AMD MI250x GPUs and a single 64 cores AMD EPYC \"Trento\" CPU. The required modules to be loaded are:</p> <p>Compiling &amp; running the code</p> <pre><code>module load PrgEnv-cray\nmodule load cray-mpich\nmodule load craype-accel-amd-gfx90a\nmodule load rocm\nmodule load cray-hdf5-parallel/1.12.2.11\n</code></pre> <p>The configuration command is standard. The <code>Kokkos</code> library, along with <code>adios2</code>, will be installed from code dependencies directly at compilation. It is also important to provide the <code>c++</code> and <code>c</code> compilers manually with environemntal variables <code>CC</code> and <code>cc</code> (they are already predefined given that all the modules mentioned above were loaded). So far, gpu-aware mpi is not supported on <code>LUMI</code>. The configuration command is the following: <pre><code>cmake -B build -D pgen=turbulence -D mpi=ON -D Kokkos_ENABLE_HIP=ON -D Kokkos_ARCH_AMD_GFX90A=ON -D CMAKE_CXX_COMPILER=CC -D CMAKE_C_COMPILER=cc -D gpu_aware_mpi=OFF\n</code></pre></p> <p>The example submit script:</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=examplejob   # Job name\n#SBATCH --output=test.out # Name of stdout output file\n#SBATCH --error=test.err  # Name of stderr error file\n#SBATCH --partition=standard-g  # partition name\n#SBATCH --nodes=8               # Total number of nodes \n#SBATCH --ntasks-per-node=8     # 8 MPI ranks per\n#SBATCH --gpus-per-node=8\n##SBATCH --mem=0\n#SBATCH --time=48:00:00       # Run time (d-hh:mm:ss)\n#SBATCH --account=project_&lt;NUMBER&gt;  # Project for billing\nexport MPICH_GPU_SUPPORT_ENABLED=1\nsrun ./entity.xc -input &lt;INPUT&gt;.toml\n</code></pre> <p>Last updated: 6/19/2025</p> <p>Trillium is a large parallel cluster built by Lenovo Canada and hosted by SciNet at the University of Toronto, the GPU subcluster has 61 nodes each with 4 x Nvidia H100 SXM (80 GB memory) (HOPPER90 architecture) and 1 x AMD EPYC 9654 (Zen 4) @ 2.4 GHz, 384MB cache L3 (96 cores). <code>Entity</code> works largely out of the box on trillium with the exception of the HDF5 format and requiring GPU aware MPI to disabled. </p> <p>Compiling &amp; running the code The following modules are confirmed to have worked for building, compilation, running and restarting</p> <pre><code>module load gcc/12.3 cmake/3.31.0 cuda/12.6 openmpi/4.1.5\n</code></pre> <p>To disable hdf5, modify the following file in the entity source directory</p> <pre><code>/path_to_src/entity/cmake/adios2Config.cmake\n</code></pre> <p>changing <pre><code># Format/compression support\nset(ADIOS2_USE_HDF5\n  OFF # &lt;-- set this to OFF\n  CACHE BOOL \"Use HDF5 for ADIOS2\")\n</code></pre></p> <p>When configuring ensure to set the flag</p> <pre><code>-D gpu_aware_mpi=OFF    \n</code></pre> <p>as the nodes are not properly configured to perform gpu to gpu direct communication (the code will still run, but errors will arise at mesh block boundaries, and the code itself will run much slower).</p> <p>A typical pbs script for running entity on the gpu subcluster is</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --gpus-per-node=4\n#SBATCH --ntasks-per-node=4  # Keep all GPUs active\n#SBATCH --time=23:59:59\n#SBATCH --partition=compute_full_node\n#SBATCH -o outjob_test.o%j\n#SBATCH -e outjob_test.e%j\n#SBATCH -J test\n\nmodule load gcc/12.3 cmake/3.31.0 cuda/12.6 openmpi/4.1.5\n\nmpirun --map-by ppr:4:node --bind-to core ./entity.xc -input fluxtube.toml\n</code></pre> <p>where here we have requested 2x4 gpus for the full 24 hour wall time. Note one can request 1, 4, and 8 gpus for brief interactive debug jobs with</p> <pre><code>$debugjob\n$debugjob 1\n$debugjob 2\n</code></pre> <p>To pip install the version of nt2py which works with the adios2 output format you will need to load the following modules</p> <pre><code>module load python texlive gcc arrow/21.0.0  \n</code></pre> <p>Last updated: 9/12/2025</p> <p>Mind the dates</p> <p>At the bottom of each section, there are tags indicating when was the last date this instruction was updated. Some of them may be outdated due to clusters being constantly updated and changed. If so, please feel free to reach out with questions or contribute updated instructions.</p>"},{"location":"content/useful/costtool/","title":"Simulation cost estimator","text":"<p>This interactive tool allows to quickly estimate the cost of a simulation (both in terms of memory usage and simulation time) to help optimize your setup for specific scales and architectures.</p> Basic parameters Mode: Dimension: Coordinates: <p>$r_0$:\u00a0</p> <p>$h$:\u00a0</p> Resolution: \u00d7 \u00d7 CFL: Extent in $x^1$: Total runtime: # of particles per cell: # of payloads: Precision: single <code>real_t</code> single <code>prtldx_t</code> GPU arch: V100 [32GB] A100 [40GB] A100 [80GB] H100 [80GB] H100 [96GB] H100 [160GB] MI250X [128GB] Ponte-Vechio [128GB] custom GPUs per node: Number of GPUs: Custom parameters: per GPU memory:            GB          timestep per particle:            ns          memory overhead:            %          performance overhead:            $\\times$          <p>Memory alignment</p> <p>Because of architecture-dependent memory alignment, the numbers cited here are only approximations. The actual memory used by the run may vary, so please make sure to leave at least 20%...30% of headroom for safety.</p> <p>Accuracy</p> <p>Current performance estimates for specific architectures are very crude, and we will most likely refine them in the future. Additionaly, keep in mind that the efficiency is proportional to the number of particles you can fit on a given GPU, so make sure to maximize the usage as much as possible for the best performance.</p>"},{"location":"content/useful/costtool/#group-two","title":"Grid &amp; particles","text":""},{"location":"content/useful/costtool/#group-three","title":"Architecture","text":""},{"location":"content/useful/costtool/#group-output-memory","title":"Memory usage","text":"Per cell/particle <p>Fields: ...</p> <p>Particles: ...</p> Total <p>         Fields: ... ... </p> <p>         Particles: ... ... </p> <p>Total: ...</p> <p>         Per GPU: ... ... </p>"},{"location":"content/useful/costtool/#group-output-runtime","title":"Runtime","text":"Timestep duration <p>$\\Delta t$: ...</p> <p>Duration: ...</p> Total <p>         Recommended number of GPUs:         ... ... </p> <p>         Number of timesteps:         ... </p> <p>Runtime: ...</p> <p>Cost: ...</p>"},{"location":"content/useful/float-comparison/","title":"Floating point comparison","text":"<p>tl;dr</p> <p>We use simplified floating-point (both single and double precision) comparison method in the code, which does not guarantee equality up to a single bit, but is good-enough for most practical purposes.</p>"},{"location":"content/useful/float-comparison/#truncation-error","title":"Truncation error","text":"<p>Comparing floating-point numbers is tricky. In IEEE-754-compatible C++ compilers, floating point numbers are represented with 32 (<code>float</code>) or 64 (<code>double</code>) bits. In this representation, the first bit (or the last, depending on who you ask) is reserved for the sign, 8 (11) bits are reserved for the exponent in single (double) precision, and the remaining 23 (52) bits are reserved for the mantissa. For instance, below is the representation of \\(-2^{-5}\\) in single and double precision (notice, the mantissa digits are exactly zero, as we are consider an exact power of \\(2\\)):</p> \\[ -2^{-5}\\equiv -0.03125 = \\overbrace{\\underbrace{1}_{\\text{sign}}~\\underbrace{01111010}_{\\text{exponent}}~\\underbrace{00000000000000000000000}_{\\text{mantissa}}}^{\\text{single precision representation}} \\] \\[ -2^{-5}\\equiv -0.03125 = \\overbrace{\\underbrace{1}_{\\text{sign}}~\\underbrace{01111111010}_{\\text{exponent}}~\\underbrace{0000000000000000000000000000000000000000000000000000}_{\\text{mantissa}}}^{\\text{double precision representation}} \\] <p>Of course, for an arbitrary decimal number, because we only have so many bits to represent it, the representation will be truncated. For instance, in single precision \\(0.2 = 0~01111100~10011001100110011001101\\) (in theory, the pattern of \\(0011\\) is repeating to infinity, just like \\(1/33 = 0.03030303...\\) in decimal representation).</p> <p>Because of this truncation, two different real numbers can have identical bit representations. At the same time, the same real number arithmetically computed in two different ways can have slightly different bit representations. So practically, any comparison of floating-point numbers is not mathematically exact. In other words, for any two real numbers, one cannot write a function \\(f\\) with a finite precision to guarantee that \\(f(a,~b) = \\textrm{false}\\) if and only if \\(a\\ne b\\); there will always be false-positives, \\(f(1,~1+10^{-8})=\\textrm{true}\\), and false-negatives, \\(f(0.3,0.1+0.2) =\\textrm{false}\\).</p> <p>Comparing with the equality sign <code>a == b</code></p> <p>Default equality operator in C++, <code>==</code>, compares the values bit-by-bit, so if any of the bits differs (even though the numbers might be very close arithmetically), it will return a <code>false</code>. For instance, this expression, <code>0.1f + 0.1f + 0.1f + 0.1f + 0.1f + 0.1f + 0.1f == 0.7f</code>, evaluates to <code>false</code>. Looking at their bitwise representation: <pre><code>0 01111110 01100110011001100110100 &lt;- 0.1f + 0.1f + 0.1f + 0.1f + 0.1f + 0.1f + 0.1f\n0 01111110 01100110011001100110011 &lt;- 0.7f\n</code></pre> we see that they are indeed different, albeit very close.</p>"},{"location":"content/useful/float-comparison/#unit-in-the-last-place-ulp","title":"Unit in the last place (ULP)","text":"<p>The most rigorous way to compare floating-point numbers in this context is to estimate their so-called ULP-distance (unit in the last place). If we picture the real number line as a continuum, with the truncated floating-point representation we can only express a finite set of the real numbers; we call these numbers float-representable. Let's consider an example. Below is the real number line, where we tag only the single precision floating-representable numbers between <code>1.0f</code> and <code>1.0f + 3e-7f</code>:</p> <p>We see that between the numbers <code>1.0f</code> and <code>1.0f + 3e-7f</code> there are only \\(2\\) distinct float-representable numbers. The ULP distance between these two numbers is, thus, \\(3\\), or \\(\\textrm{ULPd}_{32}(1, 1.0000003) = 3\\) (the subscript \\(32\\) denotes the single precision). So given this, technically, two real numbers, \\(a\\) and \\(b\\), are \"equal\" from the 32-bit floating-representation point of view, only if \\(\\textrm{ULPd}_{32}(a, b) = 0\\).</p> <p>ULP of a real number</p> <p>Notice, that we talked about the distance between two numbers. But a more common definition invokes a ULP of a single real number. For example, consider \\(\\pi\\). Let us choose two exactly float-representable real values, \\(\\pi_1\\) and \\(\\pi_2\\), such that \\(\\pi_1 &lt; \\pi &lt; \\pi_2\\), and \\(\\textrm{ULPd}_{32}(\\pi_1,\\pi_2)=1\\). This means that the ULP value of \\(\\pi\\) is \\(\\textrm{ULP}_{32}(\\pi) = \\pi_2 - \\pi_1 = 2^{-22}\\approx 2.4\\cdot 10^{-7}\\).</p> <p>In practice, of course, especially when dealing transcendental functions and numbers, it is theoretically impossible to guarantee that the result of a computation is within a certain ULP-distance from the true value. The best-practice is to allow on average between \\(0.5\\) and \\(1\\) ULP error margin. </p>"},{"location":"content/useful/float-comparison/#floating-point-comparison_1","title":"Floating-point comparison","text":""},{"location":"content/useful/float-comparison/#the-correct-way","title":"The correct way","text":"<p>More often, instead of comparing the numerical values with the \"real\" ones, we need to compare them with other numerical values: e.g., compare the energy of the system at the beginning of the simulation with its energy at the end. Since C++11, the compilers provide a function <code>std::numeric_limits&lt;T&gt;::epsilon()</code> that returns the ULP distance between \\(a\\) and the next float-representable number, where \\(a\\in [1;2)\\). For instance, <code>std::numeric_limits&lt;float&gt;::epsilon()</code> returns \\(\\varepsilon_{32}\\equiv 2^{-23}\\), which is the ULP distance between \\(1\\) and \\(1 + \\varepsilon_{32}\\approx 1 + 1.2\\cdot 10^{-7}\\). So for any given pair of numbers \\(a\\) and \\(b\\) in the \\([1;2)\\) interval, these two numbers are single-precision-float-equal within \\(1\\) ULP if \\(|a-b|&lt;=\\varepsilon_{32}\\). We can also generalize this to the numbers in an arbitrary interval by bringing them to the same exponent, to get the following equality condition:</p> <pre><code>#include &lt;cmath&gt;\n#include &lt;limits&gt;\ntemplate &lt;class T&gt; auto equal_within_1ulp(T a, T b) -&gt; bool {\n  const T m = std::min(std::fabs(a), std::fabs(b));\n  const int exp = m &lt; std::numeric_limits&lt;T&gt;::min()\n                      ? std::numeric_limits&lt;T&gt;::min_exponent - 1\n                      : std::ilogb(m);\n  return std::fabs(b - a) &lt;= std::ldexp(std::numeric_limits&lt;T&gt;::epsilon(), exp);\n}\n</code></pre> <p>Then <code>equal_within_1ulp(1.0f, 1.0f + 1e-7f)</code> evaluates to <code>false</code>, while <code>equal_within_1ulp(1.0f, 1.0f + 1e-8f)</code> evaluates to <code>true</code>. Evaluating this function can become computationally expensive, and besides that, it relies on the <code>std::</code> library functions, some of which are not portable on GPUs. So, in practice, we often use a simpler comparison method.</p>"},{"location":"content/useful/float-comparison/#the-simpler-way","title":"The simpler way","text":"<p>In the code, we use a simpler comparison method, which does not guarantee equality up to \\(1\\) ULP, but is nonetheless accurate enough for most practical purposes. Instead of directly renormalizing the exponents of each of the numbers, we simply evaluate the relative difference of the two numbers and compare it against the epsilon for the given type:</p> <pre><code>#include &lt;cmath&gt;\n#include &lt;limits&gt;\n#include &lt;type_traits&gt;\n\ntemplate &lt;typename T&gt;\ninline constexpr auto epsilon = std::numeric_limits&lt;T&gt;::epsilon();\n\ntemplate &lt;class T&gt; \nauto AlmostEqual(T a, T b, T eps = epsilon&lt;T&gt;) -&gt; bool {\n  static_assert(std::is_floating_point_v&lt;T&gt;, \"T must be a floating point type\");\n  return (a == b) ||\n         (std::fabs(a - b) &lt;= std::min(std::fabs(a), std::fabs(b)) * eps);\n}\n</code></pre> <p>For instance, one false-positive this method yields is <code>AlmostEqual(1e7f + 1.0f, 9999999.0f + 1.0f)</code>, which evaluates to <code>true</code>, whereas <code>equal_within_1ulp(1e7f + 1.0f, 9999999.0f + 1.0f)</code> evaluates to <code>false</code>. Despite that, in most of the scenarios, even this might be an overkill, so we even provide an optional <code>eps</code> parameter to control the base-10 precision of the comparison. Additionally, to compare numbers with zero, we provide a separate function:</p> <pre><code>template &lt;class T&gt;\nauto AlmostZero(T a, T eps = epsilon&lt;T&gt;) -&gt; bool {\n  static_assert(std::is_floating_point_v&lt;T&gt;, \"T must be a floating point type\");\n  return std::fabs(a) &lt;= eps;\n}\n</code></pre> <p>Comparison of the difference with zero</p> <p>Never compare the difference of two floating-point numbers with zero, e.g., <code>AlmostZero(a - b)</code>, as this may yield incorrect results. Instead, use the <code>AlmostEqual(a, b)</code> function.</p>"},{"location":"content/useful/ks/","title":"Kerr metric","text":"<p>Note</p> <p>To keep things clean we will assume \\(r_g \\equiv GM_\\bullet/c^2 = 1\\) and \\(a\\equiv J_\\bullet c/(GM_\\bullet^2)\\). Dots \"\\(\\cdot\\)\" will be used instead of zeros \"0\" for visual clarity.</p>"},{"location":"content/useful/ks/#kerr-schild-coordinates","title":"Kerr-Schild coordinates","text":"<p>Kerr metric in Kerr-Schild coordinates is given by the following covariant tensor (and its inverse contravariant counterpart):</p> \\[ g_{\\mu \\nu}=\\begin{bmatrix}-(1-z) &amp; z &amp; \\cdot &amp; -az\\sin^2{\\theta}\\\\z &amp; 1+z &amp; \\cdot &amp; -a(1+z)\\sin^2{\\theta} \\\\\\cdot &amp; \\cdot &amp; \\Sigma &amp; \\cdot \\\\-az\\sin^2{\\theta} &amp; -a(1+z)\\sin^2{\\theta} &amp; \\cdot &amp; \\frac{A\\sin^2{\\theta}}{\\Sigma}\\end{bmatrix}~~~ g^{\\mu\\nu}=\\begin{bmatrix}-(1+z) &amp; z &amp; \\cdot &amp; \\cdot\\\\z &amp; \\frac{\\Delta}{\\Sigma} &amp; \\cdot &amp; \\frac{a}{\\Sigma}\\\\\\cdot &amp; \\cdot &amp; \\frac{1}{\\Sigma} &amp; \\cdot\\\\\\cdot &amp; \\frac{a}{\\Sigma} &amp; \\cdot &amp; \\frac{1}{\\Sigma\\sin^2{\\theta}}\\end{bmatrix} \\] \\[\\begin{align*} &amp;\\Sigma=r^2+a^2\\cos^2{\\theta} &amp; &amp;\\Delta = r^2-2r+a^2\\\\ &amp;A=(r^2+a^2)^2-a^2\\Delta \\sin^2{\\theta} &amp; &amp;z=\\frac{2r}{\\Sigma}\\\\ &amp;\\sqrt{-g}=\\Sigma\\sin{\\theta} \\end{align*}\\]"},{"location":"content/useful/ks/#31-formulation","title":"3+1 formulation","text":"<p>To rewrite the metric in 3+1 formulation we find the lag function and the shift vector to be: </p> \\[\\begin{align*}    &amp; \\beta^i=\\begin{bmatrix}\\frac{z}{1+z} \\\\ \\cdot \\\\ \\cdot\\end{bmatrix}    &amp; \\alpha^2=\\frac{1}{1+z}                                              \\\\    &amp; \\beta_i=\\left[z~~ \\cdot~~ -az\\sin^2{\\theta}\\right]    &amp; \\beta_i\\beta^i=\\frac{z^2}{1+z} \\end{align*}\\] <p>Then we find</p> \\[   h_{ij}=\\begin{bmatrix}1+z &amp; \\cdot &amp; -a(1+z)\\sin^2{\\theta} \\\\ \\cdot &amp; \\Sigma &amp; \\cdot \\\\ -a(1+z)\\sin^2{\\theta} &amp; \\cdot &amp; \\frac{A\\sin^2{\\theta}}{\\Sigma}\\end{bmatrix}~~~   h^{ij}=\\begin{bmatrix}\\frac{A}{\\Sigma(\\Sigma + 2r)} &amp; \\cdot &amp; \\frac{a}{\\Sigma} \\\\ \\cdot &amp; \\frac{1}{\\Sigma} &amp; \\cdot \\\\ \\frac{a}{\\Sigma} &amp; \\cdot &amp; \\frac{1}{\\Sigma\\sin^2{\\theta}}\\end{bmatrix} \\] <p>and \\(\\sqrt{h}=\\Sigma\\sin{\\theta}/\\alpha\\).</p> <p>It is also useful to define a locally orthonormal (tetrad) basis. This special frame is defined with the following vectors:</p> \\[   e^i_{\\hat{i}}=\\begin{bmatrix}\\sqrt{h^{rr}}                                 &amp; \\cdot                     &amp; \\cdot                       \\\\                \\cdot                                         &amp; 1/\\sqrt{h_{\\theta\\theta}} &amp; \\cdot                       \\\\                -\\sqrt{h^{rr}}h_{r\\varphi}/h_{\\varphi\\varphi} &amp; \\cdot                     &amp; 1/\\sqrt{h_{\\varphi\\varphi}}\\end{bmatrix}~~~   e_i^{\\hat{i}}=\\begin{bmatrix} 1/\\sqrt{h^{rr}} &amp; \\cdot &amp; h_{r\\varphi}/\\sqrt{h_{\\varphi\\varphi}}\\\\ \\cdot &amp; \\sqrt{h_{\\theta\\theta}} &amp;  \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\sqrt{h_{\\varphi\\varphi}}\\end{bmatrix} \\] <p>Transformations to and from the tetrad basis can be performed via \\(A^{\\hat{i}}=e^{\\hat{i}}_j A^j\\) \\(A^{i}=e_{\\hat{j}}^i A^{\\hat{j}}\\) \\(a_{\\hat{i}}=e_{\\hat{i}}^j a_{j}\\) and \\(a_{i}=e_{i}^{\\hat{j}} a_{\\hat{j}}\\). In the more explicit simplified form this reads:</p> \\[\\begin{aligned}    &amp; A^{\\hat{r}}=\\frac{1}{\\sqrt{h^{rr}}}A^r                                                                 &amp;    &amp; a_{\\hat{r}}=\\sqrt{h^{rr}}a_r-\\sqrt{h^{rr}}\\frac{h_{r\\varphi}}{h_{\\varphi\\varphi}}a_\\varphi               \\\\    &amp; A^{\\hat{\\theta}}=\\sqrt{h_{\\theta\\theta}}A^\\theta                                                       &amp;    &amp; a_{\\hat{\\theta}}=\\frac{1}{\\sqrt{h_{\\theta\\theta}}}a_\\theta                                               \\\\    &amp; A^{\\hat{\\varphi}}=\\frac{h_{r\\varphi}}{\\sqrt{h_{\\varphi\\varphi}}}A^r+\\sqrt{h_{\\varphi\\varphi}}A^\\varphi &amp;    &amp; a_{\\hat{\\varphi}}=\\frac{1}{\\sqrt{h_{\\varphi\\varphi}}}a_\\varphi \\end{aligned}\\]"},{"location":"content/useful/ks/#a0-schwarzschild-solution","title":"\\(a=0\\) (Schwarzschild solution)","text":"\\[   g_{\\mu \\nu}=\\begin{bmatrix}-(1-z) &amp; z &amp; \\cdot &amp; \\cdot\\\\z &amp; 1+z &amp; \\cdot &amp; \\cdot \\\\\\cdot &amp; \\cdot &amp; \\Sigma &amp; \\cdot \\\\\\cdot &amp; \\cdot &amp; \\cdot &amp; \\frac{A\\sin^2{\\theta}}{\\Sigma}\\end{bmatrix}=\\begin{bmatrix}-\\left(1-\\frac{2}{r}\\right) &amp; \\frac{2}{r} &amp; \\cdot &amp; \\cdot\\\\\\frac{2}{r} &amp; 1 + \\frac{2}{r} &amp; \\cdot &amp; \\cdot \\\\\\cdot &amp; \\cdot &amp; r^2 &amp; \\cdot \\\\\\cdot &amp; \\cdot &amp; \\cdot &amp; r^2\\sin^2{\\theta}\\end{bmatrix} \\] \\[   g^{\\mu\\nu}=\\begin{bmatrix}-(1+z) &amp; z &amp; \\cdot &amp; \\cdot\\\\z &amp; \\frac{\\Delta}{\\Sigma} &amp; \\cdot &amp; \\cdot\\\\\\cdot &amp; \\cdot &amp; \\frac{1}{\\Sigma} &amp; \\cdot\\\\\\cdot &amp; \\cdot &amp; \\cdot &amp; \\frac{1}{\\Sigma\\sin^2{\\theta}}\\end{bmatrix}=\\begin{bmatrix}-\\left(1+\\frac{2}{r}\\right) &amp; \\frac{2}{r} &amp; \\cdot &amp; \\cdot\\\\\\frac{2}{r} &amp; 1-\\frac{2}{r} &amp; \\cdot &amp; \\cdot \\\\\\cdot &amp; \\cdot &amp; \\frac{1}{r^2} &amp; \\cdot \\\\\\cdot &amp; \\cdot &amp; \\cdot &amp; \\frac{1}{r^2\\sin^2{\\theta}}\\end{bmatrix} \\] \\[\\begin{align*}    &amp; \\Sigma=r^2      &amp;    &amp; \\Delta = r^2-2r   \\\\    &amp; A=r^4           &amp;    &amp; z=\\frac{2}{r} \\end{align*}\\] <p>And the 3+1 components reduce to the following:</p> \\[\\begin{align*}    &amp; \\beta^i=\\begin{bmatrix}\\frac{2}{r+2} \\\\ \\cdot \\\\ \\cdot\\end{bmatrix} &amp;    &amp; \\alpha^2=\\frac{r}{r+2}                                                \\\\    &amp; \\beta_i=\\left[2/r ~~ \\cdot ~~ -2(a/r)\\sin^2{\\theta}\\right]          &amp;    &amp; \\beta_i\\beta^i=\\frac{4}{r^2+2r} \\end{align*}\\] \\[   h_{ij}=\\begin{bmatrix}1+\\frac{2}{r} &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; r^2 &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; r^2\\sin^2{\\theta}\\end{bmatrix}~~~   h^{ij}=\\begin{bmatrix}\\frac{r}{r+2} &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\frac{1}{r^2} &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\frac{1}{r^2\\sin^2{\\theta}}\\end{bmatrix} \\]"},{"location":"content/useful/ks/#boyer-lindquist-coordinates","title":"Boyer-Lindquist coordinates","text":"\\[   g_{\\mu \\nu}=\\begin{bmatrix}-\\left(1-\\frac{2r}{\\Sigma}\\right) &amp; \\cdot &amp; \\cdot &amp; -\\frac{2ar\\sin^2{\\theta}}{\\Sigma}\\\\ \\cdot  &amp; \\frac{\\Sigma}{\\Delta} &amp; \\cdot &amp; \\cdot \\\\ \\cdot  &amp; \\cdot &amp; \\Sigma &amp; \\cdot \\\\-\\frac{2ar\\sin^2{\\theta}}{\\Sigma} &amp; \\cdot &amp; \\cdot &amp; \\frac{A\\sin^2{\\theta}}{\\Sigma}\\end{bmatrix}~~~   g^{\\mu\\nu}=\\begin{bmatrix}-\\frac{A}{\\Delta\\Sigma} &amp; \\cdot &amp; \\cdot &amp; -\\frac{2ar}{\\Delta\\Sigma}\\\\ \\cdot  &amp; \\frac{\\Delta}{\\Sigma} &amp; \\cdot &amp;  \\cdot \\\\ \\cdot  &amp; \\cdot &amp; \\frac{1}{\\Sigma} &amp;  \\cdot \\\\-\\frac{2ar}{\\Delta\\Sigma} &amp; \\cdot &amp; \\cdot &amp; \\frac{\\Delta -a^2\\sin^2{\\theta}}{\\Delta\\Sigma\\sin^2{\\theta}}\\end{bmatrix} \\] \\[\\begin{align*}    &amp; \\Sigma=r^2+a^2\\cos^2{\\theta}           &amp;    &amp; \\Delta = r^2-2r+a^2                      \\\\    &amp; A=(r^2+a^2)^2-a^2\\Delta \\sin^2{\\theta}   \\\\    &amp; \\sqrt{-g}=\\Sigma\\sin{\\theta} \\end{align*}\\]"},{"location":"content/useful/ks/#31-formulation_1","title":"3+1 formulation","text":"\\[\\begin{align*}    &amp; \\beta^i=\\begin{bmatrix} \\cdot  \\\\ \\cdot \\\\ -2ar/A\\end{bmatrix}       &amp;    &amp; \\alpha^2=\\frac{\\Delta \\Sigma}{A}                                       \\\\    &amp; \\beta_i=\\left[~~ \\cdot  ~~ \\cdot ~~ -2ar\\sin^2{\\theta}/\\Sigma\\right] &amp;    &amp; \\beta_i\\beta^i=\\frac{4a^2r^2\\sin^2{\\theta}}{A} \\end{align*}\\] \\[   h_{ij}=\\begin{bmatrix}\\frac{\\Sigma}{\\Delta} &amp; \\cdot &amp; \\cdot \\\\ \\cdot  &amp; \\Sigma &amp; \\cdot \\\\ \\cdot  &amp; \\cdot &amp; \\frac{A\\sin^2{\\theta}}{\\Sigma}\\end{bmatrix}~~~   h^{ij}=\\begin{bmatrix}     \\frac{A}{\\Delta\\Sigma} &amp; \\cdot &amp; \\cdot \\\\ \\cdot &amp; \\frac{1}{\\Sigma} &amp; \\cdot \\\\ \\cdot &amp; \\cdot &amp; \\frac{1}{\\Sigma\\sin^2{\\theta}} + \\frac{(4r^2-1)a^2}{\\Delta\\Sigma}   \\end{bmatrix} \\]"},{"location":"content/useful/ks/#a0-schwarzschild-solution_1","title":"\\(a=0\\) (Schwarzschild solution)","text":"\\[   g_{\\mu \\nu}=\\begin{bmatrix}-\\left(1-\\frac{2}{r}\\right) &amp; \\cdot                           &amp; \\cdot &amp; \\cdot              \\\\                \\cdot                       &amp; \\left(1-\\frac{2}{r}\\right)^{-1} &amp; \\cdot &amp; \\cdot              \\\\                \\cdot                       &amp; \\cdot                           &amp; r^2   &amp; \\cdot              \\\\                \\cdot                       &amp; \\cdot                           &amp; \\cdot &amp; r^2 \\sin^2{\\theta}\\end{bmatrix}~~~   g^{\\mu\\nu}=\\begin{bmatrix}-\\left(1-\\frac{2}{r}\\right)^{-1} &amp; \\cdot &amp; \\cdot &amp;  \\cdot \\\\ \\cdot  &amp; 1-\\frac{2}{r} &amp; \\cdot &amp;  \\cdot \\\\ \\cdot  &amp; \\cdot &amp; \\frac{1}{r^2} &amp;  \\cdot \\\\ \\cdot  &amp; \\cdot &amp; \\cdot &amp; \\frac{1}{r^2\\sin^2{\\theta}}\\end{bmatrix} \\] \\[\\begin{align*}    &amp; \\beta^i=\\begin{bmatrix} \\cdot  \\\\ \\cdot \\\\ \\cdot \\end{bmatrix} &amp;    &amp; \\alpha^2=1-\\frac{2}{r}                                           \\\\    &amp; \\beta_i=\\left[~~\\cdot ~~ \\cdot ~~ \\cdot ~~\\right]              &amp;    &amp; \\beta_i\\beta^i=0 \\end{align*}\\] \\[   h_{ij}=\\begin{bmatrix}     \\left(1-\\frac{2}{r}\\right)^{-1} &amp; \\cdot &amp; \\cdot              \\\\     \\cdot                           &amp; r^2   &amp; \\cdot              \\\\     \\cdot                           &amp; \\cdot &amp; r^2 \\sin^2{\\theta}\\end{bmatrix}~~~   h^{ij}=\\begin{bmatrix}1-\\frac{2}{r} &amp; \\cdot &amp;  \\cdot \\\\ \\cdot  &amp; \\frac{1}{r^2} &amp;  \\cdot \\\\ \\cdot  &amp; \\cdot &amp; \\frac{1}{r^2\\sin^2{\\theta}}\\end{bmatrix} \\]"},{"location":"content/useful/ks/#conversion-from-ks-to-bl","title":"Conversion from KS to BL","text":"<p>Assuming \\(g_{\\mu\\nu}\\) is the metric in KS coordinates, and \\(g_{\\tilde{\\mu}\\tilde{\\nu}}\\) -- in BL.</p> \\[ J_{\\tilde{\\nu}}^{\\mu}=\\begin{bmatrix} 1 &amp; 2r/\\Delta &amp; \\cdot&amp; .\\\\  \\cdot&amp; 1 &amp; \\cdot&amp; .\\\\  \\cdot&amp; \\cdot&amp; 1 &amp; .\\\\  \\cdot&amp; a/\\Delta &amp; \\cdot&amp; 1 \\end{bmatrix}~~~ J_{\\nu}^{\\tilde{\\mu}}=\\begin{bmatrix} 1 &amp; -2r/\\Delta &amp; \\cdot&amp; .\\\\  \\cdot&amp; 1 &amp; \\cdot&amp; .\\\\  \\cdot&amp; \\cdot&amp; 1 &amp; .\\\\  \\cdot&amp; -a/\\Delta &amp; \\cdot&amp; 1 \\end{bmatrix} \\] \\[\\begin{align*} &amp; x^{\\tilde{\\mu}}=J^{\\tilde{\\mu}}_\\nu x^\\nu &amp; &amp; x^{\\mu}=J^{\\mu}_{\\tilde{\\nu}} x^{\\tilde{\\nu}}\\\\ &amp; x_{\\tilde{\\mu}}=J_{\\tilde{\\mu}}^{\\nu} x_{\\nu} &amp; &amp; x_{\\mu}=J_{\\mu}^{\\tilde{\\nu}} x_{\\tilde{\\nu}} \\end{align*}\\]"},{"location":"content/useful/theory_pic/","title":"Theory behind PIC","text":"<p>tl;dr</p> <p>Our goal in this section be to derive an equation which could describe an evolution of a collection of charged particles in self-induced eletric and magnetic fields. Ultimately, since we will want to solve this partial differential equation on the computer, we will construct an algorithm relying on the method of characteristics which approximates the exact solution, when the number of integrated characteristics (macroparticles) tends to infinity.</p> <p>At the beginning, we make no simplifying assumptions, but, as we will see further, given the long-range nature of the interaction, we will have to sacrifice generality to make the equations tractable.</p> <p>Clasically, there are two equivalent approaches to this problem. Below, we consider both of them, which hopefully will help highlight different aspects of the resulting equation as well as the assumptions made.</p>"},{"location":"content/useful/theory_pic/#klimontovich-equation-ensemble-average","title":"Klimontovich equation &amp; ensemble average","text":"<p>We start with a distribution function for a collection of charged particles of species \\(s\\) with charge \\(q_s\\) and mass \\(m_s\\) in the phase space \\((\\bm{x},\\bm{u})\\), with \\(\\bm{u}\\) being the relativistic four-velocity:</p> \\[\\begin{equation} N_s(\\bm{x},\\bm{u},t) = \\sum\\limits_i \\delta(\\bm{x}-\\bm{x}_i^s(t))\\delta(\\bm{u}-\\bm{u}_i^s(t)), \\end{equation}\\] <p>where \\(\\bm{x}_i^s(t)\\) and \\(\\bm{u}_i^s(t)\\) are the positions and four-velocities of all the particles of species \\(s\\). We can then take the derivative of this equation in time to get an equation for the evolution of \\(N_s\\):</p> \\[\\begin{align*} \\frac{\\partial }{\\partial t}N_s(\\bm{x},\\bm{u},t) &amp;= \\sum\\limits_i \\frac{\\partial}{\\partial t} \\left[   \\delta(\\bm{x}-\\bm{x}_i^s)\\delta(\\bm{u}-\\bm{u}_i^s) \\right]=\\\\ &amp;=\\sum\\limits_i  \\left[   \\delta(\\bm{x}-\\bm{x}_i^s) \\frac{\\partial}{\\partial t} \\delta(\\bm{u}-\\bm{u}_i^s) +    \\delta(\\bm{u}-\\bm{u}_i^s) \\frac{\\partial}{\\partial t} \\delta(\\bm{x}-\\bm{x}_i^s) \\right]=\\\\ &amp; \\left|~\\frac{\\partial}{\\partial t} \\delta (\\bm{a}-\\bm{a}_i^s)\\equiv -\\frac{d\\bm{a}_i^s}{dt}\\frac{\\partial}{\\partial\\bm{a}}\\delta(\\bm{a}-\\bm{a}_i^s) ~\\right| \\\\ &amp;= -\\sum\\limits_i \\left[   \\delta(\\bm{x}-\\bm{x}_i^s)\\frac{d \\bm{u}_i^s}{dt}\\cdot\\frac{\\partial}{\\partial \\bm{u}}\\delta(\\bm{u}-\\bm{u}_i^s)+   \\delta(\\bm{u}-\\bm{u}_i^s)\\frac{d \\bm{x}_i^s}{dt}\\cdot\\frac{\\partial}{\\partial \\bm{x}}\\delta(\\bm{x}-\\bm{x}_i^s) \\right]=\\\\ &amp;\\left|~ \\frac{d\\bm{u}_i^s}{dt} = \\frac{1}{m_s}\\bm{F}_s^N(\\bm{x}_i^s,~\\bm{u}_i^s),~~ \\frac{d\\bm{x}_i^s}{dt} = \\frac{\\bm{u}_i^s}{\\gamma_i^s} ~\\right|\\\\ &amp;\\left|~ f(\\bm{a}_i^s,~\\bm{b}_i^s)\\delta(\\bm{a}-\\bm{a}_i^s)\\frac{\\partial}{\\partial \\bm{b}}\\delta(\\bm{b}-\\bm{b}_i^s) = f(\\bm{a},~\\bm{b})\\frac{\\partial}{\\partial \\bm{b}} \\left[   \\delta(\\bm{a}-\\bm{a}_i^s)\\delta(\\bm{b}-\\bm{b}_i^s) \\right],~\\textrm{if}~ \\frac{\\partial}{\\partial \\bm{b}}f(\\bm{a},~\\bm{b})=0 ~\\right|\\\\ &amp;= -\\sum\\limits_i \\left[   \\frac{\\bm{u}}{\\gamma}\\cdot\\frac{\\partial}{\\partial \\bm{x}}\\left[\\delta(\\bm{x}-\\bm{x}_i^s)\\delta(\\bm{u}-\\bm{u}_i^s)\\right]+   \\frac{\\bm{F}_s^N(\\bm{x},~\\bm{u})}{m_s}\\cdot\\frac{\\partial}{\\partial \\bm{u}}\\left[\\delta(\\bm{x}-\\bm{x}_i^s)\\delta(\\bm{u}-\\bm{u}_i^s)\\right] \\right]. \\end{align*}\\] <p>We can now use the definition \\((1)\\) to finally write:</p> \\[\\begin{equation} \\frac{\\partial N_s}{\\partial t}+\\frac{\\bm{u}}{\\gamma}\\cdot\\frac{\\partial N_s}{\\partial \\bm{x}} + \\frac{\\bm{F}_s^N}{m_s}\\cdot \\frac{\\partial N_s}{\\partial \\bm{u}} = 0, \\end{equation}\\] <p>(the non-relativistic version of) which was first obtained by Yu. L. Klimontovich in 1958. Here \\(\\gamma \\equiv \\sqrt{1+|\\bm{u}|^2}\\). For a typical plasma with no additional interactions from outside, \\(\\bm{F}_s\\) is the Lorentz force from self-induced electromagnetic fields:</p> \\[ \\bm{F}_s^N = q_s\\left(\\bm{E}^N + \\frac{\\bm{u}}{\\gamma}\\times \\bm{B}^N\\right), \\] <p>where the hyperbolic Maxwell's equations are implied:</p> \\[\\begin{align*} &amp; \\frac{\\partial \\bm{E}^N}{c\\partial t} = \\nabla\\times \\bm{B}^N - \\frac{4\\pi}{c}\\sum\\limits_s \\frac{q_s}{V}\\int \\frac{\\bm{u}'}{\\gamma'} N_s\\left(\\bm{x}',~\\bm{u}'\\right) d^3\\bm{x}'d^3\\bm{u}',\\\\ &amp; \\frac{\\partial \\bm{B}^N}{c\\partial t} = -\\nabla\\times\\bm{E}^N, \\end{align*}\\] <p>with \\(V=\\int d^3\\bm{x}'d^3\\bm{u}'\\), and the other two equations serving as \"boundary conditions\"</p> \\[\\begin{align*} &amp; \\nabla\\cdot\\bm{E}^N = 4\\pi \\sum\\limits_s \\frac{q_s}{V}\\int N_s d^3\\bm{x}'d^3\\bm{u}',\\\\ &amp; \\nabla\\cdot\\bm{B}^N=0. \\end{align*}\\] <p>The superscript \"\\(N\\)\" here implies that the fields are generated from the full distribution function \\(N_s\\).</p> <p>Notice, that in deriving eq. \\((2)\\), we made no assumptions about the strengths of interactions of plasma particles with each other, or the scales of gradients of the fields. In other words, Klimontovich's equation (coupled with Maxwell's equations above) is the most general way of describing the plasmas without the loss of generality.</p> <p>The function \\(N_s\\) still has \\(6N\\) unknowns (where \\(N\\) is the number of plasma particles), and is, in general, not very useful for studying the behavior of plasmas. To advance further, we consider an ensemble of microstates defined by \\(N\\) positions and velocities of particles, \\(\\bm{x}_i^s\\) and \\(\\bm{u}_i^s\\). We further define a special function, \\(f_s(\\bm{x},~\\bm{u})\\equiv \\langle N_s(\\bm{x},~\\bm{u}) \\rangle\\), where from now on we will assume \\(\\langle\\cdot\\rangle\\) to be an ensemble average. Notice, that by doing so, the function \\(f_s\\) loses any information about each individual particle, becoming essentially a continuous function of two phase-space variables.</p> <p>In each microscopic realization, the function \\(N_s\\) can deviate from the ensemble average, which we conveniently denote as \\(\\delta N_s \\equiv N_s - f_s\\). Similarly, we may also define \\(\\bm{E}\\equiv\\langle\\bm{E}^N\\rangle\\), \\(\\bm{B} \\equiv \\langle\\bm{B}^N\\rangle\\), and \\(\\delta \\bm{E}^N\\equiv\\bm{E}^N-\\bm{E}\\), \\(\\delta \\bm{B}^N \\equiv \\bm{B}^N - \\bm{B}\\). </p> <p>Note</p> <p>Note that by definition, \\(\\langle\\delta N_s\\rangle = 0\\), \\(\\langle\\delta \\bm{E}^N\\rangle = 0\\), \\(\\langle\\delta \\bm{B}^N\\rangle = 0\\).</p> <p>Plugging \\(N_s=f_s+\\delta N_s\\), \\(\\bm{F}^N_s=\\bm{F}_s+\\delta\\bm{F}_s^N\\) into equation \\((2)\\), and ensemble averaging the result, we find:</p> \\[\\begin{equation} \\frac{\\partial f_s}{\\partial t} + \\frac{\\bm{u}}{\\gamma}\\cdot \\frac{\\partial f_s}{\\partial\\bm{x}} + \\frac{\\bm{F}_s}{m_s}\\cdot\\frac{\\partial f_s}{\\partial \\bm{u}} = -\\left\\langle\\frac{\\delta \\bm{F}_s^N}{m_s}\\cdot\\frac{\\partial\\delta N_s}{\\partial \\bm{u}}\\right\\rangle \\end{equation}\\] <p>The left-hand-side of this equation is the advective derivative of \\(f_s\\) in phase-space. The non-conservation of the phase-space volume is thus governed solely by the right-hand-side. Ultimately, the right-hand-side contains terms proportional to \\(\\left\\langle\\delta N_s(\\bm{x}',~\\bm{u}')\\delta N_s(\\bm{x}'',~\\bm{u}'') \\right\\rangle\\), which is also referred to as the correlation function. This term describes non-linear correlations of the fluctuations of \\(N_s\\) in two different locations of the phase-space. When the fluctuations from the ensemble average are completely decorelated (independent), this term becomes zero, and we obtain the famous Vlasov equation:</p> \\[\\begin{equation}   \\frac{\\partial f_s}{\\partial t} + \\frac{\\bm{u}}{\\gamma}\\cdot \\frac{\\partial f_s}{\\partial\\bm{x}} + \\frac{\\bm{F}_s}{m_s}\\cdot\\frac{\\partial f_s}{\\partial \\bm{u}} = 0. \\end{equation}\\] <p>In which case can one assume that fluctuations are decorelated? If the evolution of each particle is solely governed by the ensemble averaged quantities (i.e., smooth electromagnetic fields), and (on average) does not depend on the particular realizations of the thermodynamic microstate, then all the non-linear terms must vanish, when ensemble averaged. This may not hold if, for instance, particles actively experience dynamically important close encounters with each other (Coulomb collisions), in which case their evolutions are no longer solely governed by the smooth average forces. For this reason, the right-hand-side of the eq. \\((3)\\) is often referred to as the collisional (Landau) integral.</p>"},{"location":"content/useful/theory_pic/#liouville-equation-bbgky-hierarchy","title":"Liouville equation &amp; BBGKY hierarchy","text":"<p>Instead of thinking of a collection of particles with predefined coordinates in phase-space in the microstate, one can also think of the whole system as a random realization one such microstate. Then we can write down a probability distribution function of the system of \\(6N\\) variables in the following form:</p> \\[\\begin{equation} f_N(\\bm{x}_1,...,\\bm{x}_N,~\\bm{u}_1,...,\\bm{u}_N,~t) = \\prod\\limits_k^N \\delta(\\bm{x}-\\bm{X}_k(t))\\delta(\\bm{u}_k-\\bm{U}_k(t)), \\end{equation}\\] <p>where \\(\\bm{X}_k(t)\\) and \\(\\bm{U}_k(t)\\) are the actual coordinates of the particles in phase-space implicitly depending on time (we drop the species index for brevity). To derive the time evolution of such a system, we differentiate \\(f_N\\) w.r.t. time:</p> \\[ \\frac{\\partial f_N}{\\partial t} = \\prod\\limits_k^N \\left[   \\sum\\limits_i^N\\left(     -\\frac{d\\bm{X}_i}{dt}\\frac{\\partial}{\\partial\\bm{x}_i}-\\frac{d\\bm{U}_i}{dt}\\frac{\\partial}{\\partial\\bm{u}_i}   \\right) \\right] \\delta(\\bm{x}_i-\\bm{X}_k(t))\\delta(\\bm{u}_k-\\bm{U}_k(t)). \\] <p>Using the properties of a delta function, and assuming that \\(\\dot{\\bm{X}}_k = \\bm{U}_k/\\Gamma_k\\) (with \\(\\Gamma_k\\equiv \\sqrt{1+|\\bm{U}_k|^2}\\)), and \\(\\dot{\\bm{U}}_k = (1/m_k)\\sum\\limits_l \\bm{F}_{l\\to k}\\), with the latter expression defining the force from particle with index \\(l\\) to particle with index \\(k\\). Plugging the expression \\((5)\\) back into this equation, we get:</p> \\[\\begin{equation} \\frac{\\partial f_N}{\\partial t} +  \\sum\\limits_k^N \\left(   \\frac{\\bm{u}_k}{\\gamma_k}\\cdot\\frac{\\partial f_N}{\\partial \\bm{x}_k} \\right) +  \\sum\\limits_k^N \\left(   \\sum\\limits_l^N   \\frac{\\bm{F}_{l\\to k}}{m_k}\\cdot    \\frac{\\partial f_N}{\\partial \\bm{u}_k} \\right)=0 \\end{equation}\\] <p>As with the Klimontovich equation before, here we have not yet made any simplifying assumptions, and eq. \\((6)\\) describes the evolution of the combined probability density function, \\(f_N\\), of \\(N\\) particles in the \\(6N\\)-dimensional phase-space in its most general form. </p> <p>In practice, most of the time we are not interested in the exact behavior of each individual particle, as for all practicle purposes they are indistinguishable. It is thus helpful to introduce the reduced probability distribution of \\(M&lt;N\\) particles as follows:</p> \\[\\begin{equation} f_M(\\bm{x}_1,...,\\bm{x}_M,~\\bm{u}_1,...,\\bm{u}_M,~t) \\equiv \\mathcal{V}^{-1} \\int  f_N(\\bm{x}_1,...,\\bm{x}_N',~\\bm{u}_1,...,\\bm{u}_N',~t) d\\bm{x}_{M+1}' d\\bm{u}_{M+1}'...d\\bm{x}_{N}' d\\bm{u}_{N}' , \\end{equation}\\] <p>In other words, in transiting from \\(N\\)-particle distribution to an \\(M\\)-particle one we \"integrate away\" the explicit dependency (or correlation) of the evolution of the first \\(M\\) particles on the last \\(N-M\\) particles. This becomes obvious, when we interate the equation \\((6)\\) w.r.t. \\(\\bm{x}_N\\) and \\(\\bm{u}_N\\) to get the evolution equation for the \\(N-1\\)-particle distribution:</p> \\[ \\frac{\\partial f_{N-1}}{\\partial t} +  \\sum\\limits_k^{N-1} \\left(   \\frac{\\bm{u}_k}{\\gamma_k}\\cdot\\frac{\\partial f_{N-1}}{\\partial \\bm{x}_k} \\right) +  \\sum\\limits_k^{N-1} \\left(   \\sum\\limits_l^{N-1}   \\frac{\\bm{F}_{l\\to k}}{m_k}\\cdot    \\frac{\\partial f_{N-1}}{\\partial \\bm{u}_k}   +\\mathcal{V}^{-1}\\int d\\bm{x}_{N}'d\\bm{u}_{N}'   \\frac{\\bm{F}_{N\\to k}}{m_k}\\cdot    \\frac{\\partial f_{N}}{\\partial \\bm{u}_k} \\right)=0. \\] <p>The dependency of the evolution of the first \\(N-1\\) particles on that of the last particle is now encoded in the last term of the equation; thus in general, the \\(M&lt;N\\) dimensional probability distribution function still depends on the full \\(N\\)-dimensional one:</p> \\[ \\frac{\\partial f_{M}}{\\partial t} +  \\sum\\limits_k^{M} \\left(   \\frac{\\bm{u}_k}{\\gamma_k}\\cdot\\frac{\\partial f_{M}}{\\partial \\bm{x}_k} \\right) +  \\sum\\limits_k^{M} \\left(   \\sum\\limits_l^{M}   \\frac{\\bm{F}_{l\\to k}}{m_k}\\cdot    \\frac{\\partial f_{M}}{\\partial \\bm{u}_k}   +\\frac{N-M}{\\mathcal{V}}\\int d\\bm{x}_{M+1}'d\\bm{u}_{M+1}'   \\frac{\\bm{F}_{M+1\\to k}}{m_k}\\cdot    \\frac{\\partial f_{M+1}}{\\partial \\bm{u}_k} \\right)=0, \\] <p>where it is obvious, that the evolution of \\(f_{M}\\) also depends on the evolution of \\(f_{M+1}\\). This is often called in the literature the BBGKY hierarchy, after Bogoliubov (1946), Born, &amp; Green (1946), Kirkwood (1946, 1947), and Yvon (1935).</p> <p>For \\(M=1\\) we find:</p> \\[ \\frac{\\partial f_1}{\\partial t} +    \\frac{\\bm{u}_1}{\\gamma_1}\\cdot\\frac{\\partial f_1}{\\partial \\bm{x}_1} +    \\frac{N-1}{\\mathcal{V}}\\int d\\bm{x}_{2}'d\\bm{u}_{2}'   \\frac{\\bm{F}_{2\\to 1}}{m_1}\\cdot    \\frac{\\partial f_{2}}{\\partial \\bm{u}_1} = 0, \\] <p>where to derive the equation for a single-particle distribution we need to know the two-particle one, \\(f_2\\). It is useful to decompose the latter into two single-particle distributions and a term describing their correlation:</p> \\[ f_2(\\bm{x}_1,\\bm{x}_2,\\bm{u}_1,\\bm{u}_2,t) = f_1(\\bm{x}_1,\\bm{u}_1,t)f_1(\\bm{x}_2,\\bm{u}_2,t) + \\delta f_{2}(\\bm{x}_1,\\bm{x}_2,\\bm{u}_1,\\bm{u}_2,t), \\] <p>where, just like before, the last term, \\(\\delta f_2\\) describes two-particle interactions. Likewise, we could have written the evolution equation for \\(f_2\\), and decomposed it further into many-body interactions. For our purposes, we will limit ourselves to the evolution of one-particle distribution ignoring the two-particle correlations. Adopting \\(f\\equiv f_1\\), and plugging the \\(f_2\\) decomposition ino the equation above (ignoring \\(\\delta f_2\\)), we reproduce the previously found Vlasov equation:</p> \\[ \\frac{\\partial f}{\\partial t} +    \\frac{\\bm{u}}{\\gamma}\\cdot\\frac{\\partial f}{\\partial \\bm{x}} +   \\frac{\\bm{F}}{m}\\cdot    \\frac{\\partial f}{\\partial \\bm{u}} = 0, \\] <p>where \\(\\bm{F} \\equiv (N/\\mathcal{V})\\int d\\bm{x}'d\\bm{u}' \\bm{F}_{2\\to 1} f(\\bm{x}',\\bm{v}',t)\\)</p>"},{"location":"content/useful/theory_pic/#deriving-equations-for-pic","title":"Deriving equations for PIC","text":"<p>We now derived the evolution equation for the continuous distribution describing a system of particles that do not undergo binary interactions, and only interact via collective large-scale self-induced fields:</p> \\[\\begin{equation} \\frac{\\partial f_s}{\\partial t} +    \\frac{\\bm{u}}{\\gamma}\\cdot\\frac{\\partial f_s}{\\partial \\bm{x}} +   \\underbrace{\\frac{q_s}{m_s}\\left(\\bm{E}+\\frac{\\bm{u}}{\\gamma}\\times\\bm{B}\\right)}_{\\bm{F}_s/m_s}\\cdot    \\frac{\\partial f_s}{\\partial \\bm{u}} = 0, \\end{equation}\\] <p>which, coupled to the two Maxwell's equations,</p> \\[\\begin{equation} \\begin{aligned} &amp; \\frac{\\partial \\bm{E}}{c\\partial t} =  \\nabla\\times \\bm{B} -  \\frac{4\\pi}{c} \\underbrace{\\sum\\limits_s q_s\\int \\frac{\\bm{u}'}{\\gamma'} f_s\\left(\\bm{x},~\\bm{u}'\\right) d^3\\bm{u}'}_{\\bm{J}},\\\\ &amp; \\frac{\\partial \\bm{B}}{c\\partial t} = -\\nabla\\times\\bm{E}, \\end{aligned} \\end{equation}\\] <p>forms a closed system of equations, otherwise known as the Vlasov-Maxwell system. The two boundary conditions</p> \\[\\begin{align*} \\nabla\\cdot\\bm{B} &amp;= 0,\\\\ \\nabla\\cdot\\bm{E} &amp;= 4\\pi \\underbrace{\\sum\\limits_s q_s\\int f_s(\\bm{x},\\bm{u}',t)d^3\\bm{u}'}_{\\rho}, \\end{align*}\\] <p>are satisfied automatically, if one follows the evolution using the system of equations in \\((8)\\) and \\((9)\\). In the most general case, this system is 6-dimensional and non-linear, so solving it numerically is not only challenging, but also costly. </p> <p>Particle-in-cell (PIC) algorithm is a widely used technique which significantly simplifies the solution of this system. To introduce the technique, we first need to decompose the 6D distribution function \\(f_s\\) into special basis functions in phase-space by introducing a finite number of the so-called macroparticles:</p> \\[\\begin{equation} f_s(\\bm{x},\\bm{u},t) \\approx \\sum\\limits_i w_i^s S (\\bm{x}-\\bm{x}_i^s(t))\\delta(\\bm{u}-\\bm{u}_i^s(t)), \\end{equation}\\] <p>where \\(w_i^s\\) are called macroparticle weights, and \\(S\\) is often referred to as a shape function. Since \\(f_s\\) in general is an arbitrary continuous function, and we only introduce a finite set of particles with phase-space coordinates \\((\\bm{x}_i^s,~\\bm{u}_i^s)\\), the equality above is only approximate, and it approaches the exact solution as the number of macroparticles increases. From the shape function we will require the following properties:</p> \\[ \\iiint\\limits_V S(\\bm{x}) d^3\\bm{x} = 1,~\\textrm{and}~S(\\bm{x})\\xrightarrow[\\bm{x}\\not\\in V]{} 0, \\] <p>where \\(V\\) is a finite volume in the real space which includes the origin. Here and further we will use the terms particle and macroparticle interchangeable, but one should really think of these entities (ha-ha), as discrete sampling of the original continuous distribution function.</p> <p>Plugging \\((10)\\) into \\((8)\\), we get the following equation (for brevity, we use \\(\\delta_{\\bm{u}} = \\delta(\\bm{u}-\\bm{u}_i^s)\\), and \\(S_{\\bm{x}}=S(\\bm{x}-\\bm{x}_i^s)\\)):</p> \\[\\begin{equation} \\begin{aligned} &amp;\\sum\\limits_i w_i^s\\left[ -\\delta_{\\bm{u}}   \\frac{d\\bm{x}_i^s}{dt}     \\cdot   \\frac{\\partial S_{\\bm{x}}}{\\partial \\bm{x}} - S_{\\bm{x}} \\frac{d\\bm{u}_i^s}{dt} \\cdot \\frac{\\partial \\delta_{\\bm{u}}}{\\partial \\bm{u}} +  \\delta_{\\bm{u}}\\frac{\\bm{u}}{\\gamma}\\cdot \\frac{\\partial S_{\\bm{x}}}{\\partial \\bm{x}} +  S_{\\bm{x}} \\frac{\\bm{F}_s}{m_s}\\cdot\\frac{\\partial \\delta_{\\bm{u}}}{\\partial \\bm{u}} \\right] = \\\\  &amp;= -\\sum\\limits_i w_i^s\\left[   \\delta_{\\bm{u}}     \\left(       \\frac{d\\bm{x}_i^s}{dt} - \\frac{\\bm{u}}{\\gamma}     \\right)     \\frac{\\partial S_{\\bm{x}}}{\\partial \\bm{x}}   +S_{\\bm{x}}   \\left(     \\frac{d\\bm{u}_i^s}{dt} - \\frac{\\bm{F}_s(\\bm{x},\\bm{u},t)}{m_s}   \\right)   \\frac{\\partial \\delta_{\\bm{u}}}{\\partial \\bm{u}} \\right] = 0, \\end{aligned} \\end{equation}\\] <p>where again, we used the fact that \\(\\partial \\bm{F}_s/\\partial \\bm{u} = 0\\). We are now ready to derive the evolution equations for macroparticles which will exactly reproduce (given large-enough number of them) the original solution of the Vlasov-Maxwell system. We integrate equation \\((11)\\) separately over \\(d^3\\bm{x}\\) and \\(d^3\\bm{u}\\):</p> \\[\\begin{align*} \\int (11)~d^3\\bm{x}'d^3\\bm{u}' =&amp;   \\sum \\limits_i w_i^s \\left[(11.1)+(11.2)\\right] = 0, \\\\ (11.1)=&amp; \\int  d^3\\bm{x}'   \\frac{\\partial S_{\\bm{x}'}}{\\partial \\bm{x}'}   \\int d^3\\bm{u}'   \\left(     \\frac{d\\bm{x}_i^s}{dt} - \\frac{\\bm{u}'}{\\gamma'}   \\right)\\delta_{\\bm{u}'}=\\\\   =&amp;\\int  d^3\\bm{x}'   \\frac{\\partial S_{\\bm{x}'}}{\\partial \\bm{x}'}\\cdot   \\left(     \\frac{d\\bm{x}_i^s}{dt} - \\frac{\\bm{u}_i^s}{\\gamma_i^s}   \\right)=\\\\   =&amp;\\oint     S_{\\bm{x}'}   \\left(     \\frac{d\\bm{x}_i^s}{dt} - \\frac{\\bm{u}_i^s}{\\gamma_i}   \\right)\\cdot d\\bm{\\xi}',\\\\ (11.2)=&amp;\\int d^3\\bm{u}'   \\frac{\\partial \\delta_{\\bm{u}'}}{\\partial \\bm{u}'}\\cdot   \\left(     \\frac{d\\bm{u}_i^s}{dt} -      \\frac{1}{m_s}\\int\\bm{F}_s(\\bm{x}',\\bm{u}',t)S_{\\bm{x}'} d^3\\bm{x}'   \\right)=\\\\   =&amp; \\frac{d\\bm{u}_i^s}{dt} -      \\frac{1}{m_s}\\int\\bm{F}_s(\\bm{x}',\\bm{u}_i^s,t)S_{\\bm{x}'} d^3\\bm{x}', \\end{align*}\\] <p>where we used the Gauss-Ostrogradsky theorem to transform the integral in phase-space volume to an integral over the boundaries. The equality is satisfied for an arbitrarily chosen volume if and only if the following two equations hold:</p> \\[\\begin{equation} \\begin{aligned} \\frac{d\\bm{x}_i^s}{dt} &amp;= \\frac{\\bm{u}_i^s}{\\gamma_i^s},\\\\ \\frac{d\\bm{u}_i^s}{dt} &amp;=     \\frac{1}{m_s}\\int\\bm{F}_s(\\bm{x}',\\bm{u}_i^s,t)S(\\bm{x}'-\\bm{x}_i^s) d^3\\bm{x}'. \\end{aligned} \\end{equation}\\] <p>Naively, one could recognize the equations of motion for particles with coordinates \\(\\bm{x}_i^s\\), and four-velocities \\(\\bm{u}_i^s\\). But the striking and crucial difference from regular equations of motion is in the fact that the force in this case is effectively \"interpolated\" for each macroparticle using the shape function \\(S\\). The absence of this in the first equation is due to our choice of the \\(\\delta\\)-function for the \"shape\" in velocity space. </p> <p>One should really think of the system \\((12)\\) as \\(6N\\)-dimensional system of characteristic equations along which the convective derivative of \\(f_s\\) (given by eq. \\(8\\)) is exactly zero. </p> <p>To see how these macroparticles should induce current densities in our algorithm to exactly satisfy the charge conservation, i.e., \\(\\nabla\\cdot\\bm{E} = 4\\pi \\rho\\), we differentiate the latter equation in time, substituting \\(f_s\\) from \\((10)\\), to get the following:</p> \\[ \\nabla\\cdot\\left(   \\frac{\\partial\\bm{E}}{\\partial t} \\right)= -4\\pi\\nabla\\cdot\\bm{J}= 4\\pi \\sum\\limits_s q_s\\sum\\limits_i w_i^s \\frac{\\partial S(\\bm{x}-\\bm{x}_i^s(t))}{\\partial t}, \\] <p>meaning that the divergence of the deposited current density has to satisfy</p> \\[\\begin{equation} \\nabla\\cdot\\bm{J}= -\\sum\\limits_s q_s\\sum\\limits_i w_i^s \\frac{\\partial S(\\bm{x}-\\bm{x}_i^s(t))}{\\partial t}. \\end{equation}\\]"},{"location":"content/useful/theory_pic/#discretization","title":"Discretization","text":"<p>Having the system of equations on macroparticles defined in \\((12)\\), and with the equations for the electromagnetic fields \\((9)\\) and the current density constrained by \\((13)\\), we can proceed to formulate the particle-in-cell numerical scheme. </p> <p>First, we discretize the electromagnetic fields and the current densities following the Yee staggering convention. According to that, the \\((i,j,k)\\) element of each of the field components is defined in slightly different spatial locations:</p> \\[\\begin{align*} &amp; E_x^{i+1/2,j,k}~~~&amp;~~~&amp; E_y^{i,j+1/2,k}~~~&amp;~~~&amp; E_z^{i,j,k+1/2}\\\\ &amp; B_x^{i,j+1/2,k+1/2}~~~&amp;~~~&amp; B_y^{i+1/2,j,k+1/2}~~~&amp;~~~&amp; B_z^{i+1/2,j+1/2,k}\\\\ &amp; J_x^{i+1/2,j,k}~~~&amp;~~~&amp; J_y^{i,j+1/2,k}~~~&amp;~~~&amp; J_z^{i,j,k+1/2} \\end{align*}\\] <p>Here we use Cartesian coordinates, but this convention naturally translates to any other coordinate system.</p> <p>Why Yee?</p> <p>The reason behind adopting this counter-intuitive convention has to do with the finite-differencing in Maxwell's equations. In particular, if we look at the \\(x\\) component of the Ampere's law, the term \\(\\partial E_x/\\partial t\\) is coaligned with the position of \\(E_x\\) (in our case, \\(i+1/2,j,k\\)). Terms in the curl, on the other hand, have a form: \\(\\partial B_z/\\partial y - \\partial B_y/\\partial z\\). If we attempt to discretize the first term, assuming that \\(B_z\\) is defined at some location \\((*,j',*)\\), we will get:</p> \\[ \\frac{B_z^{*,j',*}-B_z^{*,j'-1,*}}{\\Delta y} = \\left(\\frac{\\partial B_z}{\\partial y}\\right)^{*,j'-1/2,*} \\] <p>We must thus adopt \\(j'-1/2 = j\\), or \\(j' = j+1/2\\). Likewise, all the other components can also be inferred. In other words, the choice is dictated by the requirement that the left-hand-side of Faraday's and Ampere's laws be co-aligned with the right-hand-side written in the finite difference form.</p> <p>Time in PIC is also discretized (we typically use the index \\(n\\) to indicate the timestep). In our particular case, for the forward-integration we employ the so-called leapfrog algorithm, where the electric and magnetic fields are defined to be staggered with respect to each other by half a timestep (defined to be \\(\\Delta t\\)). In other words, we have \\(\\bm{E}^{(n)}\\), and \\(\\bm{B}^{(n-1/2)}\\). </p> <p>Coordinates and velocities of macroparticles are tracked separately and have continuous values at each discrete timestep. We use the same leapfrog algorithm for integrating particle equations of motion defined in \\((12)\\), and thus the velocities and coordinates are staggered with respect to each other in time: \\(\\bm{x}_i^{(n)}\\), and \\(\\bm{u}_i^{(n-1/2)}\\).</p> <p>There are two ingredients which were left out in this picture: interpolation of the electromagnetic force from the grid to the position of the particle (required by the second equation in eq. \\((12)\\)), and the deposition of currents on the discretized grid, which have to satisfy the requirement in \\((13)\\) to conserve charge. For both of these issues, we first need to make a choice of the shape function, \\(S(\\bm{x})\\). By far the most common choice is the first-order (linear) shape, defined as:</p> \\[\\begin{align*} S(\\bm{x}) = \\begin{cases} 1 - \\frac{|x|\\cdot|y|\\cdot|z|}{\\Delta x\\Delta y\\Delta z},~~~&amp;|x|&lt;\\Delta x,~|y|&lt;\\Delta y,~|z|&lt;\\Delta z \\\\ 0,~&amp;\\textrm{otherwise} \\end{cases} \\end{align*}\\] <p>Using this definition, the integral in \\((12)\\) is simply a linear interpolation of each field component from each corresponding location to the position of the particle.</p> <p>Current deposition is slightly trickier, and we will not go through the entire derivation of the algorithm (for the reference, see Esirkepov 2001 or Umeda+ 2003). However, it can be shown mathematically, that asserting specific properties for the shape function (e.g., symmetry in all directions, etc.), there exists a unique set of coefficients to translate the shape function of each particle at timesteps \\((n)\\) and \\((n+1)\\) to the deposited current components. </p> <p>Finally, the full timestepping algorithm with everything discussed above is shown on the page of this wiki about the PIC algorithm.</p>"},{"location":"js/vendor/highlight.js/","title":"Highlight.js CDN Assets","text":"<p>This package contains only the CDN build assets of highlight.js.</p> <p>This may be what you want if you'd like to install the pre-built distributable highlight.js client-side assets via NPM. If you're wanting to use highlight.js mainly on the server-side you likely want the highlight.js package instead.</p> <p>To access these files via CDN: https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@latest/build/</p> <p>If you just want a single .js file with the common languages built-in: https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@latest/build/highlight.min.js</p>"},{"location":"js/vendor/highlight.js/#highlightjs","title":"Highlight.js","text":"<p>Highlight.js is a syntax highlighter written in JavaScript. It works in the browser as well as on the server. It works with pretty much any markup, doesn\u2019t depend on any framework, and has automatic language detection.</p> <p>If you'd like to read the full README: https://github.com/highlightjs/highlight.js/blob/main/README.md</p>"},{"location":"js/vendor/highlight.js/#license","title":"License","text":"<p>Highlight.js is released under the BSD License. See LICENSE file for details.</p>"},{"location":"js/vendor/highlight.js/#links","title":"Links","text":"<p>The official site for the library is at https://highlightjs.org/.</p> <p>The Github project may be found at: https://github.com/highlightjs/highlight.js</p> <p>Further in-depth documentation for the API and other topics is at http://highlightjs.readthedocs.io/.</p> <p>A list of the Core Team and contributors can be found in the CONTRIBUTORS.md file.</p>"},{"location":"js/vendor/three/examples/jsm/libs/basis/","title":"Basis Universal GPU Texture Compression","text":"<p>Basis Universal is a \"supercompressed\" GPU texture and texture video compression system that outputs a highly compressed intermediate file format (.basis) that can be quickly transcoded to a wide variety of GPU texture compression formats.</p> <p>GitHub</p>"},{"location":"js/vendor/three/examples/jsm/libs/basis/#transcoders","title":"Transcoders","text":"<p>Basis Universal texture data may be used in two different file formats: <code>.basis</code> and <code>.ktx2</code>, where <code>ktx2</code> is a standardized wrapper around basis texture data.</p> <p>For further documentation about the Basis compressor and transcoder, refer to the Basis GitHub repository.</p> <p>The folder contains two files required for transcoding <code>.basis</code> or <code>.ktx2</code> textures:</p> <ul> <li><code>basis_transcoder.js</code> \u2014 JavaScript wrapper for the WebAssembly transcoder.</li> <li><code>basis_transcoder.wasm</code> \u2014 WebAssembly transcoder.</li> </ul> <p>Both are dependencies of <code>KTX2Loader</code>:</p> <pre><code>const ktx2Loader = new KTX2Loader();\nktx2Loader.setTranscoderPath( 'examples/jsm/libs/basis/' );\nktx2Loader.detectSupport( renderer );\nktx2Loader.load( 'diffuse.ktx2', function ( texture ) {\n\n    const material = new THREE.MeshStandardMaterial( { map: texture } );\n\n}, function () {\n\n    console.log( 'onProgress' );\n\n}, function ( e ) {\n\n    console.error( e );\n\n} );\n</code></pre>"},{"location":"js/vendor/three/examples/jsm/libs/basis/#license","title":"License","text":"<p>Apache License 2.0</p>"},{"location":"js/vendor/three/examples/jsm/libs/draco/","title":"Draco 3D Data Compression","text":"<p>Draco is an open-source library for compressing and decompressing 3D geometric meshes and point clouds. It is intended to improve the storage and transmission of 3D graphics.</p> <p>Website | GitHub</p>"},{"location":"js/vendor/three/examples/jsm/libs/draco/#contents","title":"Contents","text":"<p>This folder contains three utilities:</p> <ul> <li><code>draco_decoder.js</code> \u2014 Emscripten-compiled decoder, compatible with any modern browser.</li> <li><code>draco_decoder.wasm</code> \u2014 WebAssembly decoder, compatible with newer browsers and devices.</li> <li><code>draco_wasm_wrapper.js</code> \u2014 JavaScript wrapper for the WASM decoder.</li> </ul> <p>Each file is provided in two variations:</p> <ul> <li>Default: Latest stable builds, tracking the project's master branch.</li> <li>glTF: Builds targeted by the glTF mesh compression extension, tracking the corresponding Draco branch.</li> </ul> <p>Either variation may be used with <code>DRACOLoader</code>:</p> <pre><code>var dracoLoader = new DRACOLoader();\ndracoLoader.setDecoderPath('path/to/decoders/');\ndracoLoader.setDecoderConfig({type: 'js'}); // (Optional) Override detection of WASM support.\n</code></pre> <p>Further documentation on GitHub.</p>"},{"location":"js/vendor/three/examples/jsm/libs/draco/#license","title":"License","text":"<p>Apache License 2.0</p>"}]}